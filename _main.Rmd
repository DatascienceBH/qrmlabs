--- 
title: "Lab Guide to Quantitative Research Methods in Political Science, Public Policy & Public Administration"
author: Joseph Ripberger, Cody Adams, Alex Davis, and Josie Davis
site: bookdown::bookdown_site
output: 
  # bookdown::gitbook:
  bookdown::pdf_book:
book_filename: "labbook"
always_allow_html: yes
language:
  ui:
    chapter_name: "Chapter"
delete_merged_file: true
---

# Preface {-}

This book is a companion to _Quantitative Research Methods for Political Science, Public Policy and Public Administration (With Applications in R): 4th Edition_, an open-source text book that is available [here](https://bookdown.org/josiesmith/qrmbook/). It grew from our experiences teaching introductory and intermediate quantitative methods classes for graduate students in Political Science and Public Policy at the University of Oklahoma. We teach these courses using a format that pairs seminars on theory and statistics with exercises that focus on applications in `R`. We use the text book to motivate seminars and this book to guide exercises. The book is written in `R Markdown` and `bookdown`. While a "complete" copy of the book is available, we suggest that students and instructors use the "raw" `.Rmd` files for exercises. These materials ara available in our `GitHub` repository [here](https://github.com/ripberjt/qrmlabs). 

Currently, the labs use survey data from the _Meso-Scale Integrated Socio-geographic Network (M-SISNet)_ to explain concepts and methods. The M-SISNet is a quarterly survey of approximately 1,500 households in Oklahoma that is conducted with support of the National Science Foundation (Grant No. IIA-1301789). One wave of M-SISNet data is available in our `GitHub` repository. Readers can learn more about the project and download the remaining waves [here](http://crcm.ou.edu/epscordata/). We welcome students and instructors to explore and use these data, but we also encourage instructors to modify the book by incorporating data that is more relevant to the course they are teaching. 

## Acknowledgments {-}
By intent, this book represents an open-ended group project that changes over time as new ideas and new instructors become involved in teaching graduate methods in the University of Oklahoma Political Science Department. Early ideas and materials came from Hank Jenkins-Smith, who began teaching these courses (in R) in 2010. Graduate assistants Matthew Nowlin, Tyler Hughes, and Aaron Fister were responsible for developing the labs for Hank's courses. Joseph Ripberger began teaching these courses in 2015. Wesley Wehde was responsible for updating the labs for his courses. After many years of informal development, Alexander Davis, Cody Adams, and Josie Davis took the labs and turned them in to this book. We thank everyone, especially our students, for helping us write and continue to improve this book. 

## Requirements {-}

Each chapter is written in R Markdown, allowing readers to follow along and interact with examples via their favorite integrated development environment (e.g., RStudio), or to knit as a PDF. For more information on R Markdown, visit: https://rmarkdown.rstudio.com/.

Statistical analysis in R is possible via a plethora of publicly available packages. These chapters introduce various functions from select packages to expand upon the base functions of R, and therefore the following packages are required:

1. car
2. reshape2
3. descr
4. tidyverse
5. skimr
6. memisc
7. stargazer
8. MASS
9. pscl
10. broom
11. zeligverse
12. plotly
13. vcd
14. HistData
15. sfsmisc
16. interplot
17. sandwich
18. DAMisc

For convenience, executing the _setup.R_ script or knitting this document within RStudio will install these packages if they are not already installed.

```{r preface, echo=FALSE, include=FALSE}

packages <- c("DAMisc", "car", "reshape2", "descr", "psych", "tidyverse", "skimr", "memisc", "stargazer", "MASS", "pscl", "broom", "zeligverse", "plotly", "vcd", "HistData", "sfsmisc", "interplot", "sandwich")

for (pkg in packages) {
	if (pkg %in% rownames(installed.packages()) == FALSE) {
		install.packages(pkg, type="binary")
	}
}

library(car)
library(reshape2)
library(descr)
library(psych)
library(tidyverse)
library(skimr)
library(memisc)
library(stargazer)
library(MASS)
library(pscl)
library(broom)
library(sandwich)
library(zeligverse)
library(plotly)
library(vcd)
library(HistData)
library(sfsmisc)
library(interplot)
library(DAMisc)

ds <- read.csv("https://github.com/ripberjt/qrmlabs/raw/master/Class%20Data%20Set%20Factored.csv", header = TRUE)
options(scipen=999)

pagebreak <- function() {
  if(knitr::is_latex_output())
    return("\\newpage")
  else
    return('<div style="page-break-before: always;" />')
}
```

`r pagebreak()`

## Copyright {-}
Lab Guide to Quantitative Research Methods in Political Science, Public Policy & Public Administration
\
\
Joseph Ripberger\
Cody Adams\
Alex Davis\
Josie Davis
\
\
\
\
\
Published by University of Oklahoma Libraries
\
\
Norman, Oklahoma
\
\
Copyright (c) 2019 Joseph Ripberger, Cody Adams, Alex Davis, and Josie Davis
\
\
Unless otherwise noted, content is licensed with a CC BY-NC Creative Commons Attribution NonCommercial International 4.0.
\
```{r cc, echo=FALSE}
knitr::include_graphics("CC-BY_icon.png")
```
\
\
\
Download this book for free at [insert handle URL]
\
\
ISBN: X
\
\
DOI: X

`r pagebreak()`

<!--chapter:end:index.rmd-->

# Introduction to R, RStudio, and R Markdown

In these labs and the corresponding textbook, we will use the `R` programming language to learn statistical concepts and analyze real-world data. Before we dive into the details, this lab will provide an introduction to the `R` language, RStudio, and R Markdown - as well as how the three interact. 

To put it simply - `R` is the actual programming language, RStudio is a convenient interface in which to use it, and R Markdown is a specific type of file format designed to produce documents that include both code _and_ text.

## Introduction to R, the language

`R` itself is a language and environment for statistical computing and graphics, which operates as an integrated suite of software facilities for data manipulation, calculation, and graphical display. It includes:

- an effective data handling and storage facility,
- a suite of operators for calculations,
- a large, coherent, integrated collection of intermediate tools for data analysis,
- graphical facilities for data analysis and display, either on-screen or on hardcopy, and
- a well-developed, simple and effective programming language

`R` is a powerful and effective tool for computing, statistics and analysis, and producing data visualizations. However, many applications exist that can do these or similar things. `R` has a number of benefits that make it particularly useful, though. 

### Benefits of R

First, `R` is both __open source__ and __free__. This allows you to take the tool and the skills you've learned with you wherever you go; you are not dependent on your employer to buy or have a license of a particular software. This is especially relevant as many other software with similar functionality often cost hundreds, if not thousands, of dollars for a single license. 

The open source nature of `R` has also resulted in a robust set of users across a wide variety of disciplines who are constantly updating and revising the language. R therefore has some of the most up-to-date and innovative functionality and methods available to its users, should they know where to look. 

### Packages

Within `R`, these functions and tools are often implemented as packages. Packages allow advanced users of R to contribute statistical methods and computing tools to the general users of `R`. These packages are reviewed and vetted before being made available for public use; they are often frequently updated as well. In the first lab, we will install some basic packages that are frequently used throughout the course.

## RSudio

RStudio is an integrated development environment (IDE) that makes `R` a bit more user-friendly. While it is not the only way to use `R`, it provides a helpful and intuitive interface for writing and editing code, as well as creating documents and reports. It is not, however, a requirement for using the `R` language.

Additionally, it is important to note that RStudio is an entirely separate piece of software - it will need to be downloaded separately from `R`. 


### Navigating RStudio

As you can see, the RStudio interface is primarily composed of 4 quadrants.

In the upper-left corner is the __source pane__. This is the primarily location where most of your work will take place. You will write and edit collections of code - or R Scripts - here. When working in the source pane, your code will not compile untl you tell it to run; this allows you the flexibility to work at your own pace, as well as to save your work.

In the lower-left corner is the __console__, or the command window. The console is the powerhouse of the software; this is where `R` actually evaluates code. While you can type code directly into the console and receive immediate results, it is advisable to stick to the source pane while you are learning how to use R. Running code in the source pane will automatically produce output in the console pane.

The upper-right quadrant contains the __Environment__ and __History__ tabs. The Environment tab displays a list of all the data objects that you havae defined in your current R session, as well as some basic details about the data (such as the number of observations and variables in each). The History tab contains an archive of all the commands you've run in the current session.

Finally, the lower-right quadrant holds a number of helpful navigation tabs. The "Files" tab displays your hard drive's own file directory for easy access. The "Plots" tab will show the plots and visualzations you have created in your current R session. The "Packages" tabs shows a list of all the packages currently installed, as well as an indication of whther or not that are loaded in the current session. The "Help" tab is, unsurprisingly, the help menu.

Until you are comfortable writing and executing code to analyze data, the RStudio interface can seem intimidating. Remember - since these are open source software, there are plenty of resources online to help as well. A "cheat sheet" for the RStudio IDE can be found [here](https://resources.rstudio.com/rstudio-cheatsheets/rstudio-ide-cheat-sheet).

## R Markdown

Markdown is a tool for converting plain text into formatted text. R Markdown utilizes the markdown syntax in order to combine formatted text with code in a single document.

Within RStudio, R Markdown is a specific type of file format for making dynamic documents. It allows you to simultanesouly
  1. save and execute code, and
  2. produce high quality documents that include both code and text

For the purposes of our labs, R Markdown allows us to include code chunks and the text that helps explain them in an easy-to-read manner. For your own use, R Markdown will allow you to write documents and reports that include traditional formatted text, as well as the data visualizations you make in class, and present them both together in a high quality professional document.

<!--chapter:end:00-intro.Rmd-->

# Basics of R

This chapter serves as a primer to `R` by introducing the basics. It is advised to follow the lab via the `.rmd` file within RStudio rather than solely the compiled PDF. This way, students can experiment with code in the "code blocks" provided. __Note:__ In text R code is displayed in a `fixed-width` font. 

## R, as a Calculator

The first thing to know about R is that it is essentially a large calculator capable of performing arithmetic:

```{r 1_calc, echo = TRUE}
1 + 1

8 * 8

2 ^ 8 # exponent

(5 + 2) ^ 4

5 + 2 ^ 4

```

R also supports elementary and algebraic functions such as log and square root.

```{r 1_functions, echo = TRUE}
log(100)

sqrt(49)
```

### Order of Operations

R solves equations according to the order of operations, "PEMDAS":

1. Parentheses
2. Exponents
3. Multiplication
4. Division
5. Addition
6. Subtraction

Watch this video for a refresher on the order of operations: https://www.youtube.com/watch?v=94yAmf7GyHw
 
__Try this!__ Using R, solve:
(5 + 1) ^ 4 / (9 - 2) ^ 3

```{r 1_solve, echo = FALSE}

```

## Objects

R is an "object oriented" programming language. Put simply, R uses objects to store attributes. Objects are created by assigning an attibute to them via the `<-` operation. You can always view the attribute of an object by typing the object name.

```{r 1_object 1, echo = TRUE}
object1 <- 10 + 10 
object1
```

__Try this!__ Create a new object below; you can name it almost anything!

```{r 1_object 2, echo = TRUE}


```

R includes various functions for managing created objects. The `ls()` function lists all existing objects.

```{r 1_list, echo = TRUE}
ls()
```

The `rm()` function removes existing objects.

```{r 1_remove, echo = TRUE}
rm(object1)
```

There is no strict convention for naming objects; however, there are best practices:

1. Avoid spaces; use _underscores_, periods, or CamelCase (or camelCase) for long object names
    - e.g., This_is_a_long_name, This.Is.A.Long.Name, thisIsALongName
2. Avoid names of existing functions or reserved R objects
    - e.g., `mean` or `sum` 
3. Be descriptive but keep it short (less than 10 characters)
4. Avoid special characters
    - e.g., ? $ % ^ &
5. Numbers are fine, but names cannot begin with numbers.
    - e.g., `object1` is ok, but `1object` is not

__Important:__ Object names are case sensitive.
    - e.g., `object.One` and `Object.One` refer to two separate objects

```{r 1_newobject, echo = TRUE}
object.One <- 10 + 10
Object.One <- 5 + 5

object.One
Object.One
```

## Functions

In addition to elementary and algebraic functions, R includes functions that simplify statistical analysis. For example, the `mean()` function calculates the mean. 

__Note:__ Sometimes `na.rm = TRUE` is necessary within the paranetheses to instruct R to ignore missing data. 

```{r 1_mean, echo = TRUE}
mean(cars$dist)
```

R comes with a vairety of "built in" data sets, such as the `cars` data set, which contains some information about about cars. This data set is used below to demonstrate and/or experiment with R functions.

A note about syntax: the dollar sign, `$`, is used to indicate the variable of interest relative to a data set. This is important in the case of multiple data sets that contain variables of the same name. In the previous code, R calculated the mean using the `dist` variable within the `cars` data set by specifying `cars$dist`

To ignore missing data when calculating the mean of `dist`, include the `na.rm = TRUE` argument within the paranetheses as follows.

```{r 1_mean2, echo = TRUE}
mean(cars$dist, na.rm = TRUE)
```

__Note:__ The mean is exactly the same because there is no missing data in the `dist` variable.

### Object Types

Object types are improtant in R and the type of an object is contingent on the attribute stored by the object. For example, an object storing characters (e.g., "blue") has a different type than an object storing a number (e.g., 1). Use of R functions is contingent on the type of objects. For example, functions like `mean()` work only for objects containing numbers. The R `str()` function describes the structure of objects and functions.

```{r 1_structure, echo = TRUE}
str(mean)
str(object.One)
str(cars)
str(cars$dist)
```

The `str()` function described `mean` as a function, `object.One` as a numeric object, the `cars` data as a data frame, etc. 

Previously, objects were introduced as a method of storing single attributes, either a specified value or the result of arithmetic. In addition, objects can contain a collection of data via a vector or list. In mathematics and physics, a vector is defined as a quantity of both direction and magnitude. In R, vectors are defined as a collection of data of the same type. The `c()` function creates a vector.

```{r, vector, echo = TRUE}
vectorObject <- c(1, 2, 3)
vectorObject
str(vectorObject)
```

Further, a list is defined as a collection of multiple data types.

```{r 1_list, echo-TRUE}
listObject <- list("your name", 1, F)
listObject
str(listObject)
```

__Note:__ The structure of the list object consists of a character, a number, and a logic (True/False).

## Packages

Packages expand R to include additional functions important to statistical analysis.

### Installing Packages

Installing packages in R can be performed via `install.packages("packagename")`, whereby the name of the desired package must be within quotation marks.

__Note:__ Occasionally a package may require dependencies for installation. The dependencies can be automatically installed along with the package by including the `dependencies = TRUE` argument within the `install.packages()` function.

__Try this!__ Use this code to install the following packages:

1. `car`
2. `psych`
3. `memisc`
4. `Rcpp`

```{r 1_install packages, echo = FALSE, eval = FALSE}

```

### Loading Packages

After installation, packages must be loaded to use their functions within R. Packages are loaded via `library(packagename)`.
__Note:__ Unlike the `install.packages()` function, the `library()` function does not require quotation marks around the package name. 

```{r 1_library packages, echo = TRUE}
library(car)
library(psych)
library(memisc) 
library(Rcpp)
library(rmarkdown)
library(knitr)
```

__Note:__ The `memisc` package contains object/package conflicts with the `car` package for the `recode` object. A conflict occurs when two packages contain objects (e.g., functions) of the same name. A conflict will not prevent loading packages; however, use of a specific package's object requires an explicit call to the desired parent package. For example, to use the `recode()` function from `car`, the `car::recode(variable)` statement will explicitly call `recode()` from the `car` package. Vice versa, `memisc::recode()` will explicitly call `recode()` from the `memisc` package.

### Updating Packages

Most packages are regularly updated. The  `old.packages()` function compares installed packages to their latest versions online.

```{r 1_old, echo = TRUE}

```

The `update.packages()` function updates out of date packages. 

__Note:__ Updating packages requires consent. The `ask = FALSE` argument will skip the additional consent step to save time.

```{r 1_update, echo = TRUE}
update.packages()
```

The `library()` function lists currently loaded packages.

```{r 1_library, echo = TRUE}
library()
```

As previously demonstrated, occasionally conflicts exist between packages. The `conflicts()` fubction lists conflicts between loaded packages.

```{r 1_conflicts, echo = TRUE}
conflicts()
```

The `detach()` function detaches packages and is an alternative method to resolve conflicts. Supplying the `unload = TRUE` argument within the `detach()` function will unload the package. For example, to resolve the `recode()` function conflict between `car` and `memisc`, the memisc package can be detached and unloaded as follows: `detach(package:memisc, unload = TRUE)`.

```{r 1_detach, echo = TRUE}

```

## R Help

R includes a help function to assist with functions, accessible by including a `?` prior to the function name.

```{r 1_help, echo = TRUE}
? mean
```

__Note:__ The help documentation will display in the bottom right quadrant of RStudio. Alternatively, typing the function name into the help search bar will yield a similar result. 

To search all of R documentation for help about a function, use `??`.

```{r 1_search, echo = TRUE}
?? mean
```

__Note:__ Google is a valuable tool for finding help. Large communities like StackExchange provide answers and explanations to common issues in R. At times, a particular problem may seem unique, but someone else has almost certainly had the same problem and the solution likely can be found online.

## Setting a Working Directory

The working directory is the location where files are accessed and saved within a R session. Normally, the working directory is set at the beginning of every R file. The working directory should be set and the class data loaded at the beginning of each lab.

There are two methods of setting the working directory. First, the `setwd()` function can be used with the directory path. For example, `setwd("C:/Directory\_to\_folder/")`. 

__Note:__ Forward slashes are used in place of backward slashes for directory paths.

Second, within RStudio, the "Session" tab will allow you to set the working directory. The following steps provide guidance to the "Session" tab functionality: 

1. Click the "Session" tab.
2. Select "Set Working Directory."
3. Select "Choose Directory."
4. Select the working directory.

The `getwd()` function returns the set working directory.

```{r 1_directory, echo = TRUE}
getwd()
```

## Importing Your Data

R can read many different file types, including text files, Excel files, Google Sheet files, SPSS files, and Stata files. It can even read data sets directly from websites. The file type determines the function that is necessary import a data set. For example, CSV files use the function `read.csv` to import a dataset. Here is an example that uses this function:

`ds <- read.csv("Class Data Set Factored.csv", header = TRUE)`

This line of code saves the data set in `Class Data Set Factored.csv` to an object called ds (short for data set). The `header = TRUE` argument tells R that the first row in the data set provides column (or variable) names. 

__Note:__ This code assumes that `Class Data Set Factored.csv` is in the working directory. To check, use `list.files()`. If the file containing the data set is not in the working directory, provide the complete file path in the `read.csv` function, like this:

`ds <- read_csv("https://github.com/ripberjt/qrmlabs/raw/master/Class%20Data%20Set%20Factored.csv", header = TRUE)`

<!--chapter:end:01-basics.rmd-->

# Formatting, Describing, and Visualizing Data

This lab discusses the basics of formatting, describing, and visualizing data. The following packages are required for this lab: 

1. car
2. reshape2
3. descr
4. psych
5. tidyverse
6. skimr

__Reminder:__ Lab One introduced how to install packages via the `install.packages()` function. 

__Note:__ These packages should already be installed from the class initialization script.

The installed packages require loading at the beginning of a R session. Remember that this can be done via the `library()` command.

## Factoring

The previous lab introduced several types of objects in R. For the purpose of this lab, objects are classified into two broad groups: factors and numerics.

Factors are nominal data that a label is applied to.
 - e.g., race, gender, party identification, etc. 

Numerics are data consisting of numbers, which are ranked (ordinal and interval). 

When data is read into R (e.g., by importing a .csv file), R automatically classifies the data by type. When data is recognized as non-numeric, R will classify it as a factor.

In the class dataset, the variable `f.party.2` is a variable consisting of factors that identifies the party affilation of the individuals who answered a survey. The `table()` function describes the variable by category.

```{r 2_factor1, echo = TRUE}
table(ds$f.party.2)
```

Attempting to take mean of the `f.party.2` variable inevitably fails.

```{r 2_mean, echo = TRUE}
mean(ds$f.party.2)
```

Party affiliation is nominal data that cannot be described via mean. Consequently, R will error when attempting to calculate the mean or median for factor variables. 

The `str()` function describes the structure of the `f.party.2` variable:

```{r 2_structure, echo = TRUE}
str(ds$f.party.2)
```

The `f.party.2` variable is a factor with three levels: "Dem", "Ind", and "Rep."

With many data sets, data are initially coded in numbers and factored afterwards. For example, the `party` variable in the `ds` data set is numeric:

```{r 2_numeric, echo = TRUE}
table(ds$party)
```

Without a codebook to decipher the numeric values, statisticians are unable to explain what 1, 2, 3, or 4 represent within the `ds` data set. Factoring data remedies this issue. Factoring data in R serves two broad purposes:

1. Applies labels to data
2. Tells R to treat the data as categorical/nominal

At a very basic level, a variable can be factored without applying labels. At minimum, R will treat the data as categorical. This method is sufficient when a variable requires quick factoring. The basic syntax is to use the `factor()` function. 
__Note:__ Best practice for factoring an existing variable is to create an additional variable within the same data set. Best practice then suggests to append the new variable with `f.` to indicate the variable is factored.

```{r 2_fac1, echo = TRUE}
ds$f.gender <- factor(ds$gender)
```

__Reminder:__ The `$` sign shows R which data set to draw the variable from, or to tell R what data set to assign the new factored variable to. The new factored variable can be described via the `table()` function.

```{r 2_fac2, echo = TRUE}
table(ds$f.gender)
```

While this factored variable is split into two categories, it is not apparent which numbers represent male and female. Labels should be assigned to the numbers to clarify the relationship between the numbers and their meaning. When factoring a variable, R requires the number of levels within the variable and the corresponding labels. In the `f.gender` variable there are two levels, 0 and 1, that require women and men labels.

```{r 2_fac3, echo = TRUE}
ds$f.gender <- factor(ds$gender, levels = c(0, 1), 
                      labels = c("Women", "Men"))
table(ds$f.gender)
```

The vector function, `c()`, tells R the levels and labels of the variable.

The `f.party` variable should also be factored, where 1 = Dem, 2 = Rep, 3 = Ind, and 4 = Other, and a table of the variable should be created.

```{r 2_fac4, echo = TRUE}
ds$f.party <- factor(ds$party, levels = c(1, 2, 3, 4), 
                     labels = c("Dem", "Rep", "Ind", "Other"))
table(ds$f.party)
```

The structure of the `f.party` variable describes a factor with four levels.

```{r 2_fac5, echo = TRUE}
str(ds$f.party)
```

There are other types of data conversions as well. In most cases, the basic syntax is the same as the `factor()` function, except the function names are `numeric()` or `integer()`.
__Note:__ For most purposes, numeric and integer are the same.

### Coerce Factoring

Sometimes the typical commands do not work and variables must be coerced. When a variable is coerced, R is instructed to treat the variable as if it were a different object type. The coercive functions are:

1. `as.factor()`
2. `as.numeric()`
3. `as.integer()`

__Try it!__ Convert a factor variable into a numeric variable using the `numeric()` function.

```{r 2_num, echo = TRUE}

```

This fails, so the coerce function should be employed.

```{r 2_num2, echo = TRUE}
ds$n.party <- as.numeric(ds$f.party)
```

Examine the new variable with the `table()` and `str()` functions.

```{r 2_num3, echo = TRUE}
table(ds$n.party)
str(ds$n.party)
```

## Recoding

In R, the `recode()` function recodes values within a variable. There many reasons to recode a variable, including:

1. To correct or change incorrect data.
2. To restructure data, making it easier for calculations.
3. To emphasize some intended effect.

For example, a recode is necessary to look at age groups instead of exact ages. If everyone in the survey reported their exact age (e.g., 54, 23, etc.) and age groups are necessary (e.g., 18-25, 26-35, etc.), the `recode()` function is useful.

To perform a recode, follow this basic syntax: `recoded.variable <- recode(old.variable, "recode commands")`

Performing a recode is best demonstrated via example. The best practice for recoding variables suggest creating a new variable with a `r.` prefix. 

Within the class data set is an `ideol` variable describing ideology. Currently, the ideology variable goes from 1 to 7, with 1 being very liberal and 7 being very conservative. The ideology variable can be recoded from seven levels to three, liberal, moderate, and conservative, via the `recode()` function. Values from 1 to 2 will recode as 1, 3 to 5 will recode as 2, and 6 to 7 will recode as 3. This `recode()` function is provided by the `car` package. 

```{r 2_recode1, echo = TRUE}
ds$r.ideol <- car::recode(ds$ideol, "1:2 = 1; 3:5 = 2; 6:7 = 3; 
                          else = NA; NA = NA")
```

The `table()` function describes the result of recoding the `ideol` variable.

```{r 2_recode2, echo = TRUE}
table(ds$r.ideol, useNA = "always")
```

__Note:__ `else = NA; NA = NA` is included at the end of the recode function. This instructs R to regard any other responses, whether missing data or data that for some reason is outside the original range, as NA, and to treat all existing NAs as NAs. Sometimes `-99 = -99` or `-99 = NA` requires inclusion in the function as well. 

__Note:__ In the recode function, all the recode arguments are required __inside one set of quotation marks__. Each argument is separated with a semicolon. R will generate an error message if commas are used. 

Using colons to define ranges will save time. In the `recode()` function, using `3:5 = 1` instructs R to recode all values between 3 and 5 to a 2. This is the preferred method opposed to typing `3 = 2; 4 = 2; 5 = 2`.

There is no standard method to categorize or recode data. Let the research question, model design, and data determine the best approach to recoding. For example, in surveys that ask individuals to support something on a scale of 1 to 4, with 1 as very supportive and 4 as least supportive, perhaps recoding the higher value to indicate greater support is appropriate.

Now let's look at the race variable:

```{r 2_race, echo = TRUE}
table(ds$race)
```

In the `ds` data set, the `race` variable consists of codes: 1 for Caucasian, 2 for African-American, and 3 through 7 for a variety of other races (Native American, Asian, Pacific Islander, 2+ races, and Other).

__Try it!__ Recode this variable to go from 7 levels to 3, where 1 is still Caucasian, 2 is African American, and 3 includes all others. 

```{r 2_race2, echo = TRUE}
ds$r.race <- car::recode(ds$race, "1 = 1; 2 = 2; 3:7 = 3; 
                         else = NA; NA = NA")
table(ds$r.race)
```

### Factoring and Recoding

Factoring a variable is generally easier subsequent to recoding. Given the `race` variable now consists of 3 levels instead of 7, factoring will add meaningful words in place of the values. The values 1, 2, and 3 can be factored as White, African-American, and Other, respectively.

```{r 2_race3, echo = TRUE}
ds$f.race.2 <- factor(ds$r.race, levels = c(1, 2, 3), 
                      labels = c("White", "African-American", "Other"))
table(ds$f.race.2)
```

The same can be done with the `ideol` variable. The values 1, 2, and 3 can be factored as Liberal, Moderate, and Conservative, respectively.

```{r 2_ideol, echo = TRUE}
ds$f.ideol <- factor(ds$r.ideol, levels = c(1, 2, 3), 
                     labels = c("Liberal", "Moderate", "Conservative"))
table(ds$f.ideol)
```

### Creating a Dummy Variable

Factoring and recoding permit the creation of dummy variables. A dummy variable is a binary indicator (0 or 1) of some category, to test for an effect from a particular category. Dummy variables are prominent in political science, so it is imperative to understand how to create and use them.

A dummy variable will be created with the recoded `race` variable to indicate whether the respondent identified as African-American (1) or not (0). Recall that in the `r.race` variable, African-American is coded as 2.

```{r 2_dummy1, echo = TRUE}
ds$r.AfAm <- car::recode(ds$r.race, "2 = 1; else = 0; NA = NA")
table(ds$r.AfAm)
```

The newly created dummy variable will now be factored to apply meaningful labels in place of the numbers.

```{r 2_dummy2, echo = TRUE}
ds$f.AfAm <- factor(ds$r.AfAm, levels = c(0, 1), 
                    labels = c("Non African-American", "African-American"))
table(ds$f.AfAm)
```

## Part III: Building and Sorting Your Data

In R, random data can be generated easily into objects and manipulated as desired. There are a variety of methods to accomplish this, the basics of which are explored below. 

The `rnorm()` function generates `n` random values that fit a normal distribution, given a mean and standard deviation.

```{r 2_norm, echo = TRUE}
one <- rnorm(100, mean = 3)
two <- rnorm(100, mean = 7)
three <- rnorm(100, mean = 1)
```

The previous code created three objects consisting of 100 random values with different means. The three objects can be combined into a single column using the `cbind()` function. The `cbind()` function will combine the  given objects sequentially in a column in the provided order. 

```{r 2_cbind, echo = TRUE}
four <- cbind(one, two, three)
```

Alternatively, the `rbind()` function can combine the three objects into a single row. Similar to the `cbind()` function, the `rbind()` function will combine the given objects sequentially in the provided order. 

```{r 2_rbind, echo=TRUE}
five <- rbind(one, two, three)
```

### The Tidyverse

The `tidyverse` is a collection of packages and functions for exploring and visualizing data in R. Developed by Hadley Wickham, the `tidyverse` packages provide a succint and consistent method of data exploration and visualization. There are plenty of different methods of working with your data, but this class will employ the `tidyverse`, as it is considered to be intuitive and rather simple. 

Let's begin with learning how to filter your data. Filtering allows you to create a subset of your data that meets specific criteria. For example, you could filter your data to examine only men, or only Republicans, etc. To filter data, use the `filter()` verb from the `dplyr` package. The `tidyverse` functions are best optimized by using the pipe operator, `%>%`, to pipe functions together. The pipe operator takes whatever is before it and sends it on to the next function. For example:

```{r tidy, echo = TRUE}
ds.men <- ds %>%
  filter(gender == 1)
```

The code chunk above creates a subset of the data that only includes men. The syntax can be read as "First take the data set, then filter it to include men only." We also assigned it to a new object, `ds.men`.  The filter verb can include multiple specifications. You also do not have to always assign your filtered data to a new object. For example, suppose you wanted to examine women who are age 42. You can do so like this:

```{r tidy2, echo = TRUE}
ds %>%
  filter(gender == 0, age == 42)
```

Similar to `filter()`, you can use the `select()` function from the `dplyr()` package to create a new data set that includes a few variables of interest.

__Note__: `select` is a very common verb in R, and therefore is often masked by other packages. If you encounter an error when using `select()`, include `dplyr::` before calling the function, so that R knows which version of `select()` to use. Follow it up by using `na.omit()` to remove missing observations:

```{r tidy3, echo = TRUE}
ds.sub <- ds %>%
  select(gender, age, income, education, ideol, glbcc_cert) %>%
  na.omit()
```


The next verb is `arrange()`, which allows you to sort your data in ascending or descending order by a particular specification. Recall the previous code chunk where we filtered the data to include only women who are age 42. Adding another pipe operator, %>%, and `arrange()` will allow us to examine the data further. Use the `arrange()` function to sort the data by education:

```{r tidy4, echo = TRUE}
ds.sub %>%
  filter(gender == 0, age == 42) %>%
  arrange(education)
```

The `slice()` function selects a set amount of observations from the data. Use `slice()` to select the first 10 observations from the data:

```{r 1tidy5, echo = TRUE}
ds.new <- ds.sub %>%
  slice(1:10)
```

The `mutate()` verb allows you to either change an existing variable or create a new one. For example, we can create a new variable that returns income in increments of 1000s:

```{r 2tidy5, echo = TRUE}
ds.new %>%
  mutate(inc_100 = income / 1000)
```

Suppose you wanted to examine summary statistics for the data. Perhaps you needed to know the average income of everyone in the survey. The `summarize()` verb can be used to do this, and more:

```{r tidy6, echo = TRUE}
ds.sub %>%
  summarize(mean_inc = mean(income))
```

Combining the `summarize()` verb with the `filter()` verb allows you to summarize a more specific set of observations. To find the average income of men only, filter the data to include only men, then summarize:

```{r tidy7, echo = TRUE}
ds.sub %>%
  filter(gender == 1) %>%
  summarize(mean_inc = mean(income))
```


Summarize can also be used to return mulitple values of interest:

```{r tidy8, echo = TRUE}
ds.sub %>%
  filter(gender == 1) %>%
  summarize(mean_inc = mean(income), med_inc = median(income), sd_inc = sd(income))
```


The code chunk above returns the mean, median, and standard deviation of income for men in the data set. 

Filtering for multiple categories can be tedious. Fortunately, there is a tidyverse function that will return desired information for each group in a variable. For example, we can get the mean, median, and standard deviation of the income variable for men and women by using the `group_by()` verb and indicating "gender":

```{r tidy9, echo = TRUE}
ds.sub %>%
  group_by(gender) %>%
  summarize(mean_inc = mean(income), med_inc = median(income), sd_inc = sd(income))
```

The `group_by` verb can be used to find lots of information. Perhaps you wanted to find the average level of belief that humans cause cliamte change by ideology level:

```{r tidy10, echo = TRUE}
ds.sub %>%
  group_by(ideol) %>%
  summarize(mean_glbcc_cert = mean(glbcc_cert))
```


The ideology variable runs from 1 to 7, with 1 being very liberal and 7 being very conservative. It is clear that liberals, on average, are more certain that humans cause climate change. The `group_by()` verb can also be used to look at multiple groups at once. We can create the same table as above, but this time break it down by ideology and gender:

```{r tidy11, echo = TRUE}
ds.sub %>%
  group_by(gender, ideol) %>%
  summarize(mean_glbcc_cert = mean(glbcc_cert))
```

### Other Methods of Exploring Your Data

There are many functions in the tidyverse that can be used for data exploration. By loading the `tidyverse` package at the beginning of the lab, most of these functions should be readily available. The `glimpse()` verb returns an overall breakdown of what is contained in your data set:

```{r tidy12, echo = TRUE}
ds.sub %>%
  glimpse()
```

In the previous section we used `summarize()` to get breakdowns of ceratin variables. If you wanted to get diagnostics on all the variables in your data, use the `skim()` function from the `skimr` package:

```{r tidy13, echo = TRUE}
skim_with(numeric = list(hist = NULL))
skim_with(integer = list(hist = NULL))
ds.sub %>%
  skim()
```

This provides loads of useful incormation. As demonstrated in teh previous section, you can combine different tidyverse functions together in order to maximize efficiency. For example, combining `group_by()` and `skim()` provide breakdowns of all the variables by gender:

```{r tidy14, echo = TRUE}
skim_with(numeric = list(hist = NULL))
skim_with(integer = list(hist = NULL))
ds.sub %>%
  group_by(gender) %>%
  skim()
```

There are a variety of operators that can be used when working with your data. These Boolean operators assist with building data subsets in R:

1. $<$ less than
2. $<=$ less than or equal to
3. $>$ greater than
4. $>=$ greater than or equal to
5. $==$ exactly equal to
6. $!=$ not equal to 
7. $!$ not (example: !x - pronounced "not x" )
8. $|$ or (example: x | y - pronounced "x OR y") 
9. & and (example: x & y - pronounced "x AND y") 

## Working with Nominal Data

Often times data is nominal. This is data that does not necessarily have a numeric value, but rather is categorized by a word or label (e.g., race, political party, etc.). The factored `party` variable is an example.

```{r 2_nom1, echo = TRUE}
table(ds$f.party)
```

If analyzing the data, the "Other" category does not explain much. In this case, recoding the Other responses as NA will exclude Other from summaries.

```{r 2_nom2, echo = TRUE}
ds$f.party.2 <- car::recode(ds$f.party, "'Dem' = 'Dem'; 'Rep' = 'Rep'; 'Ind' = 'Ind';
                            'Other' = NA; else = NA; NA = NA")
table(ds$f.party.2)
```

__Note:__ When recoding a factored variable, label names must be in in apostrophe marks __within the quotation marks__.

### Finding the Mode

When working with nominal data, there are some inapplicable statistics. For example, the mean for the factored political party variable does not exist. However, finding the mode for nominal data is useful. Recall from mathematics that the mode is the value, or in this case the label, that occurs the most often. 

A simple way to find the mode is to use the `count` verb from the `tidyverse`. Use `count` to find the mode of the factored political party variable:
```{r 2_mode, echo = TRUE}
ds %>%
  count(f.party.2)
```

Including `sort = TRUE` will tell R to sort the values in descending order, putting the modal value first:

```{r 2_mode2, echo=TRUE}
ds %>%
  count(f.party.2, sort = TRUE)
```

To get the percentage breakdown of the political parties, use the `count` verb along with the `mutate` verb. To get the percent of each category, you would take the n size of that category and divide it by the total n size!

```{r 2_mode3, echo=TRUE}
ds %>% 
  count(f.party.2) %>%
  mutate(percent = n / sum(n))
```

Perhaps you've noticed that the inclusion of NAs complicates the percentage numbers. NAs can be dropped by using the `drop_na()` function and specifying the variable of interest. Do that, and then construct the same table as above:

```{r 2_mode4, echo=TRUE}
ds %>%
  drop_na(f.party.2) %>%
  count(f.party.2) %>%
  mutate(percent = n / sum(n))
```

### Visualizing Nominal Data

R has nearly countless ways to visualize data. `ggplot2` will be the primary visualization method for these labs. There are many different visualization packages and sets of packages, but `ggplot2` provides a consistent set of visualization tools that shares a common language and syntax. A great introcution to ggplot2 can be found here:
https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

To make a visualization with `ggplot2`, first call it with the `ggplot()` function, then specify what type of visualization you want to make. The steps are:

1. Call ggplot
2. Identify the dataset (`data`)
3. Specify the aesthetic mapping (`aes`)
4. Choose what type of visualization by using the `geom_` function.

As a note, making a visualization with `ggplot2` requires multiple functions. Make sure to include a `+` sign after each function. 

A good place to start is with a bar plot. To do this, use `geom_bar()`. 

```{r 2_bar1, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
 ggplot(ds, aes(f.party.2)) +
  geom_bar()
```

To create a visualization that only includes Democrats, Republicans, and Independents, use the `drop_na` function to filter out NAs, then pipe it to `ggplot2` Color can also be included by specifying `fill = f.party.2`. Note that inlcuding the `.` in place of the data set argument works when you pipe a dataset into `ggplot2`. Doing so tells R to use the data that is being piped into `ggplot2`.

```{r 2_bar1.5, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ds %>%
  drop_na(f.party.2) %>%
  ggplot(., aes(f.party.2, fill = f.party.2)) +
  geom_bar()
```

`ggplot2` has a default color pallete that it assigns to groups based on the order they appear in the data. However, matching the color red with Democrats and the color blue with Republicans might not make a lot of intuitive sense. By using the `scale_fill_manual()` function, specific colors can be specified.

```{r 2_bar2, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ds %>%
  drop_na(f.party.2) %>%
  ggplot(., aes(f.party.2, fill=f.party.2)) +
  geom_bar() +
  scale_fill_manual(values = c("blue", "green", "red"))
```

Construct one more visualization. This time, include a title by using `ggtitle()` and axis labels by using `xlab()` and `ylab()`.

```{r 2_bar3, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ds %>%
  drop_na(f.party.2) %>%
  ggplot(., aes(f.party.2, fill = f.party.2)) +
  geom_bar() +
  scale_fill_manual(values = c("blue", "green", "red")) +
  ggtitle("Breakdown of Political Parties") +
  xlab("Political Party") +
  ylab("Count")
```

## Working with Ordinal Data

Recall that ordinal data is data that is assigned numeric values, but on an ordered scale. An example of ordinal data is education level (e.g., some high school is higher than no high school, a high school diploma is higher than some high school, some college is higher than high school, etc). 

The `count()` function describes the `education` variable. Note that the `table()` function is a good way to look at categorical and ordinal data as well, but using `count()` allows us to see the NAs and is consistent with tidyverse syntax. 

```{r 2_ord, echo = TRUE}
ds %>%
  count(education)
```

The `education` variable contains 8 categories on an ordered scale. Factoring the `education` variable to apply labels will provide meaning to each value.

```{r 2_ord2, echo = TRUE}
ds$f.education <- factor(ds$education, levels=c(1, 2, 3, 4, 5, 6, 7, 8), 
				   labels=c("< HS", "HS/GED", "Vocational/Technical", 
				            "Some College", "2 year/Associates", "Bachelor's Degree", 
				            "Master's degree", "PhD/JD/MD"))
ds %>%
  count(f.education)
```

Including the `sort = TRUE` argument can return the modal education level.

```{r 2_ord3, echo = TRUE}
ds %>%
  count(f.education, sort = TRUE)
```

The `barplot()` function will visualize education levels.

```{r 2_ord4, echo = TRUE, fig.width = 6, fig.height = 4, fig.align = 'center'}
ggplot(ds, aes(f.education)) +
  geom_bar() 
```

Notice the very muddled labels. One way to solve this is to flip the coordinates of the plot by using `coord_flip()`:

```{r 2_ord4.5, echo = TRUE, fig.width = 6, fig.height = 4, fig.align = 'center'}
ggplot(ds, aes(f.education)) +
  geom_bar() +
  coord_flip()
```

More detail is available by adding colors. There are a few different methods of adding colors using ggplot. Include `fill = f.education` to indicate that the bars will be filled with color, and `scale_fill_manual()` to indiciate the colors:

```{r 2_ord5, echo = TRUE, fig.width = 6, fig.height = 4, fig.align = 'center'}
ggplot(ds, aes(f.education, fill = f.education)) +
  geom_bar() +
  scale_fill_manual(values = c("#fbb4ae", "#b3cde3", "#ccebc5", "#decbe4",
                               "#fed9a6", "#ffffcc", "#e5d8bd", "#d3d3d3")) +
  coord_flip()
```

__Note:__ The colors in the previous example are hexadecimal colors. Color schemes are available via the following website: http://colorbrewer2.org/

## Working with Interval Data

Interval data is similar to ordinal data, but with interval data the difference between levels is meaningful. A good example is age, in which the difference between 23 and 24 is the same as the difference between 56 and 57, and so on. 

The `psych` package provides a `describe()` function. The `describe()` function is useful to examine variables.

```{r 2_int, echo = TRUE}
describe(ds$age)
```

Find the modal value of `age`:

```{r 2_int2, echo = TRUE}
ds %>%
  count(age, sort = TRUE)
```

The modal value is 65.

A histogram is the appropriate visualization for interval data. Histograms returns values on a continuous scale as opposed to individual values. To make a histogram, follow the same basic steps as before, but this time use `geom_histogram()` instead.
`
```{r 2_hist, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ggplot(ds, aes(age)) +
  geom_histogram() 
```

To look at the density distribution, use `geom_density()` 

```{r 2_dens, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ggplot(ds, aes(age)) +
  geom_density()
```

To adjust the bandwidth, use `adjust =`  to specify a value.

```{r 2_dens2, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ggplot(ds, aes(age)) +
  geom_density(adjust = 3)
```

Last, a box plot can be generated via the `boxplot()` function. Technically a box plot in `ggplot2` requires both an x and y value, so simply indiciate `""` for the x portion of the aesthetic. 

```{r 2_box, echo = TRUE, fig.width = 4, fig.height = 3, fig.align = 'center'}
ggplot(ds, aes(x = "", y = age)) +
  geom_boxplot()
```

<!--chapter:end:02-describing.Rmd-->

# Visualizing Data, Probability, the Normal Distribution, and Z Scores

This lab discusses the basics of visualizing data, probability, the normal distribution, and z scores. The following packages are required for this lab: 

1. sfsmisc
2. psych
3. car
4. tidyverse

## Histograms and Density

Recall that histograms are used to visualize continuous data. Histograms are not used to visualize categorical data. Instead, a bar plot is advised for categorical data. The `geom_hist()` function creates histograms in R using ggplot visualizations. The following is an example of creating a histogram of the `age` variable within the `ds` data set.

```{r 3_hist1, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_histogram()
```

The histogram displays the frequency of `age` for given bins. Alternatively, the density of `age` can be shown instead of frequency by making a slight change in the visualization. Use the mapping aesthetic inside the `geom_histogram()` function and setting x as the age variable and y as `..density..`

```{r 3_hist2, echo=TRUE}
ggplot(ds) +
  geom_histogram(aes(x=age, y=..density..))
```

The shape of the plot is the same for the frequency and density histograms; however, the y-axis measures in different units. The area associated with the largest y-axis value suggest that a higher percentage of respondents are likely to provide an age within the ages on the x-axis.

Data is organized into ranges, known as bins, to compose the x-axis. The number of bins is a potentially contentious topic; however, a good recommendation is to set the number of bins equal to $\sqrt n$, where n is the number of observations. To change the number of bins, use `bins=n` inside the `geom_histogram()` function. The square root of n for the current data set is a little over 50, so set the bins to be 50.

```{r 3_hist3, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_histogram(bins=50)
```


Using various functions along with the histogram function, the visualization is improved with more meaningful information. These functions can help:


1. xlab("X-Axis Label")
    - Sets the label for the x-axis
2. ylab("Y-Axis label")
    - Sets the label for the y-axis
3. ggtitle("Title")
    - Sets the histogram title
4. coord_cartesian(ylim=c(min:max), xlim=c(min:max))
    - Sets the limits of the x and y axes.

The following is an excellent example of a histogram of the `age` data.

```{r 3_hist5, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_histogram(bins=50) + 
  xlab("Age") +
  ylab("Frequency") +
  ggtitle("Histogram of Age") +
  coord_cartesian(ylim=c(0,175), xlim=c(15,95)) +
  theme_light() # Sets the theme. There are a lot to choose from.
```

### Normal Distribution and Histograms

Data approximated by the normal distribution can define probabilities. Using R, the normal distribution "bell curve" can be projected over a histogram.

Given an identified mean and standard deviation, and a density histogram, the `stat_function()` function can project a normal distribution as follows. Specify `fun=dnorm`. 

```{r 3_norm, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_histogram(aes(x=age, y=..density..), bins=50) +
  stat_function(fun=dnorm, args = list(mean=mean(ds$age), sd=sd(ds$age)), color="red")
```

Comparing the histogram plot to the normal distribution curve generated may prove difficult. The `geom_density()` function can draw a line using density data for _age_ alongside the projected line of what the normal distribution would appear like given the mean and standard deviation. The two shapes can then be compared visually to interpret whether the _age_ data can be approximated by the normal distribution. 

```{r 3_norm2, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_histogram(aes(x=age, y=..density..), bins=50) +
  stat_function(fun=dnorm, args = list(mean=mean(ds$age), sd=sd(ds$age)), color="red") +
  geom_density(color="blue")
```

The culmination of the histogram, curve, and density line is improved via the addition of limits and labels to the x-axis and y-axis, defining a number of bins, and a chart title. Including fill and outline colors for the histogram can also make it more readable:

```{r 3_norm3, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_histogram(aes(x=age, y=..density..), bins=50, fill="#d3d3d3", color="black") +
  stat_function(fun=dnorm, args = list(mean=mean(ds$age), sd=sd(ds$age)), color="red") +
  geom_density(color="blue") +
  ggtitle("Histogram of Age") +
  xlab("Age") +
  ylab("Density") +
  theme_bw() +
  coord_cartesian(xlim=c(15,95), ylim=c(0,0.04))
```


## Probability and Distributions

R supports a number of distributions; however, for the purpose of these labs we will focus primarily on the normal and binomial distributions. View the `help(Distributions)` documentation to explore the distributions supported by R.

help(Distributions)

The following R functions are applicable to the normal distribution:

1. `dnorm()`
2. `pnorm()`
3. `qnorm()`
4. `rnorm()`

The `dnorm()` function provides the height of a probability distribution function at a given x value: `dnorm(x, mean=$\mu$, standard deviation=$\sigma$)`. __Note:__ The value returned by the `dnorm()` function is not the probability associated with the occurrence of the x value!

The default mean and standard deviation for the `dnorm()` function is 0 and 1, respectively. The following example finds the height of the probability distribution function at $x=2$ with $\mu = 4$ and $\sigma = 1.5$.

```{r 3_dnorm, echo=TRUE}
dnorm(2, mean = 4, sd=1.5)
```

The `dnorm()` function used in conjunction with the `age` variable from `ds` data set can find the height of the probability distribution function. In the following example, the `dnorm()` function will find the height of the probability distribution function for 65. Similar to previous examples, an argument exists to ignore NA and missing values.

```{r 3_dnorm2, echo=TRUE}
dnorm(65, mean=mean(ds$age, na.rm = T), sd=sd(ds$age, na.rm=T))
```

The `dnorm()` returns the height of the probability distribution function as 0.027. __Note:__ This is a random value and, by itself, is not meaningful. The `dnorm()` function returns the relative likelihood, which can lead to determining a probability; however, to understand this value further requires an explanation of calculus.

For continuous data, the probability of a single value is small (near zero), so instead the approach should be to find the probability a value occurs within a specified range. The probability associated to a value occurring within a specified range is equal to the area of the probability distribution function between the two points. In calculus this is defined as finding the integral of the probability distribution function.

$$\int_{-x_1}^{x_2}{f_X(t)\,dx}$$

The above formula is the cumulative distribution function for two points, $x_1$ and $x_2$. In this case, $x_1$ is defined as the lower bound and $x_2$ is defined as the upper bound.

R includes the calculus function `integrate.xy()` to return the probability.

```{r 3_dnorm3, echo=TRUE}
integrate.xy(density(ds$age)$x,density(ds$age)$y,65,66)
```

The probability associated to an age between 65 and 66 in the `age` variable is .029 ($\approx$ 3% chance).

Similarly, the `pnorm()` function calculates probabilities associated to a given x value. The default for the `pnorm()` function is the cumulative distribution function with a lower bound of $-\infty$ and an upper bound of x. 

The following example calculates the probability associated to an age value between $-\infty$ and 5, given $\mu = 6$ and $\sigma = 6$.

```{r 3_pnorm, echo=TRUE}
pnorm(5, mean = 6, sd = 2)
```

The `pnorm()` calculates the probability of observing a value between $-\infty$ and 5 as 0.31. The following example uses the `pnorm()` function with the `ds` data set to find the probability that a respondent is 65 or less years old.

```{r 3_pnorm2, echo=TRUE}
pnorm(65, mean=mean(ds$age, na.rm=T), sd=sd(ds$age, na.rm = T))
```

To calculate the probability associated to an age of 65 or greater, the `lower.tail = FALSE` argument will look at the upper tail (right side of the probability distribution function). This is equal to the difference between 1 and the lower tail probability previously calculated.

```{r 3_pnorm3, echo=TRUE}
pnorm(65, mean=mean(ds$age, na.rm = T), sd=sd(ds$age, na.rm = T), lower.tail = FALSE)
```

The `qnorm()` function is the inverse function of the `pnorm()` function. Given a probability, mean, and standard deviation, the `qnorm()` function will return an x value from the probability distribution function. The following example finds the upper bound x value of the probability distribution function associated to the probability, or area under the curve, of 0.3 given $\mu = 5$ and $\sigma = 1$.

```{r 3_qnorm, echo=TRUE}
qnorm(.3, mean=5, sd=1)
```

The following calculates the upper bound age from the `age` variable in the `ds` data set to demonstrate further associated to a 40% probability. That is, the `qnorm()` function calculates the age that 40% of respondents are equal or less to.

```{r 3_qnorm2, echo=TRUE}
qnorm(.4, mean=mean(ds$age, na.rm = T), sd=sd(ds$age, na.rm = T))
```

Lastly, the `rnorm()` function will generate random values that follow a normal distribution given a number of points (n), provided $\mu$, and $\sigma$. The following example calculates 200 random values given $\mu = 6$ and $\sigma = 2$. The random values are stored to the `rvalues` object.

```{r 3_rnorm, echo=TRUE}
rvalues <- rnorm(200, mean=6, sd=2)
ggplot() +
  geom_histogram(aes(rvalues), bins=14)

```

__Note:__ The discussed functions are relevant to the normal distribution functions provided by R. R includes similar functions for other distributions, with equivalent functionality. 

## Visualizing Normality

Thus far the normal distribution has been discussed without visualization. When graphed, data that follow a normal distribution resemble a bell shaped curve. To demonstrate, the following code employs the `rnorm()` function to generate 1000 random values with $\mu = 100$ and $\sigma = 20$, and assigns the values to an object named `random`.

```{r 3_three, echo=TRUE}
random <- rnorm(1000, mean = 100, sd = 20)
```

Inspecting a density histogram of the `random` object yields a bell shaped curve.

```{r 3_three2, echo=TRUE}
ggplot() +
  geom_histogram(aes(random, ..density..), fill="white", color="black")
```




Let's start figuring out how to check if our data is normally distributed. There are many packages than will generate a density curve of your data and a projected normal distribution for comparison, but building all of the visualizations in ggplot provides both an intuitive and informative method of doing so. Start by creating a density plot of the randomly generated data. 

Let's start figuring out how to check if our data is normally distributed. At the beginning of this lab we should have installed and loaded the package "sm". We use the `sm.density()` function to visualize data and to project what should be the normal distribution of the data, given the mean and standard deviation of the data. Let's start with our randomly generated data. We include the `model="Normal"` argument at the end of the function, so that `sm.density` knows to use the normal distribution.


```{r 3_three4, echo=TRUE}
ggplot() +
  geom_density(aes(random))
```

Given the `random` values consists of values generated by the `rnorm()` function, this distrubtion resembling the normal distribution is unsurprising. 

The following code generates a density line for the `age` variable from the `ds` data set and a projected normal distribution given the mean and standaard deviation of the variable. Indicating `alpha=.5` will make the line slightly transparent. 

```{r 3_three5, echo=TRUE}
ggplot(ds, aes(age)) +
  geom_density(aes(ds$age)) +
  stat_function(fun=dnorm, args=list(mean=mean(ds$age), sd=sd(ds$age)), color="red", size=2, alpha=.5)
```

The shape of the density line closely resembles a normal distribution; however, note the slight skew. 

Another method to view whether data follows a normal distribution is to view the QQ plot, available via the `qqPlot()` function. 

A QQ plot visualizes data based on the quantiles of the provided variable against the quantiles that would exist if the data were normally distributed. Data that follows the normal distribution should be in a line with a set slope. Including `stat_qq()` generates a QQ plot. The following example generates a QQ plot of the `age` variable.

```{r 3_three6, echo=TRUE}
ggplot(ds, aes(sample=age)) +
  stat_qq()
```


To further inspect the normality, a diagonal line can be generated that will visualize what the slope of the data should be if it were normally distributed. To include such a line, use `geom_abline()`. The slope and intercept of the line must also be calculated. Doing so requires a small amount of basic linear algebra. Find the first and third quantiles of the `age` variable and set it to an object named `y`, then find the theoretical first and third quantiles for normally-distributed data and set it to `x`. Calculate the slope by taking the diference in `y` over the difference in `x` and set that to an object named `slope`. Then solve for the intercept and it it to an object named `intercept`. 

```{r 3_three7, echo=TRUE}
y     <- quantile(ds$age, c(0.25, 0.75)) # Find the 1st and 3rd quantiles
x     <- qnorm( c(0.25, 0.75))           # Find the matching normal values on the x-axis
slope <- diff(y) / diff(x)               # Compute the line slope
int   <- y[1] - slope * x[1]             # Compute the line intercept

ggplot(ds, aes(sample = age)) +
  stat_qq() + 
  geom_abline(intercept = int, slope = slope, color = "blue", size = 2, alpha = 0.5 )
```


In the graphic above, the solid blue line exhibits where data should fall if it follows a normal distribution, and the blue dash lines represent confidence intervals. The individual circles represent data points from the variable. If the data points is within the intervals, then the data likely follows a normal distribution. The interpretation of this QQ plot yields that the data likely follows a normal distribution, as expected given the data was generated via the `rnorm()` function. 

The QQ plot confirms the sm.density() plot: the `age` variable closely follows a normal distribution. Note that some values are outside the confidence interval. These are the points associated to the skew previously observed. 

Another last method to inspect whether data follows a normal distribution is the box plot. Box plots provides quartiles and the median, and returns individual unique values at the edges of our data. The following code generates a box plot of the `age` variable.

```{r 3_three8,echo=TRUE}
ggplot(ds, aes(x="", y=age)) +
  geom_boxplot()
```

Note the middle line is the middle quartile, or the median. The distance between the median line and the line below it represents the second quartile. The distance from the bottom of the second quartile line to the very bottom line, or "whisker" is the first quartile. Above the median, the distance between the median line and the next line above it is the third quartile, and the area above that, and below the top whisker, is the fourth quartile. Notice that for the box plot of our randomly generated data, the distance between quartiles is relatively even. 

The distance between quartiles is relatively even; however, note the difference to the previously generated box plot from the `rnorm()` data. 

## Z-Scores

Standardizing, or scaling, data provides conveniences in discussing data. For instance, discussing how many standard deviations a particular value occurs from the mean is more meaningful than purely the distance. Scaling data in terms of z-scores provides the number of standard deviations a value is from the mean.

The following example employs the `scale()` function to calculate z-score for each data point and assigns them to a newly created `z.age` variable in the `ds` data set. To do this, use the `mutate()` function, which is a tidyverse function that creates new variables or modifies existing variables. The `scale()` function is enclosed by the `c()` function to ensure the result is a 1 dimensional vector: 

```{r 3_z, echo=TRUE}
ds <- ds %>%
  mutate(z.age = c(scale(age)))
```

Using a filter approach, the following example finds the z-score associated to women younger than 19 years old. First filter the data with the preferred stipulations, then use the `select()` verb from the `dplyr()` package (part of the tidyverse!) to view the results. All of this is connected using the pipe function, %>%.

```{r 3_z2, echo=TRUE}
ds %>% 
  filter(f.gender == "Women" & age < 19) %>%
  dplyr::select(f.gender, age, z.age)
```

The result shows that, within the `ds` data set, there is one respondent that identified as a woman under 19 years old. The z-score for this respondent is -2.98, which is interpreted as this respondent's age is 2.98 standard deviations below the mean.

Given a z-score, the mean age of respondents is assumed as much higher than the 18 year old woman. The difference between the mean age of respondents and the woman is the product of the z-score and standard deviation. The following calculates the standard deviation of the `age` variable.

```{r 3_z3, echo=TRUE}
sd(ds$age, na.rm = TRUE)
```

The `sd()` function returns the standard deviation of data. The product of the `sd()` function and calculated z-score, is the difference between the 18 year old woman and mean age of respondents.

```{r 3_z4, echo=TRUE}
2.981748*sd(ds$age, na.rm = TRUE)
```

The difference between the 18 year old woman and mean age of respondents is $\approx$ 43.38 years.

<!--chapter:end:03-visualizing.Rmd-->

# Foundations for Inference

This lab introduces the tools for the foundation of inference by examining the normal distribution, standard errors, confidence errors, and single sample t-tests. The following packages are required:

1. car
2. psych
3. sm
4. HistData
5. tidyverse

## Testing for Normality

Lab three introduced the basics of normal distributions. Much of the statistical work done is built on the condition of data that follows a normal distribution. Inspecting data for normality is done via numerous methods.

Recall the `rnorm()` function to generate random values that follow a normal distribution given specified parameters. The following `random` object will consist of 1000 random values given $\mu$ = 50 and $\sigma$ = 5:

```{r 4_norm1, echo=TRUE}
random <- rnorm(1000, mean = 50, sd = 5)
```

Visualizing data is an important first step to inspecting data for normality. The previous lab introduced density plots, box plots, and QQ plots as means of visualizing data and their relationships to a normal distribution. Recall that the QQ plot graphs the quantiles of data against quantiles of a normal distribution, given the $\mu$ and $\sigma$ of the data.

```{r 4_qq, echo=TRUE}
ggplot(as.data.frame(random), aes(sample = random)) +
  stat_qq() +
  stat_qq_line()
```

The QQ plot of the `random` object demonstrates the data closely follows a normal distribution. This is expected given the `random` object was created via the `rnorm()` function. __Note:__ Visualizing data for normality is an informal approach to inspecting for normality. Various empirical methods exist to test whether data follows a normal distribution, the most popular of which are:

1. Shapiro-Wilk test
2. Anderson-Darling test
3. Kolmogorov-Smirnov test
4. Pearson's Chi-Squared test

The Shapiro-Wilk test is the most popular method, as the test has been demonstrate as providing the most power for a given significance.

### Shapiro-Wilk Test

The `shapiro.test()` function in R employs the Shapiro-Wilk test on data to test whether the data are normally distributed. Use of the Shapiro-Wilk test is contingent on univariate and continuous data. The hypotheses for the Shapiro-Wilk test are:

$H_0$: The data are normally distributed  
$H_1$: The data are not normally distributed

__Note:__ The Shapiro-Wilk test for normality is a statistical test that provides a p-value of the test statistic, `W`. This lab will focus on the p-value approach for statistical tests, using an $\alpha$ value of 0.05 as the desired significance level.

```{r 4_sw test, echo=TRUE}
shapiro.test(random)
```

The Shapiro-Wilk test p-value is greater than $\alpha$ = 0.05, therefore failing to reject $H_0$ concluding the data are normally distributed. Again, this is expected given the `random` object was created via the `rnorm()` function.

### Testing Normality 

R provides data pertaining to Yellowstone National Park's Old Faithful geyser in the `faithful` object. Old Faithful eruptions are recorded as duration, in minutes, between events. First, the `describe()` function provides valuable information for the `eruptions` variable in the `faithful` object.

```{r 4_faith, echo=TRUE}
describe(faithful$eruptions)
```

Comparing the `eruptions` data to a normal distribution, given $\mu$ and $\sigma$ from the `eruptions` data, is available via the *geom_density()* and *stat_function()* functions.

```{r 4_faith2, echo=TRUE}
ggplot(faithful, aes(eruptions)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(faithful$eruptions), 
                                         sd = sd(faithful$eruptions)), color = "blue")
```

The black line represents the `eruptions` data, and the blue line represents the normal distribution given the $\mu$ and $\sigma$ values of `eruptions`. __Note:__ The `eruptions` data appears bimodal, and does not fit the normal distribution model given the parameters calculated via the `eruptions` data. 

The `eruptions` data is further examined using QQ plots via the `qqPlot()` function:

```{r 4_faith4, echo=TRUE}
ggplot(faithful, aes(sample = eruptions)) +
  stat_qq() +
  stat_qq_line()
```

Most of the points in the QQ plot fall outside the region defined by the dashed lines, further suggesting the `eruptions` data is likely not normally distributed. Lastly, a Shapiro-Wilk test can confirm whether the `eruptions` data is normally distributed:

```{r 4_faith3, echo=TRUE}
shapiro.test(faithful$eruptions)
```

The Shapiro-Wilk test p-value is less than $\alpha$ = 0.05, leading to reject $H_0$: data are normally distributed. In conclusion, the `eruptions` data is not normally distributed. __Note:__ The visual plots are likely enough to confirm the `eruptions` data are not normally distributed.

The `MacdonellDF` variable within the `HistData` package consists of finger length data. Again, visualizing the data for normality is performed via various methods. A box plot is generated for the `MacdonellDF` data:

```{r 4_f, echo=TRUE}
ggplot(MacdonellDF, aes(x = "", y = finger)) +
  geom_boxplot()
```

The box plot appears balanced, indicating normality. Generating a density plot should show the distribution of `MacdonellDF` variable as similar to the projected normal distribution given the $\mu$ and $\sigma$ parameters of the `MacdonellDF` variable.

```{r 4_f2, echo=TRUE}
ggplot(MacdonellDF, aes(finger)) +
  geom_density() +
  stat_function(fun = "dnorm", args = list(mean = mean(MacdonellDF$finger), 
                                                       sd = sd(MacdonellDF$finger)),
                color = "red", size = 1.5,
                alpha = .3)
```

Additionally, a QQ plot:

```{r 4_f3, echo=TRUE}
ggplot(MacdonellDF, aes(sample = finger)) +
  stat_qq() +
  stat_qq_line()
```

The visualizations all suggest the data is normally distributed; however, a Shapiro-Wilk test is still useful to test for normality.

```{r 4_f4, echo=TRUE}
shapiro.test(MacdonellDF$finger)
```

__Note:__ Despite the visualizations suggesting normality, the Shapiro-Wilk test p-value is less than $\alpha$ = 0.05, resulting in rejecting the null hypothesis that the data are normally distributed. A closer examination of the QQ plot yields that most points exist outside the region, though at first glance nothing appears suspect. This merits a conversation on judgment for assessing whether the data could be treated as normally distributed.

##  Standard Errors

Recall that the standard error is the standard deviation of the sample distribution, calculated as the square root of the standard deviation divided by the square root of the sample size.

- Population: $\frac{\sigma}{\sqrt{n}}$
- Sample: $\frac{s}{\sqrt{n}}$

R does not provide a single purpose function to calculate the standard error. As a result, the following demonstrates the previous formula to calculate the standard error:

```{r 4_se, echo=TRUE}
sd(ds$age, na.rm = T)/sqrt(length(ds$age) - sum(is.na(ds$age)))
```

Alternatively, the `describe()` function includes the standard error statistic for a given variable, as follows:

```{r 4_se2, echo=TRUE}
describe(ds$age)
```

__Note:__ The previous two methods return the same result.

## Confidence Intervals

The standard error is vital to inferential statistics. For example, the standard error is required to calculate confidence intervals. Confidence intervals employ standard error to assist with inferring information from a sample of a larger population, through inclusion of uncertainty.

Confidence intervals synthesize  knowledge of standard error and z-scores. Recall that with the normal distribution, a z-score standardizes standard deviations values are from the mean. To calculate a level of confidence (90%, 95%, 99%), the z-score associated with those levels of confidence is necessary:

 - 90% confidence = 1.645
 - 95% confidence = 1.960
 - 99% confidence = 2.576
 
The formula for confidence level is:

$$CI=\bar{x} \pm z\frac{s}{\sqrt{n}},$$ 
or

$$CI=\bar{x} \pm z*SE$$

Put simply, the confidence interval is the sample mean plus/minus the product of the z-score and standard error. 

Using the `age` variable within the `ds` data set, the a 95% confidence interval is calculated as follows: 

First calculate the mean:

```{r 4_ci1, echo=TRUE}
mean(ds$age, na.rm = T)
```

Second, find the standard error:

```{r 4_ci2, echo=TRUE}
describe(ds$age)$se # this is a way to pull the standard error out without having to look at everything else
```

Given a 95% confidence interval, the z-score is 1.96. The upper bound of the confidence interval is calculated as follows:

```{r 4_ci3, echo=TRUE}
60.38 + 1.96 * 0.28
```

The lower bound of the confidence interval is calculated as follows:

```{r 4_ci4, echo=TRUE}
60.38 - 1.96 * 0.28
```

With 95% confidence, the population mean of `age` is between 59.83 and 60.93 (rounded).

Increasing confidence will increase the interval of the confidence interval. For example, the z-score associated to 99% is 2.58, so the confidence interval for 99% is calculated as follows:

```{r 4_ci5, echo=TRUE}
age.se <- describe(ds$age)$se
```

__Note:__ The standard error of the `age` variable was stored as to the `age.se` object via specifying only to return the standard error from the `describe()` function. Using the `se` object simplifies the confidence interval calculation as exemplified:

```{r 4_ci6, echo=TRUE}
mean(ds$age, na.rm = T) + 2.58 * age.se
mean(ds$age, na.rm = T) - 2.58 * age.se
```

With 99% confidence, the population mean of `age` is between 59.64 and 61.09. This interval is larger than the interval calculated for 95% confidence.

Alternatively, the `t.test()` function in R provides the 95% confidence interval for a given variable.

```{r 4`ci7, echo=TRUE}
t.test(ds$age)
```

The results of _t.test()_ is similar to the manual calculations performed previously. By default, the _t.test()_ tests whether $\mu$ for the variable is equal to zero. The hypotheses for the _t.test()_ function are:

- $H_0: \bar{x} = \mu$
- $H_1: \bar{x} \neq \mu$

Note the returned p-value of the previous `t.test()` performed is less than $\alpha$ = 0.05; therefore, the null hypothesis is rejected. That is, given the sample data, with 95% confidence the $\mu$ is not zero.

To explain a different way, using the same variable, but with a defined $\mu$ of 50, the `t.test()` function can test whether $\mu$ is 50.

```{r 4_ci8, echo=TRUE}
t.test(ds$age, mu = 50)
```

The same result occurs: the null hypothesis is rejected. Once again, the `t.test()` function is employed to test whether $\mu$ = 60:

```{r 4_ci9, echo=TRUE}
t.test(ds$age, mu = 60)
```

The result differs, such that the null hypothesis is not rejected as the p-value is greater than $\alpha$ = 0.05. That is, with 95% confidence, $\mu$ = 60.

__Note:__ The p-values can be calculated manually from z-scores to test population means using confidence intervals. This requires:

1. Calculate $\bar{x}$
2. Calculate confidence intervals (as previously shown)
3. Calculate the p-value associated with $H_0$

Objects are employed to simplify the calculation:

```{r 4_p, echo=TRUE}
xbar <- mean(ds$age, na.rm = T)
a <- 60
s <- sd(ds$age, na.rm = T)
n <- 2547
```

The p-value for if the population mean $\neq$ 60:

```{r 4_p2, echo=TRUE}
2 * (1 - pnorm(xbar, mean = a, sd = s/sqrt(n)))
```

The p-value of 0.191 is greater than $\alpha$ = 0.05; therefore, failing to reject $H_0$: $\mu$ = 60.

## More on Single Sample T-tests

The previous confidence interval section introduced a preliminary discussion of t-tests. The Student's t distribution is based on sample estimates, whereas the normal distribution is used when $\sigma$ and $\mu$ are known. 

Performing a t-test requires:

1. n
2. s
3. $\bar{x}$
4. t statistic

To demonstrate, the following tests whether the population mean is 58 for the given data by defining the above parameters:

```{r 4_t test, echo=TRUE}
nAge <- length(ds$age) - sum(is.na(ds$age))
sdAge <- sd(ds$age, na.rm = TRUE) # Standard deviation of age
seAge <- sdAge/(sqrt(nAge)) # Standard error of age
meanAge <- mean(ds$age, na.rm = TRUE)
```

To calculate the t statistic, the population mean is subtracted from the sample mean, and divided by the standard error as follows: 

```{r 4_t test2, echo=TRUE}
t_value <- (meanAge - 58)/seAge   
t_value
```

To calculate the two-sided p value under the t distribution:

```{r 4_t test3, echo=TRUE}
2 * (1 - pt(abs(t_value), df = nAge - 1))

```

The returned p-value is less than $\alpha$ = 0.05, rejecting $H_0$: $\mu$ = 60. Put another way, $H_1$:  
$\mu \neq 60$.

```{r 4_t test4, echo=TRUE}
t.test(ds$age, mu = 58)
```

With 95% confidence, $\mu \neq 60$

<!--chapter:end:04-foundations.Rmd-->

# Inference for Two Populations

This lab covers the basics of inference for two populations. We go through proportions and cross tabulations; the different types of two sample t-tests; difference in one and two tailed tests; and how to plot means and the differences between means. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
4. vcd
5. reshape2
6. skimr

## Proportions

To start with proportions, We will use the `glbcc` variable from the class dataset that contains respondents' opinions on global climate change. Specifically, it asks if they believe that greenhouse gases cause global temperatures to increase. We start with describing the data. A zero indicates "no" and 1 indicates "yes."

```{r 5_prop, echo=TRUE}
ds %>%
  count(glbcc)
```

Now we can describe the population proportions.

```{r 5_prop2, echo=TRUE}
ds %>%
  count(glbcc) %>%
  mutate(prop = n / sum(n))
```

The above table describes the proportions of the population that believe humans cause climate change. Let's visualize the proportions. 

Data frames are required for ggplot visualizations. Sometimes you have to construct the data frame manually, using the `data.frame()` function and creating vectors inside it with `c()`. However, when working with tidyverse functions, there are often shortcuts. Below we are able to visualize the proportion table by simply piping it directly to `ggplot2` and putting a `.` in place of the data set.

```{r 5_prop3, echo=TRUE}
ds %>%
  count(glbcc) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(., aes(glbcc, prop)) +
  geom_bar(stat = "identity")
```

As we learned in the last lab, there are uncertainties associated to point estimates. We can include confidence intervals to get a range of values for which we are confident (at some level) the value actually falls between. To calculate confidence intervals we need to find the standard error of the proportionn and assign it to an object that we will name `se.gcc`.

```{r 5_prop4, echo=TRUE}
se.gcc <-sqrt((0.5713 * (1 - 0.5713)/2547)) 
se.gcc
```

Alternatively, we can use the `describe()` function to avoid rounding errors:

```{r 5_prop5, echo=TRUE}
se.gcc <- describe(ds$glbcc)$se
se.gcc
```

With the standard error object, we can calculate the confidence intervals. Here we will calculate the following: the upper bounds and lower bounds for the yes and no confidence intervals.

$$CI=\hat{p} \pm z\frac{s}{\sqrt{n}},$$

Where $\hat{p}$ is the sample proportion. Recall that 95% confidence corresponds to a 1.96 z-score.

The proportion table from earlier provides the values we will use:

```{r 5_prop6, echo=TRUE}
ds %>%
  count(glbcc) %>%
  mutate(prop = n / sum(n))
```

First store the standard error as an object:

```{r 5_prop7, echo=TRUE}
se.gcc <- se.gcc <- describe(ds$glbcc)$se
```

Recall that the `mutate()` verb can be used to create multiple variables at once. With this in mind, we can easily create a data frame that contains the measurements of interest, the standard error, and the lower and upper bounds of the confidence interval. For a 95% confidence interval, you take the proportion, plus or minus 1.96 mulipltied by the standard error. Inside the `mutate()` function create three new variables: the standard error, lower, and upper:

```{r 5_prop71, echo=TRUE}
new.ds <- ds %>%
  count(glbcc) %>%
  mutate(prop = n / sum(n),
         se.gcc = se.gcc,
         lower = prop - 1.96 * se.gcc,
         upper = prop + 1.96 * se.gcc)
new.ds
```


With the new data frame we can create a visualization that includes the confidence intervals. The code is similar to the previous `ggplot2` code used, but now with the addition of the `geom_errorbar()` function. The `geom_errorbar()` function requires arguments to use the lower and upper bound values for the confidence intervals:

```{r 5_prop10, echo=TRUE}
ggplot(new.ds, aes(x = glbcc, y = prop)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper))
```

### Two Populations

Research is often interested in differences between distinct populations. Using the `glbcc` variable, we will review responses differentiated by gender, using the `group_by()` function. 

```{r 5_2prop, echo=TRUE}
ds %>%
  group_by(f.gender) %>%
  count(glbcc) %>%
  drop_na() %>%
  mutate(prop = n / sum(n)) 
```


To visually examine which gender has, on average, higher belief that greenshouse gases cause climate change, plotting the two mean levels of belief for the genders is a good place to start. To create a visualization, the mean level of `glbcc` needs to be found for each gender. In this case, the proportion of each gender believing in climate change is the same as the "mean" amount of men or women believing in climate change. The following code returns a new data frame that is just like the previous one, but filters the data to include only responses indicate a belief that greenhouse gases cause climage change. 

```{r 5_2prop2, echo=TRUE}
ds %>%
  group_by(f.gender) %>%
  count(glbcc) %>%
  drop_na() %>%
  mutate(prop = n / sum(n)) %>%
  filter(glbcc == 1)
```

Similarly to earlier in the lab, confidence intervals can also be calculated by finding the standard error. In this case, we need to find the standard error for men and women separately. We will use the `filter()` function to simplify the calculations:

```{r 5_2prop3, echo=TRUE}
male <- filter(ds, gender==1)
female <- filter(ds, gender==0)
m.se <- describe(male$glbcc)$se
f.se <- describe(female$glbcc)$se
```

Now that we have the standard errors for men and women, we can return to the previous data frame we constucted and add a column for the standard error. This is done using the tidyverse verb `add_column()`. Assign the new data frame to a new object, then print the data frame:

```{r 5_2prop4, echo=TRUE}
df2 <- ds %>%
  group_by(f.gender) %>%
  count(glbcc) %>%
  drop_na() %>%
  mutate(prop = n / sum(n)) %>%
  filter(glbcc == 1) %>%
  add_column(se = c(m.se, f.se))
```

Now construct the visualization. Like the previous visualization, use `geom_errorbar()` to construct the confidence intervals. However, this time you will need to calculate them inside the `geom_errorbar()` function itself!

```{r 5_2prop8, echo=TRUE}
ggplot(df2, aes(f.gender, prop)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = prop - 1.96 * se,
                ymax = prop + 1.96 * se))
```

Suppose we wondered whether women believe humans cause climate change more than men: this visualization provides only a partial answer. By the "eye test," the visualization appears to show that women have a higher value than men, and furthermore the confidence intervals do not overlap; however, the eye test alone is insufficient. An empirical test is required.

To start, we formulate the following hypotheses:  
$H_0$: there is no difference between genders  
$H_1$: there is a difference between genders  

We can use a two sample t-test to test these hypotheses. Using the different data sets created earlier for genders and the `glbcc` variable we will find the 95% confidence interval, p-value, and point estimate.

```{r 5_2prop9, echo=TRUE}
t.test(male$glbcc, female$glbcc)
```

The t-test yields a p-value < $\alpha$ = 0.05, thereby the null hypothesis is rejected to conclude there is a statistical significance in responses by gender. Further, the point estimate calculated as 0.074 informs us 7% more women than men believe humans cause climate change. __Note:__ The confidence interval tells us that, with 95% confidence, the difference between women and men is between 3% and 11%. Judgment is required to determine whether gender difference is substantive.

## Cross Tabulations

Another way to examine the difference of gender and beliefs about climate change is cross tabulation. Cross tabulations describe relationships between two variables. The basic building block of cross tabulations are tables, a skill acquired in previous labs.

For this section, we will use the `glbcc_risk` variable, that measures the level of risk respondents associate with climate change (on a scale of zero to ten). The range associated to this scale is simplified to zero to five using the `recode()` function:


```{r 5_ct1, echo=TRUE}
ds$r.gccrsk <- car::recode(ds$glbcc_risk, "0:1=1; 2:3=2; 4:6=3; 7:8:=4; 9:10=5")
table(ds$r.gccrsk)
```

Next the variable is separated by gender using the `table()` function. The dependent variable, `r.gccrsk`, is specified followed by the independent variable, `f.gender`:

```{r 5_ct2, echo=TRUE}
gcc.table <- table(ds$r.gccrsk, ds$f.gender)
gcc.table
```

The `prop.table()` function describes the relationship by proportions. We convert the proportion to percentage of each response level by gender by including `margin=2`.

```{r 5_ct3, echo=TRUE}
gcc.table %>% prop.table(margin = 2) * 100
```

There appears to be a difference in genders; however, as these differences are within a sample we cannot infer there is a difference in the population without an empirical test. We will use the Chi-Square test to empirically test whether there is a statistically significant difference in genders. The `chisq.test()` function performs the Chi-Square test given on a table:

```{r 5_ct4, echo=TRUE}
chisq.test(gcc.table)
```

__Note:__ The `summary()` function will provide additional information about this table, independent of a Chi-Square test.

```{r 5_ct5, echo=TRUE}
summary(gcc.table)
```

Given the Chi-Square test p-value < $\alpha$ = 0.05, the null hypothesis is rejected such that there is a statistically significant difference between gender and perceived risk of climate change. Substantive difference is as important as statistical significance, and a variety of methods exist to test the strength of relationships. For the Chi-Square test, finding Cramer's Vs is the appropriate method. The `assocstats()` function will return a variety of coefficients and numbers, including Cramer's V.


```{r 5_ct6, echo=TRUE}
assocstats(gcc.table)
```

Cramer's V is a score ranging from 0 to 1, whereby 0 indicates a weak association and 1 indicates strong association. The Cramer's V score returned for the previous test is quite small, indicating a weak association.

Perhaps we are interested in gender as a control variable, with ideology as the independent variable. A new table for perceived risk of climate change, ideology (conservative, moderate, liberal), and gender is as follows:

```{r, ct7, echo=TRUE}
gcc.table2 <- table(ds$r.gccrsk, ds$f.ideology, ds$f.gender)
gcc.table2
```

Again, a Chi-Square test and Cramer's V will provide insight. To separate the genders, [,,1] is used for male and [,,2] is used for female.

For male:

```{r 5_ct8, echo=TRUE}
chisq.test(gcc.table2[,,1])
assocstats(gcc.table2[,,1])
```

For female:

```{r 5_ct9, echo=TRUE}
chisq.test(gcc.table2[,,2])
assocstats(gcc.table2[,,2])
```

Both chi-square tests return significant results, and both Cramer's V scores are about .4. The interpretation of this Cramer's V score is clearly stronger than the previous score; however, the score itself is not judged as a strong association.

### Other Coefficients

There are other coefficient scores that are appropriate to use in certain situations. The Phi coefficient is used with a 2x2 contingency table. The contingency coefficient, C, is used for square tables. Keep these different methods in mind when doing cross-tabulations and chi-square tests.

## Independent t-tests

This section elaborates on t-tests. So far we have emphasized t-tests, as the Student's t distribution can be used when n < 30, and when $\sigma$ is unknown. The rule of thumb is to use Student's t distribution in these two instances, and the normal distributions for all other cases; however, the Student's t distribution begins to approximate the normal distribution with high n sizes, so t-tests are applicable in most instances.

When using t-tests for two populations, you need to first decide if you should use an independent t-test or a paired t-test. An independent t-test is used when the two groups are independent from each other. A paired t-test is used for paired or connected groups. For the class data set the independent t-tests is appropriate. For an independent t-test, the variance between groups must be unequal.

Let's examine if there is a difference between the risk associated with climate change for Democrats and Republicans in our survey. In order to test only Democrats and Republicans, we need to recode our factored party variable to only include Democrats and Republicans:

```{r 5_ind, echo=TRUE}
ds$f.part <- car::recode(ds$f.party.2, "'Dem'='Dem'; 'Rep'='Rep'; else=NA")
ds %>%
  count(f.part) %>%
  drop_na()
```

We need to compare variances for Democrats and Republicans on their risk perception of climate change. First use `skim()` to take a look at some summary statistics by political party.

```{r 5_ind2, echo=TRUE}
skim_with(numeric = list(hist = NULL))
skim_with(ingerger = list(hist = NULL))
ds %>%
  group_by(f.part) %>%
  skim(glbcc_risk) 
```

The Levene test is used to test if the difference in variances is statistically significant. Similar to previously discussed hypotheses, the Levene tests the following hypotheses:

-$H_0$: variances are equal  
-$H_1$: variances are not equal  

```{r 5_ind3, echo=TRUE}
leveneTest(ds$glbcc_risk, ds$f.part)
```

Given the p-value < $\alpha$ = 0.05, we reject the null hypothesis such that there is a statistically significant difference in variances. With this result we can perform an independent t-test for two populations. The framework for forming hypotheses is as follows:

-$H_0$: the two populations are indifferent; "there is no difference in perceived risk of climate change between Democrats and Republicans"
-$H_1$: the two populations are different; "there is a difference in perceived risk of climate change between Democrats and Republicans"

```{r 5_ind4, echo=TRUE}
t.test(ds$glbcc_risk ~ ds$f.part, var.equal = FALSE) # var.equal=FALSE is default
```

Given the p-value < $\alpha$ = 0.05, we reject the null hypothesis such that there is a statistically significant difference in perceived risk of climate change between democrats and republicans.

The previous test is an example of a two-tailed test, which lacks directionality (greater than, less than). In contrast, a one-tailed t-test tests hypotheses that include direction. To perform a one-tailed t-test, we include the command alt="greater" inside the `t.test()` function:

```{r 5_ind5, echo=TRUE}
t.test(ds$glbcc_risk ~ ds$f.part, alt="greater")
```

### Other Independent Sample Tests

Not all research leads to testing two populations. For example, consider whether a difference exists in perceived risk of climate change among Democrats, Republicans, and independents. Further, if there are statistically significant differences, how could ideologies be ranked? Unfortunately, performing pairwise t-tests on the possible combinations is inadequate, due to the "family-wise error rate" associated to the p-value. The `pairwise.t.test()` function employs the family-wise correction method (by default, Holm-Bonferroni method) to correct the p-value.

Like traditional independent sample tests involving two samples, the variances must be unequal:

```{r 5_hsd, echo=TRUE}
leveneTest(ds$glbcc_risk ~ ds$f.party.2)
```

Given that the p-value < $\alpha$ = 0.05, we reject the null hypothesis such that there is a difference in variances. The `pairwise.t.test()` function can perform the pairwise test, using the `f.party_2` variable that was factored for Democrats, Republicans, and independents:

```{r 5_pairwise, echo=TRUE}
pairwise.t.test(ds$glbcc_risk, ds$f.party.2, pool.sd=FALSE)
```

The output of the `pairwise.t.test()` function is read as a table, whereby each value is associated to the pairwise comparison.

An alternative, multiple comparisons test, is the Tukey Honest Significance Difference test ("Tukey's test"), that is commonly used with Analysis of Variance (ANOVA). In short, Tukey's test is used to find means that are significantly difference between multiple samples. Tukey's test requires using the `aov()` function to perform ANOVA, as follows:

```{r 5_hsd2, echo=TRUE}
aov(ds$glbcc_risk ~ ds$f.party.2) %>% TukeyHSD()
```

The p-value < $\alpha$ = 0.05 between each pair. 

## Paired t-test

Paired t-tests are appropriate for data that are not independent. Consider the need to compare data from the same population for different points in time, such as semester exams for the same students in a class.

The teacher wants to examine if there is a difference in the students' performances between exams one and two. The following hypothetical data frame could represent the students and their performance:

```{r 5_paired, echo=TRUE}
Student <- c("st1", "st2", "st3", "st4", "st5", "st6", "st7", "st8", "st9", "st10")
Exam1 <- c(99, 98, 67, 68, 70, 71, 72, 88, 75, 83)
Exam2 <- c(94, 93, 62, 63, 65, 66, 67, 83, 70, 76) 
exam.ds <- data.frame(Student, Exam1, Exam2)
exam.ds
```

The `exam.ds` data frame consists of three vectors: the student, exam one, and exam two.

First check the variance between the two groups:

```{r 5_paired2, echo=TRUE}
var.test(exam.ds$Exam1, exam.ds$Exam2)
```

Given the p-value > $\alpha$ = 0.05, the null hypothesis is not rejected such that there is no statistically significant difference in variances.  
  
Next, we define our hypotheses for the paired test:  
-$H_0$: there is no difference between exams one and two for students  
-$H_1$: there is a difference between exams one and two for students

```{r 5_paired3, echo=TRUE}
t.test(exam.ds$Exam1, exam.ds$Exam2, paired = TRUE, var.equal = TRUE)
```

Given the p-value < $\alpha$ = 0.05, the null hypothesis is rejected such that there is a statistically significant difference in means. Next, to determine directionality, the `alt="greater"` argument is included in the `t.test()` function. Additionally, the hypotheses are modified as follows:
$H_0$: there is no difference between exams one and two for students  
$H_1$: exams one performance is greater than exam two performance for students

```{r 5_paired4, echo=TRUE}
t.test(exam.ds$Exam1, exam.ds$Exam2, paired = TRUE, var.equal = TRUE, alt = "greater")
```

Given the p-value < $\alpha$ = 0.05, the null hypothesis is rejected.

## Visualizing Differences in Means

Visualization of data is essential to interpreting and communicating difference in means. To demonstrate, we will use `ggplot2` to visualize the difference in exam scores used with the paired t-tests.

First, we consider the difference of means with confidence intervals. We start by calculating the means for each exam:

```{r 5_vis, echo=TRUE}
exam1 <- mean(exam.ds$Exam1, na.rm = T)
exam2 <- mean(exam.ds$Exam2, na.rm = T)

```

Then, the standard errors:

```{r 5_vis2, echo=TRUE}
exam1.se <- describe(exam.ds$Exam1)$se
exam2.se <- describe(exam.ds$Exam2)$se
```

Lastly, the bounds for the confidence intervals. The 1.96 z-score is used for 95% confidence:

```{r 5_vis3, echo=TRUE}
exam1up <- exam1 + exam1.se*1.96
exam1low <- exam1 - exam1.se*1.96
exam2up <- exam2 + exam2.se*1.96
exam2low <- exam2 - exam2.se*1.96
```

Next, a data frame is constructed with the previous information in respective vectors:

```{r 5_vis4, echo=TRUE}
test <- c("Exam1", "Exam2")
scores <- c(exam1, exam2)
upper <- c(exam1up, exam2up)
lower <- c(exam1low, exam2low)
examdf <- data.frame(test, scores, upper, lower)
examdf
```

Now, for the visualization. Instead of a bar plot, we will create a graph consisting of a point for the means for each exam, and the confidence intervals. In additional, we will add color and labels. These arguments are available for other data sets and visualizations using `ggplot2`:

```{r 5_vis5, echo = TRUE}
ggplot(examdf, aes(x = test, y = scores)) +
  geom_point(size = 3, shape = 19, col = "dodgerblue3") + 
  geom_errorbar(aes(ymin = lower, ymax = upper), width = .1, col = "dodgerblue3") + 
  ylim(10, 100) + 
  ggtitle("Exam Scores") + 
  xlab("Exams") + 
  ylab("Scores") 
```

Following the basic steps above will help you make attractive visualizations for different needs.

Suppose the teacher wanted a visualization that plots each students' test scores between the two exams for easy comparison. A grouped bar plot is a simple approach to accomplish this.

This requires a different type of data frame. So far the data has been "wide" data, but we need "long" data. The `melt()` function found in the `dplyr` package converts data from wide to long format. Include "Exam1" and "Exam2" as the measured variables. 

```{r 5_vis6, echo=TRUE}
exam.m <- melt(exam.ds, measure.vars = c("Exam1", "Exam2"))
```

The new data frame pairs the exams with the students and the respective score. This is what is meant by "long" data. 

```{r 5_vis7, echo=TRUE}
exam.m
```

The grouped bar plot is constructed similar to a bar plot, except that the visualization is based on the exam and uses the `position.dodge()` function to place bars near each other and the `fill=Exam` argument to specify what the groups 

```{r 5_vis8, echo=TRUE}
ggplot(exam.m, aes(x=Student, y=value, fill=variable)) +
  geom_bar(stat="identity", position = position_dodge())
```

<!--chapter:end:05-inference.Rmd-->

# Covariance and Correlation

Research is not always interested in the contrast of two variables, but oftentimes the relationship of two variables. For instance, what is the relationship between climate science and ideology? The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
4. vcd

## Covariance

Covariance is the measure of change in one variable associated to change in another variable. That is, the measure of how two random variables vary together.

Calculating covariance of two variables of a known population is trivial, as the product of variation of two variables. However, for samples, covariance is calculated via the following formula:

$$cov(x,y)=\frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{{n-1}}$$

Where `x` and `y` are two random variables, `i` is each observation (row), and `n` is the sample size.

### Covariance by Hand 

To find covariance by hand, let's construct a hypothetical data set with variables x and y. We only give each variable three values so that the calculation will be shorter.

```{r 6_cov, echo=TRUE}
x <- c(25, 27, 29)
y <- c(5, 15, 9)
```

First we calculate the difference of each value and the mean for the variables:

```{r 6_cov2, echo=TRUE}
xdev <- x - mean(x)
ydev <- y - mean(y)
```

Next we find the product of the above differences:

```{r 6_cov3, echo=TRUE}
xdev_ydev <- xdev * ydev
xdev_ydev
```

We complete the numerator by finding the sum of the products:

```{r 6_cov4, echo=TRUE}
sum_xdev_ydev <- sum(xdev_ydev)
```

Lastly, we complete the covariance calculation by dividing the numerator by the denominator. The denominator is calculated as one less than the sample size:

```{r 6_cov5, echo=TRUE}
cov_xy <- sum_xdev_ydev / (3 - 1)
cov_xy
```

### Covariance in R

By using the `cov()` function, R calculates the covariance. We demonstrate the `cov()` function by confirming the previous section's calculations:

```{r 6_cov6, echo=TRUE}
cov(x, y)
```

### Covariance in Class Data Set

To demonstrate further we will calculate covariance for various pairs of variables within the class data set. First, suppose we are interested in the relationship between certainty that humans cause climate change (`glbcc_cert`) and the perceived risk of climate change (`glbcc_risk`). That is, is there a relationship between respondents' certainty that humans cause climate change and their perceived risk of climate change? Our hypothesis could be that individuals that are more certain that climate change is a consequence of humans are likely more concerned about the associated risk. 

```{r 6_cov7, echo=TRUE}
cov(ds$glbcc_cert, ds$glbcc_risk, use = "complete.obs")
```

__Note:__ The `cov()` function requires `use="complete.obs"` to remove NA entries.

The calculated covariance of certainty and risk perception is positive, indicating the two variables are positively related. As one variable changes, the other variable will change in the same direction with a magnitude of 3.09. Increased certainty that humans cause climate change increases the perceived risk of climate change.

Suppose we are also interested in the relationship of income and perceived risk of climate change:

```{r 6_cov8, echo=TRUE}
cov(ds$income, ds$glbcc_risk, use = "complete.obs")
```

The calculated covariance of income and perceived risk of climate change is negative, indicating the two variables are negatively related. As one variable changes, the other variable will change in the opposite direction with a magnitude of -10826.91.

An important follow-up question is: which variable is more strongly associated to perceived risk, certainty or income? We cannot compare magnitudes to the difference in scales and units. Income is measured on a scale inclusive of higher numbers compared to certainty. To compare strengths of association we need to standardize covariance.

## Correlation

Correlation standardizes covariance on a scale of negative one to one, whereby the magnitude from zero indicates strength of relationship. Similar to covariance, a positive and negative value reflects the respective relationship. The formula for correlation is the following:

$$r_{xy}=\frac{cov(x,y)}{s_xs_y}$$
  
Where x and y are two random variables.

### Correlation by Hand

To find covariance by hand, let's use the data set we constructed earlier to calculate covariance.

Recall we calculated covariance manually via the following steps:

```{r 6_cor, echo=TRUE}
x <- c(25, 27, 29)
y <- c(5, 15, 9)
xdev <- x - mean(x)
ydev <- y - mean(y)
xdev_ydev <- xdev * ydev
sum_xdev_ydev <- sum(xdev_ydev)
cov_xy <- (1 / (3 - 1)) * sum_xdev_ydev
cov_xy
```

For calculating correlation we need the product of the standard deviations of variables `x` and `y` for the denominator:

```{r 6_cor2, echo=TRUE}
stnd.dev <- sd(x)*sd(y)
```

Now we find the quotient of the covariance numerator and standard deviations denominator:

```{r 6_cor3, echo=TRUE}
cov_xy/stnd.dev
```

The relationship is positive; however, the correlation coefficient is $\approx$ 0.40. 

Returning to the class data set, we can find the correlation coefficient for certainty humans cause climate change and perceived risk of climate change from the class data set:

```{r 6_cor4, echo=TRUE}
numerator <- cov(ds$glbcc_cert, ds$glbcc_risk, use = "complete.obs")
denominator <- sd(ds$glbcc_cert, na.rm = T) * sd(ds$glbcc_risk, na.rm = T)
numerator / denominator
```

The correlation coefficient is $\approx$ 0.37.

Now let's find the correlation coefficient for ideology and perceived risk from climate change.

First we will create a subset from the class data set `ds` for the variables of interest, absent of missing observations. __Note:__ This subset includes additional variables, `f.gender` and `f.party.2` for use later in this lab.

```{r 6_cor5, echo=TRUE}
ds.sub <- ds %>%
  dplyr::select("glbcc_risk", "ideol", "f.gender", "f.party.2") %>%
  na.omit()
```

The perceived risk and ideology variables are assigned to `x` and `y` variables within the `ds.sub` object, as to follow the formula.

```{r 6_cor6, echo=TRUE}
ds.sub$x <- ds.sub$glbcc_risk
ds.sub$y <- ds.sub$ideol
```

The `x` and `y` variables are then used to find the covariance, similar to the steps demonstrated earlier:

```{r 6_cor7, echo=TRUE}
xbar <- mean(ds.sub$x)
ybar <- mean(ds.sub$y)

x.m.xbar <- ds.sub$x - xbar
y.m.ybar <- ds.sub$y - ybar

n <- length(ds.sub$x)
n

cov.xy <- (sum(x.m.xbar * y.m.ybar)) / (n - 1)
cov.xy
```

Next, we find the correlation coefficient using the covariance as the numerator and the product of both variable standard deviations as the denominator: 

```{r 6_cor11, echo=TRUE}
sd.x <- sqrt((sum(x.m.xbar^2)) / (n - 1))
sd.x

sd.y <- sqrt((sum(y.m.ybar^2)) / (n - 1))
sd.y

sd.xy <- sd.x*sd.y

cov.xy/sd.xy
```

The correlation coefficient for the variables is $\approx$ -0.59. The manual calculation is confirmed using the `cor()` function in R:

```{r 6_cor15, echo=TRUE}
cor(ds.sub$glbcc_risk, ds.sub$ideol)
```

__Note:__ To calculate the correlation coefficient manually in one line of code:

`sum((x-mean(x))*(y-mean(y))) /(sqrt(sum((x-mean(x))^2))*sqrt(sum((y-mean(y))^2)))`

### Correlation Tests

The previous section demonstrated the `cor()` function to confirm the manual calculation of the correlation coefficient. Using this function we can find the correlation coefficient of the income and perceived risk variables:

```{r 6_cor16, echo=TRUE}
cor(ds$income, ds$glbcc_risk, use = "complete.obs")
```

The correlation coefficient of -0.06 informs us of two things: the relationship is negative and very weak. __Note:__ The correlation coefficient is drawn from observations within a sample, and therefore is a random value. That is, if we were to collect multiple samples we would calculate different correlation coefficients for each sample collected. This leads to a new hypothesis: is there a relationship between income and perceived risk? To test this we employ Pearson's product-moment correlation available via the `cor.test()` function. Our testing hypotheses are:  

-$H_0$: the true correlation coefficient is zero  
-$H_1$: the true correlation coefficient is not zero

```{r 6_cor.test, echo=TRUE}
cor.test(ds$income, ds$glbcc_risk, use="complete.obs")
```

The correlation test yields a p-value < $\alpha$ = 0.05, thereby the null hypothesis is rejected such that the true correlation coefficient is not zero. 

__Note:__ Despite rejecting the null hypothesis, our random correlation value is still quite small at -0.06. This indicates a very weak, or non-substance, relationship between income and perceived risk.

To demonstrate a potentially substantive relationship we look at ideology and perceived risk of climate change using the `cor.test()` function:

```{r 6_cor.test2, echo=TRUE}
cor.test(ds$ideol, ds$glbcc_risk)
```

The default correlation test method for the `cor.test()` function is the Pearson test. The Spearman test is required for ordinal data via the `method="spearman"` argument within the `cor.test()` function. For calculation correlation with ordinal data.

### Correlation Across Groups

Suppose you are interested in correlations across multiple variables. The `cor()` function examines the correlation between each variable pair for an entire data set. As such, if you are interested in only the correlation for select variables then use the `select()` function and select your variablesof interest, then pipe them into the `cor()` function. Include `drop_na()`. To demonstrate using variables from the class data set:

```{r 6_cor.group, echo=TRUE}
ds %>% 
  dplyr::select(glbcc_risk, ideol, income, age) %>%
  drop_na() %>%
  cor()
```

This resulting matrix provides the correlation coefficients for two variables at a time. The correlation coefficients are defined by the intercept of the row and columns corresponding to each variable.

Additionally, recalling that each correlation coefficient itself is a random value, a test is required for inference. The `corr.test()` provided by the `psych` package is an alternative to the `cor.test()` function previously used. The `corr.test()` function supports examining variable pairs simultaneously within a given data set. Use the `print()` function with the `short=FALSE` argument to view the complete test with confidence intervals and p-values.

```{r 6_cor.group3, echo=TRUE}
ds %>% 
  dplyr::select(glbcc_risk, ideol, income, age) %>%
  drop_na() %>%
  corr.test %>%
  print(short = FALSE)
```

## Visualizing Correlation

The simplest form to visualize correlation is a scatter plot with a trend line.

We will review the relationship between ideology and perceived risk from climate change. Build the basic visualization by using `ggplot()` and the `geom_point` functions, with ideology on the x axis and perceived risk about climate change on the y axis. Use the `ds.sub` dataset, because we removed the missing values. 

```{r 6_vis, echo=TRUE}
ggplot(ds.sub, aes(x = ideol, y = glbcc_risk)) +
  geom_point(shape = 1)
```

Notice how this doesn't really make sense? This is because there are thousands of observations being placed on a discrete set of values. To get a better idea of the relationship, we can tell R to jitter the points. "Jittering" provides a tiny bit of white noise and variance to the values, so that we can see where there is high overlap of observations. This provides a better picture of the relationship. This time use the `geom_jitter` function instead of the `geom_point` function:

```{r 6_vis2, echo=TRUE}
ggplot(ds.sub, aes(x = ideol, y = glbcc_risk)) +
  geom_jitter(shape = 1)
```

Notice the apparent negative correlation. Verify this by checking the correlation:

```{r 6_vis3, echo=TRUE}
ds %>%
  dplyr::select(ideol, glbcc_risk) %>%
  cor(use = "complete.obs")
```

A trend line to the scatter plot helps to interpret directionality of a given variable pair. The next lab will further introduce trend lines, but for now we introduce it via the `geom_smooth()` function by including the `method=lm` argument. We also need to include `geom_point()` for each point.

```{r 6_vis4, echo=TRUE}
ggplot(ds.sub, aes(x = ideol, y = glbcc_risk)) +
  geom_point(shape = 1) +
  geom_smooth(method = lm) +
  geom_jitter(shape = 1)
```

The ideology points can be differentiated by other variables, such as gender, to examine potential difference among gender by defining the `color` argument as follows:

```{r 6_vis5, echo=TRUE}
ggplot(ds.sub, aes(x = ideol, y = glbcc_risk, color = f.gender)) +
  geom_point(shape = 1) +
  geom_smooth(method = lm) +
  geom_jitter(shape = 1)
```

### Another Example: Political Party

Let's look at this relationship broken down by political party. Perhaps you wanted to see if the relationship looks different for Republicans and Democrats. We can first subset the data for Republicans and Democrats:

```{r 6_part, echo=TRUE}
ds.dem <- filter(ds.sub, f.party.2 == "Dem")
ds.rep <- filter(ds.sub, f.party.2 == "Rep")
```

Next we investigate the correlation between ideology and perceived risk of climate change for Republicans and Democrats separately:

```{r 6_part2, echo=TRUE}
cor.test(ds.rep$ideol, ds.rep$glbcc_risk)
cor.test(ds.dem$ideol, ds.dem$glbcc_risk)
```

The correlation coefficient is slightly more negative for Democrats.

To create a visualization that compares the two parties and the relationship between climate change risk and ideology, a few simple lines of code can get the job done. First filter the data to include Democrats and Republicans, select the variables of interest, drop NAs, pipe it all into `ggplot2`, then use `facet_wrap()` to create two visualizations, one for each party:

```{r 6_2vis.5, echo=TRUE}
ds %>%
  filter(f.party.2 == "Dem" | f.party.2 == "Rep") %>%
  dplyr::select(ideol, glbcc_risk, f.party.2) %>%
  na.omit() %>%
  ggplot(., aes(ideol, glbcc_risk)) +
  geom_jitter(shape = 1) +
  geom_smooth(method = lm) +
  facet_wrap(~ f.party.2, scales = "fixed")
```


### One More Visualization

We cap this lab off with creating one last visualization. First create a new subset of our data exclusive of all missing observations. Include variables for climate change risk and age. 

```{r 6_2vis, echo=TRUE}
sub.ds <- filter(ds) %>%
  dplyr::select("glbcc_risk", "age") %>%
  na.omit()
```

First we look at our age variable.

```{r 6_2vis2, echo=TRUE}
describe(sub.ds$age)
```

Now find the correlation:

```{r 6_2vis3, echo=TRUE}
cor.test(sub.ds$glbcc_risk, sub.ds$age)
```

There is a slight negative correlation. We can interpret this as indicating that younger people are slightly more concerned about climate change. Now we construct the visualization:

```{r 6_2vis4, echo=TRUE}
ggplot(sub.ds, aes(y = glbcc_risk, x = age)) +
  geom_point(shape = 20, color = "#e20000") +
  geom_jitter(shape = 20, color = "#e20000") +
  geom_smooth(method = lm) +
  xlab("Age") +
  ylab("Climate Change Risk") +
  ggtitle("Age and Climate Change Risk") +
  scale_y_continuous(breaks = c(0:10),
                     labels = c("0","1","2", "3", "4", "5", "6", "7", "8", "9", "10")) +
  theme_bw()
```

<!--chapter:end:06-covariance.Rmd-->

# Bivariate Linear Regression

This lab will cover the basics of bivariate linear regression, introducing via manual calculations and R functions. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
4. memisc
5. stargazer
6. reshape2

## Bivariate Linear Regression by Hand

The goal of bivariate linear regression is to estimate a line (slope and intercept) that minimizes the error term (residual). The bivariate linear regression model is as follows:

$$y_i=\alpha+\beta x_i+\varepsilon_i$$

Where, `y` is the dependent variable, `i` is the unit of analysis, $\alpha$ is the y-intercept, $\beta$ is the slope, `x` is the independent variable, and $\varepsilon$ is the error term (residuals).

When working with samples, we develop an estimated model:

$$\hat{y}=\hat{\alpha}+\hat{\beta} x$$

Where, the hat implies the coefficients are estimates from data.

This lab focuses on the ordinary least squares method to find $\hat{\alpha}$ and $\hat{\beta}$ such that if given a value of `x` you can return an estimate of y ($\hat{y}$).

To demonstrate, we will explore the relationship between ideology and concern for natural resources. The first step is to subset the variables of interest absent of NA values:

```{r 7_71, echo=TRUE}
ds %>% 
  dplyr::select("ideol", "cncrn_natres") %>%
  na.omit() -> ds.sub
```

Reviewing the variables is an important first step:

```{r 7_72, echo=TRUE}
describe(ds.sub$ideol)
describe(ds.sub$cncrn_natres)
```

The ideology variable ranges from 1 "very liberal" to 7 "very conservative." The _cncrn_natres_ variable measures concern for natural resources ranging from 0 "not concerned" to 10 "extremely concerned." The dependent and independent variables are defined as:  

DV: Ideology  
IV: Concern for natural resources

We need to find $\hat{\alpha}$ and $\hat{\beta}$ to develop the estimated bivariate regression model. Our first step is to find $\hat{\beta}$. Recall the formula for $\hat{\beta}$ is:

$$\hat{\beta}=\frac{cov(x,y)}{var(x)}$$

First create `x` and `y` variables to make the calculations simpler, with `x` being the independent variable and `y` being the dependent variable:

```{r 7_7biv, echo=TRUE}
ds.sub$x <- ds.sub$ideol
ds.sub$y <- ds.sub$cncrn_natres
```

Next we calculate the cov(x,y) for the numerator and var(x) for the denominator:

```{r 7_7biv2, echo=TRUE}
r <- cor(ds.sub$x, ds.sub$y)
cov.xy <- cov(ds.sub$x, ds.sub$y, use = "complete.obs")
var.x <- var(ds.sub$x, na.rm = T)
```

Finally, to calculate $\hat{\beta}$:

```{r 7_7biv3, echo=TRUE}
beta.hat <- cov.xy / var.x
```

Now that we have $\hat{\beta}$, we can calculate $\hat{\alpha}$ Recall the formula for calculating $\hat{\alpha}$:

$$\hat{\alpha}=\bar{y} - \hat{\beta}\bar{x}$$

Calculate $\bar{y}$ and $\bar{x}$ from the sample data:

```{r 7_7biv4, echo=TRUE}
ybar <- mean(ds.sub$y, na.rm = T)
xbar <- mean(ds.sub$x, na.rm = T)
```

Use $\bar{y}$, $\bar{x}$, and $\hat{\beta}$ to calculate $\hat{\alpha}$:

```{r 7_7biv5, echo=TRUE}
alpha.hat <- ybar - beta.hat * xbar
```

Take a look at our calculated $\hat{\alpha}$ and $\hat{\beta}$:

```{r 7_7biv6, echo=TRUE}
alpha.hat
beta.hat
```

Now create some predicted values of y, concern for natural resources, based on our estimated regression model:

```{r 7_7biv7, echo=TRUE}
yhat <- alpha.hat + beta.hat * ds.sub$x
head(yhat) # Returns the first 5 values
```

To draw inference using the estimated regression model we need to understand how `x` helps us understand `y`. More formally, we explore this relationship by evaluating the residuals associated to the coefficients $\hat{\alpha}$ and $\hat{\beta}$. We are interested in whether the coefficients are statistically different than zero.

-$H_0$: $\beta$ = 0  
-$H_1$: $\beta$ $\neq$ 0

Put simply, if the coefficient $\hat{\beta}$ is zero, than that is to say that no value of x helps us understand y ($0*x=0$).

If we assume the residuals follow a normal distribution, then we can calculate the t-statistic to examine the coefficient's statistical significance.

Recall that residuals are the differences in the observed values of the data and the predicted values of the estimated regression model. We can calculate residuals by subtracting the $\hat{y}$ values we just calculated from the `y` values:

```{r 7_7biv8, echo=TRUE}
res <- ds.sub$y - yhat
```

Now that we have the residuals, we need to calculate the residual standard error, which measures the spread of observations around the regression line we just calculated. We also need to know the residual standard error so that we can find the standard errors for the regression coefficients, which are then used to calculate the t scores of the coefficients. To calculate the residual standard error we need to find:

1. The residual sum of squares
2. The degrees of freedom for our model

```{r 7_7rss, echo=TRUE}
res.sqr <- res^2
RSS <- sum(res.sqr, na.rm=T)
```

Now we need to calculate the degrees of freedom. In this case, we have the intercept and the ideology variable, so we subtract two. Since we subset our data to remove all missing observations, we know the n size for x and y are the same:

```{r 7_7df, echo=TRUE}
df <- length(ds.sub$y) - 2
df
```

With the residual sum of squares and the degrees of freedom, we have what we need to find the residual standard error:

```{r 7_7rse, echo=TRUE}
RSE <- sqrt(RSS / df)
RSE
```

With the residual standard error, we can now start to calculate the standard errors for our coefficients: $\hat{\alpha}$ and $\hat{\beta}$ (ideology). To calculate these, we need to find the total sum of squares of the independent variable, x:

```{r 7_7tssx, echo=TRUE}
TSSx <- sum((ds.sub$x - xbar)^2)
TSSx
```

Now that we have the total sum of squares for the independent variable, we can find the standard errors of our coefficients:

```{r 7_7SEb, echo=TRUE}
SEB <- RSE / sqrt(TSSx)
SEB
```

For the standard error of $\hat{\alpha}$, the calculation is different:

```{r 7_7SEa, echo=TRUE}
SEA <- RSE * sqrt((1 / 2508)+(xbar^2 / TSSx))
SEA
```

With the standard errors calculated, we can now find the corresponding t-statistics:

```{r 7_7t, echo=TRUE}
t.B <- beta.hat / SEB
t.B
```

```{r 7_7t2, echo=TRUE}
t.A <- alpha.hat / SEA
t.A
```

These t-statistics tell us how many standard deviations the coefficients are away from 0. 

### Calculating Goodness of Fit

Now we turn to $R^2$, the measure how well the estimated regression model explains the variability of the data. To find $R^2$, you need to know:

1. The residual sum of squares
2. The total sum of squares
3. The explained sum of squares.

We already have the residual sum of squares. We found it by taking the sum of the residuals squared. Earlier we calculated the total sum of squares for our independent variable, x, but now we need to find the total sum of squares for our dependent variable, y.

```{r 7_7r, echo=TRUE}
TSS <- sum((ds.sub$y - ybar)^2)
TSS
```

Now that we have the residual sum of squares and the total sum of squares, we can find the explained sum of squares. The explained sum of squares tells us how much of the variance of the dependent variable is accounted for by our model.

```{r 7_7ess, echo=TRUE}
ESS <- TSS - RSS
ESS
```

$R^2$ is found by dividing the explained sum of squares by the total sum of squares:

```{r 7_7r2, echo=TRUE}
r.sqr <- ESS / TSS
r.sqr
```

4% of the variability of the data is explained by the estimated regression model.

#### Adjusted R Squared

There is a slightly more accurate measure of model fit, though, known as adjusted R squared. Adjusted R squared addresses some problems that are inherent in the R squared calculation, like the realtiy that R squared tends to increase as you add more predictors to your model, even if it's more due to chance than actual predicting power. Adjusted R squared addresses this issue by penalizing the model for an increased number of predictors. Use this formula to find adjusted R squared

$$1-\frac{(1-R^{2})(n-1)}{n-k-1}$$

where k is the number of predictors in our model, not including the intercept(A). We can actually find this pretty simply. Recall that we are working with an n size of 2508. Our k value is 1, becuase we only have one predictor in the model (ideology). Find the adjusted R squared value:

```{r 7_7adjr, echo=TRUE}
adj.r2 <- 1-(((1 - r.sqr) * (2508 - 1)) / (2508 - 1 - 1))
adj.r2
```

### Checking Our Work

To check our work in the previous steps we will employ the native R functions for linear models: `lm()`. We provide the observed y and x values from data as an argument, separated by a tilda in the following format:
`lm(y ~ x)`

To demonstrate with the `cncrn_natres` and `ideol` variables:

```{r 7_7ols, echo=TRUE}
model <- lm(ds.sub$cncrn_natres ~ ds.sub$ideol)
```

Further, the `summary()` function will provide results of our model, including t-statistics and $R^2$ values:

```{r 7_7ols2, echo=TRUE}
summary(model)
```

Let's compare to our coefficients, residual standard error, coefficient standard errors, t scores, r squared, and adjusted r squared:

```{r 7_7ols3, echo=TRUE}
stats <- data.frame(name = c("Intercept", "Beta", "RSE", "IntSE", "BetaSE", "IntT",
                             "BetaT", "Rsqr", "AdjRsqr"),
                    values = c(alpha.hat, beta.hat, RSE, SEA, SEB, t.A, t.B,
                             r.sqr, adj.r2))
stats
```

## Bivariate Regression in R

Suppose you are interested in the relationship between ideology and opinions about how much of Oklahoma's electricity should come from renewable sources. The class data set includes these variables as `ideol` and `okelec_renew`. A new subset data set is created with these variables, absent missing observations, to develop an estimated regression model:

```{r 7_7br, echo=TRUE}
sub.ds <- ds %>% 
  dplyr::select("okelec_renew", "ideol") %>%
  na.omit()
```

The `okelec_renew` variable is new to us, so we should examine its structure:

```{r 7_7br2, echo=TRUE}
str(sub.ds$okelec_renew)
```

The variable appears to be a factor, which needs to be coerced as a numeric type. Recall the `as.numeric()` function:

```{r 7_7br3, echo=TRUE}
sub.ds %>%
  mutate(renew = as.numeric(okelec_renew)) %>%
  drop_na() -> sub.ds
```

Reassess the structure to ensure it is numeric:

```{r 7_7br4, echo=TRUE}
str(sub.ds$renew)
```

With the variable now numeric, examine it using the `describe()` function:

```{r 7_7br5, echo=TRUE}
describe(sub.ds$renew)
```

Before proceeding, we should formalize a hypothesis. Perhaps we hypothesize that conservatives want a lower percentage of renewable energy than liberals. Using renewable energy as a function of ideology, we further specify our hypothesis as a more conservative ideology corresponds to a decrease preference for renewable energy. The null hypothesis is there is no difference in preference among ideologies.

First we  examine the normality of both variables. Create a histogram and overlay it with a normal distribution curve, with the correct mean and standard deviation.:

```{r 7_7br6, echo=TRUE}
ggplot(sub.ds, aes(renew)) +
  geom_histogram(aes(y= ..density.. ), bins = 20) +
  stat_function(fun = dnorm, args = list(mean = mean(sub.ds$renew),
                                         sd = sd(sub.ds$renew)))
```


```{r 7_7br7, echo=TRUE}
ggplot(sub.ds, aes(ideol)) +
  geom_histogram(aes(y = ..density..), bins = 7) +
  stat_function(fun = dnorm, args = list(mean = mean(sub.ds$ideol), sd = sd(sub.ds$ideol)))
```

Now construct the model:

```{r 7_7br8, echo=TRUE}
model1 <- lm(sub.ds$renew ~ sub.ds$ideol)
summary(model1)
```

Based on the results, ideology helps us understand preference for renewable energy. Further examination of the coefficient for `ideol` yields an estimate value of -2.45. This is interpreted as a -2.45 unit decrease in renewable energy preference for each unit increase in ideology. That is, an increase on the ideology scale (1 "liberal" to 7 "conservative") results in a reduction in preference for renewable energy.

We should always visualize a relationship that we're trying to convey. Start by constructing a scatter plot and adding a regression line:

```{r 7_7br9, echo=TRUE}
ggplot(sub.ds, aes(x = ideol, y = renew)) +
  geom_point(shape = 1) +
  geom_smooth(method = lm) +
  geom_jitter(shape = 1)
```

Sometimes it is more beneficial when visualizing the relationship to plot the regression line without first showing the scatter plot. To do this with ggplot2, simply do not include the `geom_point()` or `geom_jitter()` functions in the visualization.

```{r 7_7br10, echo=TRUE}
ggplot(sub.ds, aes(x = ideol, y = renew)) +
  geom_smooth(method = lm) 
```

## The Residuals

Recall that when using Ordinary Least Squares regression, there are three assumptions made about the error terms:

1. Errors have identical distributions
2. Errors are independent of X and other error terms
3. Errors are normally distributed

To look at the residual values for the estimated regression model, use the `names()` function:

```{r 7_7res, echo=TRUE}
names(model)
```

In R we can examine the distribution of the residuals relatively simply. First assign the residuals to an object:

```{r 7_7res2, echo=TRUE}
resid <- model$residuals
```

Now plot a histogram of the residuals, adding a normal density curve with the mean and standard deviation of our residuals:

```{r 7_7res3, echo=TRUE}
ggplot(model, aes(model$residuals)) +
  geom_histogram(aes(y =  ..density..)) +
   stat_function(fun = dnorm, args = list(mean = mean(model$residuals), sd = sd(model$residuals)))
```

We also look at a QQ plot of the residuals:

```{r 7_7res4, echo=TRUE}
ggplot(model, aes(sample = model$residuals)) +
  stat_qq() +
  stat_qq_line()
```

## Comparing Models

Suppose you wanted to create multiple bivariate models and contrast them. This is possible using the `mtable()` function from the `memisc` package. To demonstrate, we create three models looking at the relationship between an independent variable and concern about global climate change. The three independent variables will be ideology, certainty that humans cause climate change, and age. We start by creating a subset of our data and removing missing observations:

```{r 7_7comp, echo=TRUE}
sub <- ds %>%
  dplyr::select("glbcc_risk", "glbcc_cert", "age", "ideol") %>%
  na.omit() 
model1 <- lm(sub$glbcc_risk ~ sub$ideol)
model2 <- lm(sub$glbcc_risk ~ sub$glbcc_cert)
model3 <- lm(sub$glbcc_risk ~ sub$age)
```

Using the `mtable()` function, we can create regression tables that compare all three of the models:

```{r 7_7comp2, echo=TRUE}
mtable(model1, model2, model3)
```

Alternatively, you can use the `stargazer()` function to create tables for your models. The `stargazer()` function is different in that you can specify a table created with text or with LaTex code that you can subsequently paste into a LaTex document. We'll create a text table, but if you wanted to create a Latex table, you would use the `type=latex` argument.

```{r 7_7com3, echo=TRUE}
stargazer(model1, model2, model3, type="text", style="apsr")
```
 
### Visualizing Multiple Models

The three models can be visualized together by melting the data set into long form, with the three IVs as measure variables, then using `ggplot2` and facet wrapping by independent variable. Including `scales = "free_x"` inside the `facet_wrap()` function will make the visualization so that each plot has its own independent x axis but is on the same fixed y axis. 

```{r 7_7vis, echo=TRUE}
melt(sub, measure.vars = c("ideol", "glbcc_cert", "age"),
              variable.name = c("IV")) %>%
  ggplot(., aes(value, glbcc_risk)) +
  geom_smooth(method = lm) +
  facet_wrap(~ IV, scales = "free_x")
```

## Hypothesis Testing

Let's do one more example of how we would hypothesis test with bivariate regression. In the class data set there is a variable, `wtr_comm`, that asks the respondent if they think the supplies of water in their region will be adequate to meet their community's needs over the next 25 years. In essence, this measures concern about water supply for the community.

Start with a research question: What is the relationship between concern for water supply and concern about climate change?

Now build a theory: We could reasonably theorize that individuals who are more concerned about water supply are also likely more concerned about climate change. There is likely a link in their head between climate change and a shortened water supply. 

Built on this theory, we can specify a hypothesis that individuals more concerned about climate change will be more concerned about water supply for their community. The null hypothesis is that there is no relationship between climate change concern and water concern.

We create a subset of the dataset that includes out two variables and remove missing observations:

```{r 7_7hyp, echo=TRUE}
new.ds <- ds %>%
  dplyr::select("wtr_comm", "glbcc_risk") %>%
  na.omit()
```

Now examine the variables:

```{r 7_7hyp2, echo=TRUE}
describe(new.ds$wtr_comm)
```

```{r 7_7hyp3, echo=TRUE}
describe(new.ds$glbcc_risk)
```

Note that the climate change risk variable goes from 0 to 10 and the water supply concern variable ranges from 1 to 5, with 1 being definitely no (the supplies of water are NOT enough) and 5 being definitely yes.
 
Now visualize the normality of the variables:

```{r 7_7hyp4, echo=TRUE}
ggplot(new.ds, aes(wtr_comm)) +
  geom_density(adjust = 3) +
  stat_function(fun = dnorm, args = list(mean = mean(new.ds$wtr_comm), sd = sd(new.ds$wtr_comm)), color = "blue")
```

```{r 7_7hyp5, echo=TRUE}
ggplot(new.ds, aes(glbcc_risk)) +
  geom_density(adjust = 3) +
  stat_function(fun = dnorm, args = list(mean = mean(new.ds$glbcc_risk), sd = sd(new.ds$glbcc_risk)), color = "blue")
```

Next, create the model. Recall that the dependent variable is concern for water supply and the independent variable is climate change risk.

```{r 7_7hyp6, echo=TRUE}
lm1 <- lm(new.ds$wtr_comm ~ new.ds$glbcc_risk)
```

Now examine and interpret the results:

```{r 7_7hyp7, echo=TRUE}
summary(lm1)
```

The independent variable coefficient is about -.09, with a corresponding p-value $\approx$ 0. This is interpreted as a one unit change in climate change risk corresponds with a -0.09 unit change in water supply concern. Remember that the water supply variable goes from 1 (there is definitely not enough water) to 5 (there definitely is enough water). These findings suggest that an individual more concerned about climate change is also more concerned about water supply.

We examine the normality of the residuals:

```{r 7_7hyp8, echo=TRUE}
ggplot(lm1, aes(lm1$residuals)) +
  geom_density(adjust = 3) +
  stat_function(fun = dnorm, args = list(mean = mean(lm1$residuals), sd = sd(lm1$residuals)), color = "blue")
```

Now build a good visualization that would be worthy of a paper:

```{r 7_7hyp9, echo=TRUE}
ggplot(new.ds, aes(wtr_comm, glbcc_risk)) +
  geom_smooth(method = lm) +
  coord_cartesian(ylim = c(2, 9), xlim = c(1, 5)) +
  ggtitle("Concern for Water and Climate Change") +
  xlab("Considers Water Supply Adequate") +
  ylab("Perceived Climate Change Risk") +
  scale_x_continuous(breaks=c(1, 2 ,3 ,4 ,5), labels=c("Definitely No", 
                                                       "Probably No", "Unsure",
                                                       "Probably Yes", "Definitely Yes")) +
  theme_bw()
  
```

Our findings support that individuals less concerned about climate change are also less concerned about their community's water supply.

<!--chapter:end:07-bivariate.Rmd-->

# Multivariable Linear Regression

This lab covers the basics of multivariable linear regression. We begin by reviewing linear algebra to perform ordinary least squares (OLS) regression in matrix form. Then we will cover an introduction to multiple linear regression and visualizations with R. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
4. stargazer
5. reshape2
6. broom
7. skimr

## Calculating Least-Squared Estimates

The previous lab introduced the estimated bivariate linear regression model as follows:

$$\hat{y}=\hat{\alpha}+\hat{\beta} x$$

Where $\hat{\alpha}$ and $\hat{\beta}$ are solved via the following formulas:

$$\hat{\alpha}=\bar{y} - \hat{\beta}\bar{x}$$

$$\hat{\beta}=\frac{cov(x,y)}{var(x)}$$

In this lab we use matrix algebra to calculate the least-squared estimates. This proves useful for multivariable linear regression models where the methods introduced for bivariate regression models become more complex and computationally cumbersome to express as equations.

### Matrix Algebra

For multivariable regression analysis, the formulas for calculating coefficients are more easily found using matrix algebra. In this section we cover the basics of matrix algebra relevant to calculating the coefficients for an estimated linear regression model. A matrix is a rectangular array of numbers organized in rows and columns and each number within a matrix is an element. A matrix is defined as consisting of `m` rows and `n` columns. __Note:__ If m = n, the matrix is referred to as a square matrix.

The following is a general form of a matrix:

$$x_{m\times n} = \begin{bmatrix}
    x_{11} & \dots & x_{1n} \\
    \vdots & \ddots & \vdots \\
    x_{m1} & \dots & x_{mn}
\end{bmatrix}$$

There are a number of operations in matrix algebra; however, this review focuses on those pertinent to solutions of the least-squared equations in multiple regression:

#### Transpose of a matrix

The transpose of a matrix __A__, denoted as __A'__, is obtained by interchanging the rows and columns of the ___A___ matrix:

$$A = \begin{bmatrix}
    1 & 2 & 4 \\
    5 & 7 & 6
\end{bmatrix}, A' = \begin{bmatrix}
    1 & 5 \\
    2 & 7 \\
    4 & 5
\end{bmatrix}$$

__Note:__ The product of a matrix and its transpose is a square matrix.

The `matrix()` function in R will create a matrix object consisting of provided values in a defined number of rows and columns. The above matrix, `A`, is created as follows:

```{r 8_mat, echo=TRUE}
A <- matrix(c(1, 2, 4, 5, 7, 6), 2, 3)
A
```

Further, the `t()` function will transpose a given matrix:


```{r 8_mat2, echo=TRUE}
Aprime <- t(A)
Aprime
```

#### Row-column multiplication

Matrix multiplication is done by summing the products of the row elements in the first matrix by the column elements in the second matrix in corresponding position. This is shown in the following example:

$$A = \begin{bmatrix}
    1 & 2 & 4 \\
    5 & 7 & 6
\end{bmatrix} \times A' = \begin{bmatrix}
    1 & 5 \\
    2 & 7 \\
    4 & 5
\end{bmatrix} = \begin{bmatrix}
    1*1+2*2+4*4 & 1*5+2*7+4*6 \\
    5*1+7*2+6*4 & 5*5+7*7+6*6
\end{bmatrix} = \begin{bmatrix}
    21 & 43 \\
    43 & 110
\end{bmatrix}$$

Similarly, the product of matrices can be calculated in R using the %*% operator:

```{r 8_mat3, echo=TRUE}
AxAprime <- A %*% Aprime
AxAprime
```

__Note:__ Not all matrices can be multiplied. The number of columns in the first matrix must equal the number of rows in the second matrix. Further, unlike ordinary algebra multiplication, matrix multiplication is NOT commutative (order of operands matter). In the previous example we were able to find the product of A and A', because the number of columns in A (3) is equal to the number of rows in A' (3). Suppose we wanted the product of A' and B, where B is defined as the following matrix:

$$B = \begin{bmatrix}
    1 & 2 & 3\\
    2 & 6 & 4 \\
    4 & 5 & 6
\end{bmatrix}$$

We are unable to find the product of A'B because the number of columns in A' (2) does not equal the number of rows in B (3). We are able to find the product of BA' as follows:

$$ A' \times B = \begin{bmatrix}
    1 & 2 & 4 \\
    5 & 7 & 6
\end{bmatrix} \times \begin{bmatrix}
    1 & 2 & 3\\
    2 & 6 & 4 \\
    4 & 5 & 6
\end{bmatrix} = \begin{bmatrix}
    21 & 34 & 35 \\
    42 & 82 & 79
\end{bmatrix}$$

Or, in R:

```{r 8_mat4, echo=TRUE}
B <- matrix(c(1, 2, 3, 2, 6, 4, 4, 5, 6), 2, 3)
BxAprime <- B %*% Aprime
BxAprime
```

In ordinary algebra, 1 is the identity element, whereby any number multiplied by 1 returns that number:

$$ 5 \times 1 = 5$$

Similarly, there exists an identity element in matrix algebra, denoted by the symbol `I`. The identity matrix is a square matrix with a 1 in the same pattern, regardless of size:

$$I_{1\times1}=\begin{bmatrix} 1 \end{bmatrix}, I_{2\times2}=\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}, I_{3\times3} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$$

The following demonstrates the identity property for matrices:

```{r 8_mat5, echo=TRUE}
I <- matrix(c(1, 0, 0, 0, 1, 0, 0, 0, 1),3, 3)
AI <- A %*% I
AI
A
```

Lastly, the reciprocal of A is known as the inverse matrix, denoted as $A^{-1}$. In ordinary algebra, the product of a number and its reciprocal is 1. In matrix algebra, the product of a matrix and its inverse is the identity matrix. $AA^{-1}=A^{-1}A=I$ __Note:__ Inverse matrices only exist for square matrices, and not all square matrices possess inverses. The inverse of a matrix can be found via the `solve()` function as follows:

```{r 8_mat6, echo=TRUE}
A.inv <- solve(AxAprime)
A.inv
```

The following R example demonstrates `I` as the product of $AA^{-1}$:

```{r 8_mat7, echo=TRUE}
Aident <- A.inv %*%  A
Aident
```

### Representing System of Linear Equations as Matrices

The previous lab introduced the estimated bivariate linear regression model as follows:

$$\hat{y_i}=\hat{\alpha}+\hat{\beta} x_i$$

Where $\hat{\alpha}$ anf $\hat{\beta}$ could be solved via the following formulas:

$$\hat{\alpha}=\bar{y} - \hat{\beta}\bar{x}$$

$$\hat{\beta}=\frac{cov(x,y)}{var(x)}$$

In this lab we demonstrate how to use matrix algebra to calculate the least-squared estimates. For bivariate linear regression, the above formulas and matrix algebra will produce the same result; however, for multivariable linear regression the underlying methods that develop the above formulas become computationally complex such that matrix algebra is the preferred method to calculate the coefficients. 

Using bivariate regression we explored hypotheses related to how preference for renewable energy is a function of ideology. Sometimes the dependent variable (`y`) is not easily explainable via a single independent variable (`x`), but rather multiple independent variables ($x_0, x_1, ..., x_n$). We can modify the bivariate regression model to append the additional variables of interest, as follows:

 __Note:__ The $\alpha$ intercept coefficient is replaced with $\beta_0$.

$$y_i=\beta_0+\sum^n_{j=1}{\beta_j x_{ij}}+\varepsilon_i$$

Where, n is the number of independent variables, $\beta_0$ is the regression intercept, and $\beta_j$ is the slope for the $j^{th}$ variable. The above model is the general form of a regression model, and as such, will work for bivariate and multivariable models. If $n=1$, the model is exactly the same as the model stated in the textbook and previous lab.

Using this model we can imagine collected data as a system of linear equations. To demonstrate, suppose we collected the following data:

```{r 8_mat8, echo=TRUE}
ex.ds <- data.frame(x = c(1, 2, 3, 4, 5),
                    y = c(1, 1, 2, 2, 4))
ex.ds
```

Using the linear regression model above, we can state the data as a system of linear equations:

$$1 = \beta_0 + \beta_1 * 1$$

$$1 = \beta_0 + \beta_1 * 2$$

$$2 = \beta_0 + \beta_1 * 3$$

$$2 = \beta_0 + \beta_1 * 4$$

$$4 = \beta_0 + \beta_1 * 5$$

Given we are working with two variables, `x` and `y`, our `n` is 1, so we are working with the bivariate linear regression model. Now, we can use ordinary algebra to solve for $\beta_0$ and $\beta_1$. To do so, we will solve for one variable, then solve for the other. Given our system consists of 5 linear equations, the ordinary algebra approach is not practical.

This is where matrix algebra is useful. The general regression model can be expressed in the following matrix form, where we capitalize variables to represent matrices:

$$Y=X\beta_1+\varepsilon$$

The system of linear equations can be converted into the following matrices:

$$Y = \begin{bmatrix}1 \\ 1 \\ 2 \\ 2 \\ 4\end{bmatrix}, X = \begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5\end{bmatrix}, and \beta = \begin{bmatrix}\beta_0 \\ \beta_1\end{bmatrix}$$

__Note:__ The first column of the X matrix is always 1. 

### OLS Regression and Matrices

With data expressed in matrix form, we then use matrix algebra to calculate the least-squared estimates. The least-squared estimates formula is:

$$\hat{\beta}=(X'X)^{-1}X'Y$$

Using the matrices formed from the system of linear equations, we demonstrate calculating the least-squared estimates using R:


```{r 8_ols, echo=TRUE}
Y <- matrix(c(1, 1, 2, 2, 4), 5, 1)
Y
X <- matrix(c(1, 1, 1, 1, 1, 1, 2, 3, 4, 5), 5, 2)
X
```

Now we need to find X' with the `t()` function:

```{r 8_ols2, echo=TRUE}
Xprime <- t(X)
Xprime
```

Next calculate (X'X):

```{r 8_ols3, echo=TRUE}
XprimeX <- Xprime %*% X
XprimeX
```

Now find the inverse of (X'X):

```{r 8_ols4, echo=TRUE}
XprimeXinv <- solve(XprimeX)
XprimeXinv
```

Multiply $(X'X)^{-1}$ by $X'$:

```{r 8_ols5, echo=TRUE}
XprimeXinvXprime <- XprimeXinv %*% Xprime
XprimeXinvXprime
```

Now multiply by Y to find $\hat{\beta}$:

```{r 8_ols6, echo=TRUE}
b <- XprimeXinvXprime %*% Y
b
```

We read this matrix as $\beta_0$ is the first position and $\beta_1$ is the following position. __Note:__ We could just as easily used the method introduced in the last lab. The following R code demonstrates the equivalence:

```{r 8_ols7, echo=TRUE}
df <- data.frame(x = c(1, 2, 3, 4, 5), y = c(1, 1, 2, 2, 4))
covar <- cov(df$x, df$y)
vari <- var(df$x)
bhat <- covar / vari
xbar <- mean(df$x)
ybar <- mean(df$y)
alpha <- ybar - bhat * xbar

alpha
bhat
```

__Note:__ In the previous code block, `alpha` is $\beta_0$ and `bhat` is $\beta_1$.

With the coefficients calculated, our estimated linear regression model is:

$$\hat{y}=-0.10+0.70x$$

Let's check our work using the `lm()` function:

```{r 8_ols10, echo=TRUE}
ols <- lm(Y ~ 0 + X)
ols
b
```

The previous example demonstrated calculating least-squared estimates for a bivariate regression model. Next is an example using matrix algebra to calculate the least-squared estimates for a multivariable linear regression model.

Suppose we have the following data set:

```{r 8_ols11, echo=TRUE}
mv.df <- data.frame(y = c(1, 1, 2, 2, 4),
                    x1 = c(1, 2, 3, 4, 5),
                    x2 = c(1, 2, 2, 4, 3))
mv.df
```

The system of linear equations is:

$1 = \beta_0 + \beta_1 * 1 + \beta_2 * 1$
$1 = \beta_0 + \beta_1 * 2 + \beta_2 * 2$
$2 = \beta_0 + \beta_1 * 3 + \beta_2 * 2$
$2 = \beta_0 + \beta_1 * 4 + \beta_2 * 4$
$4 = \beta_0 + \beta_1 * 5 + \beta_2 * 3$

Converted to matrix form:

$$\begin{bmatrix}1 & 1 & 1 \\ 1 & 2 & 2 \\ 1 & 3 & 2 \\ 1 & 4 & 4 \\ 1 & 5 & 3 \end{bmatrix} \times \begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_2\end{bmatrix} = \begin{bmatrix}1\\1\\2\\2\\4\end{bmatrix}$$

We will use R to find the least-squared coefficients. Note that calculating Bhat in R has been reduced to a single line:

```{r 8_ols12, echo=TRUE}
Y <- matrix(c(1, 2, 2, 4, 4), 5, 1)
X <- matrix(c(1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 1, 2, 2, 4, 3), 5, 3)
Bhat <- (solve((t(X) %*% X))) %*% (t(X) %*% Y)
Bhat
```

Again, we check our work using the `lm()` function:

```{r 8_ols13, echo=TRUE}
ols <- lm(Y ~ 0 + X)
ols
```

## Multiple Regression in R

The R syntax for multiple linear regression is similar to what we used for bivariate regression: add the independent variables to the `lm()` function. Construct a model that looks at climate change certainty as the dependent variable with age and ideology as the independent variables:

```{r 8_mlr, echo=TRUE}
sub.ds <- ds %>%
  dplyr::select("glbcc_cert", "ideol", "age") %>%
  na.omit()
model1 <- lm(sub.ds$glbcc_cert ~ sub.ds$ideol + sub.ds$age)
```

Now look at the model:

```{r 8_mlr2, echo=TRUE}
summary(model1)
```

Before interpreting these results, we need to review partial effects. Chapter 12 of the textbook discusses partial effects in great detail. Essentially, multivariable regression "controls" for the effects of other dependent variables when reporting the effect of one particular variable. This is not accidental. Explore this for our model. First construct a bivariate regression model for each of independent variable in the multivariable regression model:

```{r 8_mlr3, echo=TRUE}
model2 <- lm(sub.ds$glbcc_cert ~ sub.ds$ideol)
model3 <- lm(sub.ds$glbcc_cert ~ sub.ds$age)
stargazer(model2, model3, single.row = TRUE, type = "text")
```

Notice the ideology coefficient in the bivariate model is larger than the ideology coefficient in the multivariable regression model. The same is also true for the age variable. This is because the bivariate model reports the total effects of X on Y, but the multivariable regression model reports the effects of X on Y when "controlling" for the other independent variables. Look at the multivariable regression model again:

```{r 8_mlr4, echo=TRUE}
summary(model1)
```

To interpret these results: a one unit increase in ideology corresponds with a -0.392 unit decrease in climate change certainty with all other variables held constant at their means. Further, given the p-value < $\alpha$ = 0.05, the change in climate change certainty is statistically significant. Further, assessing the p-value of the age coefficient yields that the partial effect of age on climate change certainty is not statistically significant. You may have noticed a difference in the p-value for the age variable's coefficient between the bivariate and multivariable regression models. This is due to the reduction in error in the model by adding the ideology variable. This is a good example of why, in most cases, multivariable regression provides a clearer picture of relationships. Only looking at age and climate change risk, we could potentially conclude that there is a statistically significant relationship; however, when appending ideology to model, we find that ideology is the more likely cause of change in climate change certainty.

## Hypothesis Testing with Multivariable Regression

Perhaps we want to explore the relationship between opinions about fossil fuels and ideology. The class data set includes a question asking respondents what percentage of Oklahoma's electricity should come from fossil fuels. It is reasonable to posit that more conservative individuals will want a higher percentage of the state's electricity to come from fossil fuels. For this test, we include other independent variables: age, income, and education. We establish a hypothesis that the more conservative a respondent is, the more electricity they want to come from fossil fuels, all other variables held constant. The null hypothesis is that there is no difference in preferred percentage of electricity coming from fossil fuels by ideology. First we look at our variables:

```{r 8_mlr5, echo=TRUE}
str(ds$okelec_foss)
```

Notice that R reads the fossil fuels variable as a factor. We change this to a numeric variable. We coerce it and create a new variable:

```{r 8_mlr6, echo=TRUE}
ds$foss <- as.numeric(ds$okelec_foss)
```

Now let's look at all the variables. First we create a subset of the data and remove missing observations, then use the `skim()` function:

```{r 8_mlr7, echo=TRUE}
ds.sub <- ds %>% 
  dplyr::select("income", "education", "ideol", "foss", "age") %>%
  na.omit()

ds.sub %>%
  skim()
```

Now construct the model:

```{r 8_mlr8, echo=TRUE}
model <- lm(foss ~ income + education + age + ideol, data = ds.sub)
```

Note the different syntax in constructing this model. Instead of writing `dataset$variable` every time, you can write the variable names, and at the end of the model include `data=dataset`. 

Now examine the model:

```{r 8_mlr9, echo=TRUE}
summary(model)
```

To interpret these results, we start with the intercept. In a regression model, the intercept is the expected mean of our dependent variable when our independent variables are 0. In this case the expected mean is 5.83. In some models this has meaning; however, given none of our independent variables adopt a zero value, this provides minimal value to interpretation. Now examine the variable that the hypothesis is concerned with: ideology. We see there is a statistically significant coefficient of 3.07. We interpret this by saying that a one unit increase in ideology (from liberal to conservative) corresponds with a 3.07 unit increase in preferred percentage of electricity coming from fossil fuels, all other variables held constant. There are also statistically significant relationships for age and income, suggesting an increase in those correspond with an increase in the dependent variable as well. The adjusted R squared value of .15 suggests that our model accounts for 15 percent of the variability in the dependent variable. 

Next we need to visualize the model. Visualizing multiple linear regression is not as simple as visualizing bivariate relationships. There is no intuitive way for us to visualize, in one graphic, how all of these variables look while all others are held constant simultaneously. For a relationship with one dependent variable and two independent variables, we can make a three dimensional scatter plot and regression plane; however, these still don't provide a very intuitive visualization. 

### Visualizing Multivariable Linear Regression

The best way to visualize multiple linear regression is to create a visualization for each independent variable while holding the other independent variables constant. Doing this allows us to see how each relationship between the DV and IV looks. Constructing a quick and dirty visualization for each IV in `ggplot2` is simiular to the methods used for bivariate linear regression. We will go into greater detail in the next section when we cover predictions with OLS, but making a quick visualization is rather simple. The `augment()` function from the `broom` package is very useful for this. The function transforms the data created from an OLS model into a tidyverse data frame format. Use `augment()`, then melt the data into long form, and create a `ggplot2` visualization for each IV by using `facet_wrap()`.

```{r 8_mlr10, echo=TRUE}
model %>%
  augment() %>%
  melt(measure.vars = c("ideol", "age", "education", "income"), variable.name = c("IV")) %>%
  ggplot(., aes(value, foss)) +
  geom_smooth(method = "lm") +
  facet_wrap(~IV, scales = "free_x")
```

Now we can see the general relationship between each independent variable and the dependent variable. The `geom_smooth()` function defaults to showing 95% confidence intervals. You can disable the confidence intervals with the `se=FALSE` argument, but that is not recommended. Notice the very large confidence interval in the income visualization, especially at the higher income levels. This happens because there are fewer observations with very high incomes. A smaller sample size leads to a larger standard error, and in turn larger confidence intervals. 


## Predicting with OLS Regression

Regression analysis is performed not only to explain relationships, but also to predict. In R, we can use the model we built to predict values of the dependent variable based on the values of the independent variables. Doing so is rather simple. Recall that our model attempts to explain how much of the state's electricity a respondent thinks should come from fossil fuels as a function of their ideology, age, education, and income. 

To start predicting, we need to identify what values we want to assign to our independent variables. Perhaps you wanted to know the preferred percentage of energy coming from fossil fuels for an individual with a bachelor's degree, an income of 45000, 40 years old, and a moderate (3) ideology. First we need to know the beta coefficients for each variable:

```{r 8_predict, echo=TRUE}
coef(model)
```

Now recall the scalar formula for multiple linear regression:

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \hat{\beta_3}x_3$$

Therefore for our model, the formula would be

$$\hat{y} = \beta_{intercept} + \beta_{income} + \beta_{educ} + \beta_{age} + \beta_{ideol}$$

__Note:__ For a bachelor's degree, the value of the education variable is 6. 

```{r 8_predict2, echo=TRUE}
(5.83 + (.000014 * (45000)) + (-.19 * (6)) + (.19 * (40)) + (3.07 * (3)))
```

Based on the calculation, a predicted result is 22% of the state's electricity should come from fossil fuels. There is a more precise way to do this calculation, as is often the case. Using the `augment()` function from the `broom` package, we can tell R to return predicted values based on specifications of variable values. 

```{r 8_predict3, echo=TRUE}
model %>%
  augment(newdata = data.frame(ideol = 3, income = 45000, education = 6, age = 40))
```

__Note:__ The original estimate is a little off due to rounding. Using `augment()` returns a data frame in tidy format. The .fitted value is the predicted value of interest. You can use the `predict()` function in a similar way, but doing so returns information in vector, not data frame, format. 

The `augment()` function can return multiple values at a time. We can tell R to predict values for a sequence of values, like the range of ideology values. Sequencing one variable while holding the others constant is very common in OLS analysis. The method we will most often use will be holding all other IVs constant at their means. 

```{r 8_predict4, echo=TRUE}
model %>%
  augment(newdata = data.frame(ideol = 1:7, income = mean(ds.sub$income),
                               education = mean(ds.sub$education),
                               age = mean(ds.sub$age)))
```




Recall that the earlier hypothesis stated that the more conservative a respondent, the more electricity they will prefer comes from fossil fuels. We can safely reject the null hypothesis, given the clear relationship and overall evidence that there is a positive relationship. To conlcude, let's build a solid, paper-worthy visualization of the relationship between ideology and opinions on fossil fuels from our model.

__Note__:Until now, we have used `geom_smooth()` to create regression lines. However, the superior method of creating regression lines is to generate predictions outside of ggplot and use `geom_line()` to plot the line and `geom_ribbom()` to plot the confidence intervals. This is because `geom_smooth()` does not let you set the specific IV values. However, the `augment()` (or `predict()`!) function tells R to predict values of the DV based on specific values of the IV. To create a good regression line and confidence interval using the `augment()` function, instruct R to hold all DVs at their means except ideology. Sequence ideology from 1 to 7, and include `se.fit=TRUE`, then assign the fitted values to an object:

The next step is to calculate the confidence interval. This is rather simple! Using the `augment()` function in tandem with the `mutate()` function, we can create a tidy data frame that provides us with all the information needed to construct a visualization with a confidence interval. Assign the data frame to an object called `fit.df`.

```{r 9_predict5, echo=TRUE}
model %>%
  augment(newdata = data.frame(ideol = 1:7, income = mean(ds.sub$income),
                               education = mean(ds.sub$education),
                               age = mean(ds.sub$age))) %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit) -> fit.df
fit.df
```

Now build the visualization:

```{r 8_mlr12, echo=TRUE}
ggplot(fit.df, aes(ideol, .fitted)) +
  geom_line(size=1.5, color = "dodgerblue2") +
  geom_ribbon(aes(ymax = upper, ymin = lower), alpha = .5, fill = "dodgerblue2") +
  ggtitle("Fossil Fuel Energy by Ideology") +
  ylab("% of State's Electricity Should Come From Fossil Fuels") +
  xlab("Ideology") +
  scale_x_continuous(breaks=c(1:7), labels = c("1", "2", "3", "4", "5", "6", "7")) +
  coord_cartesian(ylim = c(15, 40), xlim = c(1, 7)) +
  theme_bw()
```





<!--chapter:end:08-multivariate.Rmd-->

# Categorical Explanatory Variables, Dummy Variables, and Interactions

This lab focuses on ways in which we use and understand categorical independent variables. So far the independent variables we have worked with have been interval or ordinal data. When working with categorical data, there are different approaches and techniques of interpretation. The following packages are required for this lab: 

1. tidyverse
2. psych
3. stargazer
4. interplot
5. car
6. reshape2
7. broom

## Dummy Variables

We often have situations in the social sciences that require constructing models to include qualitative variables. To facilitate this, we employ dichotomous dummy variables to make the model function via 0s and 1s. When using dichotomous dummy variables for catergorical data, the presense of the category of interest receives a value of 1 and in its absence the value is 0.

To demonstrate dummy variables in models we will look to the class data set. The gender variable is coded as a 0 for women and 1 for men. This makes it a dummy variable for men, with women as the referent group. If we wanted to construct a model that looked at how certainty of climate change varied by ideology, education, income, age, and gender, our model would look like this:

$$Y_i=\alpha + \beta_{ideol} + \beta_{educ} + \beta_{inc} + \beta_{age} + \beta_{gend} + \epsilon_i$$
Where `B_gend` is a binary indicator of gender, 0 for female and 1 for male. This means that when gender is female, gender equals 0.

Pull the data, omit missing variables, and look at the gender variable we are going to use:


```{r dum, echo=TRUE}
ds.sub <- ds %>% dplyr::select("ideol", "education", "income",
                                       "age", "gender", "f.gender",
                                       "glbcc_cert", "f.party", "glbcc_risk",
                                       "glbwrm_risk_fed_mgmt") %>%
  na.omit()
```



```{r 9_dum, echo=TRUE}
table(ds.sub$f.gender)
```

__Note:__ The factored gender variable lists men as 0 and women as 1. If you look at a table of the non-factored version, it shows the opposite. This is because R reads factored variables in alphabetical order. If we wanted to change the order of the factored variable:

```{r 9_dum2, echo=TRUE}
ds.sub$f.gender <- factor(ds.sub$gender, levels = c(0, 1), labels = c("Women", "Men"))
ds.sub %>%
  count(f.gender, gender)
```

When working with a binary categorical explanatory variable (like the gender variable), you can use the numeric version of the variable. However, when working with categorical variables with more than two categories, it is often easier to use the factored version of the variable, for reasons we will discuss shortly. We will use the factored gender variable in our model:

```{r 9_dum3, echo=TRUE}
lm1 <- lm(glbcc_cert ~ ideol + education + income + age + f.gender, data = ds.sub)
summary(lm1)
```

Relying on our understanding from previous labs, we know that ideology and education have an effect on someone's certainty of climate change; however, now we want to look at the role gender plays. We used the factored version of gender in the model, so we need to interpret the results as such. The summary table says "f.genderMen", which means the variable is a dummy variable for men, with the referent category being women. The coefficient is interpreted as: the difference in the dependent variable from the referent category to the dummy category. In this case, the coefficient is statistically significant. To interpret it, we say that men are on average .408 units more convinced of climate change, on a scale of 0 to 10, all else held constant. The rest of the coefficients are interpreted as they have been in the past. But now you likely have a more accurate model, since you are "controlling" for gender. 

Visualizing the model should likely this more clear. If we want to visualize the relationship between ideology and climate change risk in our model __by gender__, we go about it in a similar way to previous visualizations:

1. Generate fitted values and standard errors for each ideology level and gender via the `augment()` function.
2. Use the `full_join()` function to join the data frames for men and women together.
3. Calculate the upper and lower bounds of the confidence interval using `mutate()`
4. Visualize

```{r 9_dum4.5, echo=TRUE}
lm1 %>%
  augment(newdata = data.frame(f.gender = "Women",
                               ideol = 1:7,
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age))) -> fit.w
lm1 %>%
  augment(newdata = data.frame(f.gender = "Men",
                               ideol = 1:7,
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age))) -> fit.m
```

Now join them:

```{r 9_dum4.7, echo=TRUE}
fit.df <- full_join(fit.w, fit.m)
```

Now add confidence intervals:

```{r 9_dum4.75, echo = TRUE}
fit.df <- fit.df %>%
  mutate(up = .fitted + 1.96 * .se.fit,
         low = .fitted - 1.96 * .se.fit)
```

Now visualize! To separate men and women, you can use `group=DummyVariable`, to separate the two groups. Unfortunately, this does not give a way to distinguish between them. You can also use `color=DummyVariable` to separate the groups, assign colors, and include a legend. 

```{r 9_dum4, echo=TRUE}
ggplot(fit.df, aes(ideol, .fitted, color = f.gender)) +
  geom_line(size=1.5) +
  geom_ribbon(aes(ymin = low, ymax = up, fill = f.gender), alpha = .5)
```

You can think of the effect of dummy variables as a change in the value of the intercept. In this case our dummy variable for men is about 0.41, and you will notice that the line for men looks about that much above the women line. 

### Multiple Dummy Variables

Sometimes multiple dummy variables are necessary in models. This is the case when you need to include categorical variables with greater than two options, such as ideology (e.g., Republican, Democrat, Independent, Other). When working with these categorical variables, you need to select a referent group. Sometimes this decision is driven by the theory and or by convenience. R will automatically select a referent group if nothing is supplied. When using categorical variables with multiple options, the model will consist of multiple dummy variables for each of the groups (minus the referent group). You will always have one less dummy variable than the number of options. For example, for Republican, Democrat, Independent, and Other as the options, with Republican as the referent group, you will have 3 dummy variables.

Let's look at an example using political party as a dummy variable. Start by looking at a table of the factored party variable:

```{r 9_dum5, echo=TRUE}
table(ds.sub$f.party)
```

__Note:__ Democrat is listed first, therefore it is the referent category. Therefore, in a model, there would exist coefficients and dummy variables for each of the political parties sans Democrat. In other words, R reads ideology as a factored variable and treats every party option as an independent dummy variable with Democrats as the referent category. Let's create a model based on the model we used earlier, but include the factored party variable as an independent variable. Due to potential multicollinearity issues, we will omit the ideology variable from the model. To make calculations simpler, we're going to use the non-factored version of gender. Since its a binary group, this will not change any of the coefficients:

```{r 9_dum6, echo=TRUE}
lm2 <- lm(glbcc_cert ~ f.party  +education + income + age + gender, data = ds.sub)
summary(lm2)
```

We can see our model suggests that Independents and Republicans are, on average, less certain about climate change. The coefficient for Other is not significant, which makes sense given Other could indicate a panoply of political parties spanning the ideological spectrum. 

Now we will visualize this model. Dummy variables are similar to performing t-tests, but with statistical controls. First we  predict values based on party affiliation using the `augment()` function for R to return predicted climate change certainty values based on political party, along with associated standard errors, holding each other variable constant. Assign the newly created object to a data frame and print the data frame:

```{r 9_dum7, echo=TRUE}
lm2 %>%
  augment(newdata = data.frame(f.party = c("Dem", "Ind", "Other", "Rep"),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age),
                               gender = mean(ds.sub$gender))) -> fit2.df
fit2.df

```


We will also calculate confidence intervals using the `mutate()` function. Remember, a t score of 1.96 is associated with 95% confidence intervals:

```{r 9_dum8, echo=TRUE}
fit2.df %>%
   mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit) -> fit2.df
```

With the data frame constructed, next build the visualization. Our x-axis is party and the y-axis is climate change certainty. We'll use `geom_point()` and `geom_errorbar()` to build the point estimates and confidence intervals:

```{r 9_dum13, echo=TRUE}
ggplot(fit2.df, aes(x = f.party, y = .fitted)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
  ylim(4, 8) +
  ggtitle("Climate Change Certainty by Political Party") +
  ylab("Climate Change Certainty") +
  xlab("Political Party")
```

Perhaps you noticed that our model suggests that gender also plays a role. Next we are going to breakdown the relationship by party and gender simultaneously, by creating different predictions for each gender within each party. We create a new model including the factored gender variable:

```{r 9_dum14, echo=TRUE}
lm3 <- lm(glbcc_cert ~ f.party  + education 
          + income + age + f.gender, data = ds.sub)
summary(lm3)
```

The difference between the models is that the factored gender variable is used, which does not change any of the results. Now we follow a similar process in constructing the graphic, except we predict different values for men and women, and build data frames separately before combining them. Use `augment()` twice, once for men and once for women, then use `full_join()` to combine them into one data frame. From there we can calculate the confidence intervals just like we always do:

```{r 9_dum15, echo=TRUE}
lm3 %>%
  augment(newdata = data.frame(f.party = c("Dem", "Ind", "Other", "Rep"),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age),
                               f.gender = "Men")) -> fit3.m
lm3 %>%
  augment(newdata = data.frame(f.party = c("Dem", "Ind", "Other", "Rep"),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age),
                               f.gender = "Women")) -> fit3.w

fit3.df <- full_join(fit3.m, fit3.w)
```

Now the confidence intervals:

```{r 9_dum16, echo=TRUE}
fit3.df %>%
  mutate(up = .fitted + 1.96 * .se.fit,
         low = .fitted - 1.96 * .se.fit) -> fit3.df
fit3.df
```

Build the visualization. Creating a grouped bar plot will allow us to see each party broken down by gender. To create a grouped bar plot, include `position = position_dodge()` in the `geom_bar()` and `geom_errorbar()` functions.

```{r 9_dum22, echo=TRUE}
ggplot(fit3.df, aes(f.party, .fitted, color = f.gender)) +
  geom_point(stat = "identity", position = position_dodge())  +
  geom_errorbar(aes(ymin = low, ymax = up))
```

It appears that there might be a difference in certainty of climate change between the political parties by gender. If we were to test how political beliefs vary as a function of gender and are related to opinions about climate change certainty, we would need to explore interaction terms.

## Interactions

Interactions occur when the effect of one x is dependent on the value of another within a model. Previously, the value at any point of x was the same across all levels of another in predicting y. To demonstrate an interaction effect we will explore the interaction of gender and ideology on climate change certainty. We include the other predictors and specify this model: 

$$y_i=\beta_0 + \beta_1*(ideol) + \beta_2*(gender) + \beta_3*(ideol*gend) + \beta_4*(educ) + \beta_5*(inc) + \beta_6*(age) + \varepsilon_i$$

where gender is a binary indicator of men (1) or women (0). To specify this model in R:

```{r 9_int, echo=TRUE}
lm4 <- lm(glbcc_cert ~ ideol * gender + education + income + age , data = ds.sub)
summary(lm4)
```

__Note:__ The formula includes an ideology and gender interaction but does not specify the variables individually. R interprets the interaction and includes the separate variable terms for you. To interpret the results, notice that the `ideol:gender` interaction coefficient is not statistically significant.

Let's review a new model looking at climate change risk instead of certainty. The independent variables, and interaction, remain the same:

```{r 9_int2, echo=TRUE}
lm5 <- lm(glbcc_risk ~ ideol * f.gender + education + income + age , data = ds.sub)
summary(lm5)
```

As would be expected, ideology, education and income also exert statistically significant influence. Further, the interaction of ideology and gender is also statistically significant. To interpret these results, we would say that there is an interaction (ideology affects perceived climate change risk as a function of gender). We also know that the slope of the lines is negative. Often times the most intuitive way to understand interactions is to make predictions and visualize them.

Visualizing an interaction effect when the interaction term is binary is rather simple. There are two possible lines, when z=0 and when z=1, in this case when the gender is female or male. This makes the visualizing process similar to the first visualization, with the dummy variable:

```{r 9_int3, echo=TRUE}
lm5 %>%
  augment(newdata = data.frame(ideol = 1:7,
                               f.gender = "Men",
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age))) -> fit5.m

lm5 %>%
  augment(newdata = data.frame(ideol = 1:7,
                               f.gender = "Women",
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income),
                               age = mean(ds.sub$age))) -> fit5.w

full_join(fit5.m, fit5.w) %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit) -> fit5.df
fit5.df
```

Now the visualization:

```{r 9_int3.5, echo=TRUE}
ggplot(fit5.df, aes(ideol, .fitted, color = f.gender)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = f.gender), alpha = .5)
```

Notice how the slopes are different for men and women. The slope is steeper for men, suggesting that there is more of an interaction for men.


The difference between the first predicted value and the last is called the "first difference." Find the first differences for men and women:

```{r 9_int4, echo=TRUE}
fit5.m$.fitted[1] - fit5.m$.fitted[7]
fit5.w$.fitted[1] - fit5.w$.fitted[7]
```


We can tell that the first difference is larger for men. They have a higher first value and a lower last value.

### Interactions with Two Non-binary Variables

Theory and hypotheses often dictate the need to include an interaction between two variables when neither are binary. This makes the interpretation of interaction coefficients difficult, but nonetheless the process is still the same. Suppose you want to explore people's attitudes about the role of federal government in climate change management. We can theorize that two primary predictors of these attitudes are ideology and climate change risk. Conservatives tend  to oppose federal government intervention and someone more concerned about climate change should likely support the attitudes about the role of federal government and perceived risk of climate change will be different among liberals and conservatives. We could further theorize that the relationship between federal climate change management and climate change risk will be positive regardless of group, with individuals perceiving greater risk from climate change supporting more government management, but that relationship will be weaker for conservatives. We will specify the following hypothesis:

_The relationship between perceived climate change risk and support for federal government management of climate change will be positive, but conditional on ideology. The relationship will be more pronounced for liberals and less pronounced for conservatives._ 

First take a look at the federal climate change management variable:

```{r 9_int7, echo=TRUE}
describe(ds.sub$glbwrm_risk_fed_mgmt)
```

We see that it is an ordinal variable ranging from 0 (not involved) to 10 (very involved).

Now we should specify the model, including appropriate controls:

```{r 9_int8, echo=TRUE}
lm6 <- lm(glbwrm_risk_fed_mgmt ~ ideol * glbcc_risk + education + gender + income 
          + age, data = ds.sub)
summary(lm6)
```

Right from the start we see that ideology and climate change risk both play significant roles. These coefficients are both statistically significant and substantive. A one unit change in either of the variables corresponds with more than a half point change in opinions about federal climate change management. Education and income also play roles, and notice that the interaction is significant. There is not a lot of intuitive interpretation we can gather from the coefficient alone; however, we see that it is very small, .026, and will likely not alter the slopes much. The best way to understand an interaction of two non-binary variables is to make predictions and visualize. We start with predictions.

First predict values of y for liberals:

```{r 9_int9, echo=TRUE}
lm6 %>%
  augment(newdata = data.frame(ideol = (1), gender = mean(ds.sub$gender), 
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> lib
```

Now conservatives:

```{r 9_int10, echo=TRUE}
lm6 %>%
  augment(newdata = data.frame(ideol = (7), gender = mean(ds.sub$gender), 
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> con
```

Now compare, starting with liberals:

Liberals:
```{r 9_int11, echo=TRUE}
lib$.fitted
```

Conservatives:

```{r 9_int12, echo=TRUE}
con$.fitted

```

Find the first difference of each:

Liberals: 
```{r 9_int13, echo=TRUE}
lib$.fitted[10] - lib$.fitted[1]
```

Conservatives:

```{r 9_int14, echo=TRUE}
con$.fitted[10] - con$.fitted[1]

```

There is a greater first difference for conservatives. Combined with a significant interaction coefficient that is positive, we can start to see that perhaps the slopes of the lines are steeper for conservatives. Let's build a visualization that includes a prediction line for every ideology level. To do so we need to:

1. Generate predictions for every ideology score.
2. Put those predictions in a data frame.
3. Visualize each individual line on a single plot.

It might look like a lot of code, but the process is rather simple!

Start with an ideology of 1 and then go to 7:

```{r 9_int15, echo=TRUE}
lm6 %>%
  augment(newdata = data.frame(ideol = 1, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id1
lm6 %>%
  augment(newdata = data.frame(ideol = 2, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id2
lm6 %>%
  augment(newdata = data.frame(ideol = 3, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id3
lm6 %>%
  augment(newdata = data.frame(ideol = 4, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id4
lm6 %>%
  augment(newdata = data.frame(ideol = 5, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id5
lm6 %>%
  augment(newdata = data.frame(ideol = 6, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id6
lm6 %>%
  augment(newdata = data.frame(ideol = 7, gender = mean(ds.sub$gender),
                               education = mean(ds.sub$education),
                               income = mean(ds.sub$income), age = mean(ds.sub$age),
                               glbcc_risk = seq(1, 10, 1))) -> id7
```

Now put all the data frames into one data frame. The `full_join()` function only merges two data sets at a time, so we are going to join them step-by-step. Recall that when piping one function to the next, a `.` can be used to stand in for the previous data frame, therefore we can build one data frame like this:

```{r 9_int16, echo=TRUE}
full_join(id1, id2) %>%
  full_join(., id3) %>%
  full_join(., id4) %>%
  full_join(., id5) %>%
  full_join(., id6) %>%
  full_join(., id7) -> fit6.df
```

Next we are going to create a color scale. This will help us interpret the visualization because we are visualizing multiple lines. Create a scale with 7 values, one for each ideology score, that goes from blue (liberal) to red (conservative):

```{r 9_int17, echo=TRUE}
col_scale<-colorRampPalette(c("#0200bd50","#FF000050"))(7)
```

Now build the visualization. Let's make this a complete visualization, including axis labels and a title. Because of the numeric nature of the ideology variable, `ggplot2` will try and read it as a numeric set of values. However, we are creating a different line for each level of ideology, so we need to instruct `ggplot2` to treat ideology as a factor. Use `as.factor()` to do so. 

```{r 9_int18, echo=TRUE}
ggplot(fit6.df, aes(glbcc_risk, .fitted, color = as.factor(ideol))) +
  geom_line(size = 2) +
  scale_color_manual(values = c(col_scale[1], col_scale[2], col_scale[3],
                       col_scale[4], col_scale[5], col_scale[6], col_scale[7]),
                     labels = c("1", "2", "3", "4", "5", "6", "7"),
                     name = "Ideology") +
  ggtitle("Federal Climate Change Management by Ideology") +
  xlab("Climate Change Risk") +
  ylab("Preferred Level of Federal Involvement") +
  theme_bw()
```

Consider everything we've found so far. The positive interaction coefficient, the larger first difference for conservatives, and now this visualization. It is quite clear that the relationship between climate change risk and preferred level of federal climate change management appears to be stronger for conservatives. The slopes of the lines become steeper as the ideology score increases. This was not what we hypothesized, and therefore we cannot reject the null hypothesis.

## Releveling Variables

As mentioned earlier, R can sometimes re-order a variable. This is the case for factored variables, when R automatically reads them alphabetically. Sometimes you need to relevel a variable by necessity and othertimes by preference. Let's look at the factored party variable:

```{r 9_rel, echo=TRUE}
table(ds$f.party)
```

Notice the variable is in alphabetical order. If we included this variable in our model, R would read the Democrat category as the referent group. Perhaps you wanted Republicans to be the referent group. There are a couple ways to do this. The first way would be to refactor the numeric version of the variable. First look at the unfactored version:

```{r 9_rel2, echo=TRUE}
table(ds$party)
```

We compare this to the factored version and extrapolate that the 2 value indicates Republicans. So when factoring the variable we will list 2 first:

```{r 9_rel3, echo=TRUE}
ds$f.party2 <- factor(ds$party, levels = c(2,1,3,4), labels = c("Rep", "Dem", "Ind", "Other"))
table(ds$f.party2)
```

This is one way to go about it. This way has its merits, such as wanting to re-order every level of the variable, not just the referent group. But there is another way, one that changes the referent group one: using the `relevel()` function, and indicating the referent group with the `ref=` argument. Let's do so, and make Republicans the referent group. Make sure R reads the `f.party` variable as a factor!

```{r 9_rel4, echo=TRUE}
ds$f.party3 <- relevel(as.factor(ds$f.party), ref = "Rep")
table(ds$f.party3)
```

## Interaction Plots

Our exploration of interactions plotted estimated Y values as X varies as a function of Z. Specifically, we looked at the relationship between climate change management and climate change risk as a function of ideology. We plotted prediction lines for every level of ideology. However, there is another way we can explore interaction effects. Using the `interplot()` function, we calculate and visualize estimates of the coefficient of an independent variable in an interaction. Meaning, instead of looking at predicted values of a dependent variable, we are looking at the estimated effect of an independent variable on a dependent variable in a model that includes a two-way interaction. 

To create an interaction plot for our earlier model we need to specify the two variables in the interaction. The first variable we specify is the variable of interest, the one for which we want estimated coefficients. The second is the other variable in the interaction.

```{r 9_ip, echo=TRUE}
interplot(lm6, var1="glbcc_risk", var2 = "ideol")
```

This plot graphs the estimated coefficient of climate change risk by ideology score. The estimated effect of climate change risk on federal climate change management appears stronger for more conservative individuals. 

Instead of plotting individual estimates, we could plot a line that also visualizes the relationship. To do this we would specify `hist=T`, which plots a histogram on the bottom and a line for the estimated coefficients. Let's add titles on this plot as well:

```{r 9_ip2, echo=TRUE}
interplot(m=lm6, var1="glbcc_risk", var2="ideol", hist=T) +
  ggtitle("Estimated Coefficient of Climate Change Risk by Ideology") +
  theme_bw() +
  xlab("Ideology: Liberal to Conservative") +
  ylab("Climate Change Risk Coefficient")
```

<!--chapter:end:09-categorical.Rmd-->

# Non-linearity, Non-normality, and Multicollinearity

This lab focuses on issues that arise with non-linearity, non-normality, and multicollinearity. We begin with non-linearity. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
4. stargazer
5. reshape2
6. skimr
7. broom

## Non-linearity

### Exploring Non-linearity

A critical assumption of OLS is that the relationship between variables is linear in their functional form, therefore it is imperative to inspect whether a model consist of non-linear relationships between variables of interest. As the book describes, regression diagnostics is largely an art. To demonstrate, suppose you want to examine the relationship between ideology and candidate support in the 2016 presidential election. The survey associated with the class data set asked respondents to indicate on a scale of 1 to 5 the level of support they have for the candidate they voted for in the 2016 presidential election. We will start by exploring data in preparation for a linear regression model:

```{r 10_poly, echo=TRUE}
sub <- ds %>%
  dplyr::select("vote_cand_spt", "ideol","education",
                                    "income", "age", "gender", "f.gender",
                                    "glbcc_cert", "f.party", "glbcc_risk",
                                    "cncrn_econ") %>%
  na.omit()
```

Now examine the candidate support variable:

```{r 10_poly2, echo=TRUE}
psych::describe(sub$vote_cand_spt)
```

Now a table:

```{r 10_poly3, echo=TRUE}
table(sub$vote_cand_spt)
```

We can see that the modal value is 3, a response indicating moderate support for the candidate the respondent voted for. We also observe a negative skew indicating more responses in the higher values.

Next build the linear regression model:

```{r 10_lin, echo=TRUE}
model <- lm(vote_cand_spt ~ ideol + education + income + age + gender, data = sub)
summary(model)
```

Based on the p-values for the model variables, the ideology variable does not appear to help us understand what candidate the responded supported; however, we should examine the linearity of the variables. One way to do this is to plot the residuals by the values of the independent variables. Residual relationships should constitute a straight line with the residuals spread around a line at zero. To build this visualization we need to:

1. Use `augment()` to predict values and create a data frame containing fitted values and residuals
2. Melt the data into long form, sorted by independent variables.
3. Visualize the  relationship between the residuals and IVs simultaneously using `facet_wrap()`.

First the residuals and fitted values. Use `head()` to look at the first five observations in the data frame:

```{r 10_lin2, echo=TRUE}
model %>%
  augment() -> m.df
head(m.df)
```

Now we need to melt the data into rows with unique id-variable combinations.

```{r 10_lin3, echo=TRUE}
m.df %>%
melt(measure.vars = c("ideol", "education", "income", "age",
                                       "gender", ".fitted")) -> m.df
head(m.df)
```

The next step is to plot the residuals by the values of the independent variables. We're going to use `geom_smooth()` to create a loess line that approximates the spread of our data, and we will use `geom_hline()` to plot a horizontal line at 0. Then we will use `facet_wrap()` to tell R to create different plots for each independent variable of the model:

```{r 10_lin4, echo=TRUE, warning=FALSE}
ggplot(m.df, aes(value, .std.resid)) +
  geom_point(shape = 1) +
  geom_smooth(aes(value, .std.resid), method = "loess") +
  geom_hline(yintercept = 0) +
  facet_wrap(~variable, scales = "free_x")
```

We can see there are some potential non-linear relationships; for instance, the ideology variable. The next step is to consider adding an exponent the ideology variable. __Note:__ We want to avoid over fitting our model to the data. Therefore it is important to have an understanding of why you are including an exponent.

For example, when thinking about how ideology might influence candidate support, you could imagine this might be an instance when a quadratic model would be appropriate. Think about it: it wouldn't make sense to theorize that the more conservative an individual, the more enthusiastic they were for the candidate they voted for (regardless of who the candidate was), but it **does** make sense to theorize that the more ideologically extreme an individual is (very liberal or very conservative) the more they supported the candidate they voted for. Perhaps the moderates voting in the 2016 election felt left our or alienated by the polarized political environment, and therefore might have had less support for the candidate they voted for. 

With this in mind, let's build a new model to include ideology as a squared term. __Note:__ The syntax to use a polynomial is: `poly(var,# of powers)`. The `poly` function creates an independent variable for each of the powers as required to create orthogonal power terms. Let's construct the model:

```{r 10_poly4, echo=TRUE}
pol <- lm(vote_cand_spt ~ poly(ideol,2) + education + income + age + gender, data = sub)
summary(pol)
```

We see that our squared ideology term is statistically significant, but the coefficient may not provide us an intuitive interpretation. Before visualizing the model, it may be helpful to compare the new model to the previous, via the `anova()` function, to compare their fit to the data:

```{r 10_comp, echo=TRUE}
anova(model, pol)
```

The significant p-value for the second line implies the second model is a better fit. We can also compare the adjusted $R^2$ values:

```{r 10_comp2, echo=TRUE}
stargazer(model, pol, single.row = TRUE, type = "text")
```

The coefficients for the individual variables in the quadratic model are statistically significant, suggesting they help us understand our dependent variable. Let's move onto the visualization. Since ideology is our variable of interest, we will visualize the relationship between ideology and candidate support while holding the other variables constant at their means.

Start by looking at a scatter plot. To assist we need to jitter the data because ideology is a discrete variable:

```{r 10_poly5, echo=TRUE}
ggplot(sub, aes(ideol, vote_cand_spt)) +
  geom_jitter(shape = 1) 
```

Now add the regression line and confidence interval. To do this we will:

1. Create predicted values and standard error values of candidate support using the `augment()` function, while holding all other values constant at their mean.
2. Generate upper and lower limits of the confidence interval using `mutate()`
3. Visualize.

First: predict. We are going to sequence ideology from 1 to 7 by 0.1 instead of by 1 to produce a smoother line:

```{r 10_poly6, echo=TRUE}
pol %>%
  augment(newdata = data.frame(ideol = seq(1,7,.1),
                               education = mean(sub$education),
                               income = mean(sub$income),
                               age = mean(sub$age),
                               gender= mean(sub$gender))) -> pol.df
```

Now create the upper and lower limits of the confidence interval:

```{r 10_poly7, echo=TRUE}
pol.df %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit) -> pol.df
```

The next step is to visualize. Use `geom_line()` to create the line, and `geom_ribbon()` to create the confidence interval. For practice, let's label the axes and add a title. 

```{r 10_poly9, echo=TRUE}
ggplot(pol.df, aes(ideol, .fitted)) +
  geom_line(size = 1, color = "dodgerblue") +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "dodgerblue", alpha = .5) +
  coord_cartesian(ylim = c(2:5), xlim = c(1:7)) +
  xlab("Ideology") +
  ylab("Support for Candidate") +
  ggtitle("Ideology and Support for Candidate") +
  theme_bw()
  
```

This visualization shows the relationship specified in our theory: that the relationship between ideology and candidate support is quadratic, where more ideologically extreme individuals have more support for the candidate they voted for than moderates. 

Let's use an example where a cubed exponent would be appropriate. A very common model we've used has been exploring the relationship between ideology and climate change risk. Perhaps the relationship between the two is not best described linearly. Let's first make a model we've looked at many times before:

```{r 10_cube, echo=TRUE}
lm1 <- lm(glbcc_risk ~ age + gender + ideol + education, data = sub)
summary(lm1)
```

Now plot the residuals by the independent variables:

```{r 10_cube2, echo=TRUE, warning=FALSE}
lm1 %>%
  augment() %>%
  melt(measure.vars = c("ideol", "education", "age",
                        "gender", ".fitted")) %>%
ggplot(., aes(value, .std.resid)) +
  geom_point(shape = 1) +
  geom_smooth(aes(value, .std.resid), method = "loess") +
  geom_hline(yintercept = 0) +
  facet_wrap(~variable, scales = "free_x")
```

Looking at the ideology variable, we can see that it is likely not linear. It looks more like the loess line moves above and below the line at zero. This may imply that a cube term might be appropriate. Build a model that cubes ideology:

```{r 10_cube3, echo=TRUE}
cubed <- lm(formula = glbcc_risk ~ age + gender + poly(ideol, 3)  + education, data = sub)
summary(cubed)
```

Now compare the two models:

```{r 10_cube4, echo=TRUE}
stargazer(lm1, cubed, single.row = TRUE, type = "text")
```

It actually looks like the cube term does not describe the data very well. The adjusted R squared appears to marginally increase in the cubed model, but that does not tell us much. Let's use ANOVA:

```{r 10_cube5, echo=TRUE}
anova(lm1, cubed)
```

The ANOVA test tells us that our cubed model is a better fit. This is likely due to the square term, which is statistically significant. Nonetheless, let's visualize this so you have experience seeing cubed lines. 

Follow the same steps as last time, predicting values, calculating the confidence interval, and plotting using `geom_line()` and `geom_ribbon()`. 

```{r 10_cube6, echo=TRUE}
cubed %>%
  augment(newdata = data.frame(ideol = seq(1,7,.1),
                               education = mean(sub$education),
                               age = mean(sub$age),
                               gender= mean(sub$gender))) %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit) -> cube.df

ggplot(cube.df, aes(ideol, .fitted)) +
  geom_line(size = 1, color = "dodgerblue") +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "dodgerblue", alpha = .5) +
  coord_cartesian(ylim = c(1:10), xlim = c(1:7)) +
  xlab("Ideology") +
  ylab("Climate Change Risk") +
  ggtitle("Ideology and Climate Change Risk") +
  theme_bw()
```

## Non-normality

This section will go over the problem of non-normality and how to deal with it. One of the key assumptions of OLS is that the residuals of the model are normally distributed. It is also important to make sure your variables are not skewed too far either negatively or positively. Let's cover how to examine whether residuals are normally distributed, as well as how to handle greatly-skewed variables. 

To begin, suppose you want to examine how different independent variables are related to a vote for Donald Trump in the 2016 presidential election. We need to clean up the class data set variable on 2016 presidential votes. Let's look at it:

```{r 10_non, echo=TRUE}
table(ds$vote_cand)
```

The codebook for the class data set tells us that a response of 0 indicates a vote for Trump, 1 is for Clinton, 2 for Gary Johnson, 3 for Jill Stein, and 4 for a different candidate. Change the variable so that it only includes Trump and Clinton. To transform this variable into a binary indicator of a vote for Trump or Clinton, we need to recode the variable so that responses of 1 equals 0, a response of 0 equals 1, with all else an NA:

```{r 10_non2, echo=TRUE}
ds$v.trump <- car::recode(ds$vote_cand, "0 = 1;1 = 0;else = NA;NA = NA")
table(ds$v.trump)
```

Now pull the variables into a new data set and remove missing observations:

```{r 10_non3, echo=TRUE}
new.ds <- ds %>% dplyr::select("income", "gender", "ideol",
                                       "v.trump", "education") %>%
  na.omit()
```

Now review independent variables that are not binary:

```{r 10_non4, echo=TRUE}
psych::describe(new.ds$income)
psych::describe(new.ds$education)
psych::describe(new.ds$ideol)
```

Look at the density distributions:

```{r 10_non5, echo=TRUE}
new.ds %>%
  melt(measure.vars = c("income", "education", "ideol")) %>%
  ggplot(., aes(value)) +
  geom_density(adjust = 2) +
  facet_wrap(~ variable, scales = "free")
```

It is clear that the income variable has a large positive skew. Education and ideology are slightly skewed. One way to fix a skewed variable is to transform it, often by a log. Let's try that:

```{r 10_non6, echo=TRUE}
new.ds$log.inc <- log(new.ds$income)
```

Now review it:

```{r 10_non7, echo=TRUE}
psych::describe(new.ds$log.inc)
```

```{r 10_non8, echo=TRUE}
ggplot(new.ds, aes(log.inc))+
  geom_density(adjust = 3)
```

The skew appears to be reduced. Transforming a variable does not really change how you use it in a model, but it does change the interpretation. Now the variable is ordered in logs. So a mean of 10.96 indicates 10.96 natural logged income. Now let's build the model, using the log of income instead of the income variable:

```{r 10_non9, echo=TRUE}
lm.trump <- lm(v.trump ~  log.inc + education + gender + ideol, data = new.ds)
summary(lm.trump)
```

Regarding the ideology variable, there really should not be any surprises. More conservative individuals voted for Trump. Education is significant, but the coefficient is rather small. Our model suggests that education helps us explain voting, with more education tending to go with voting for Clinton. Our log.income variable does not help us to explain voting. 

Let's now look at the normality of the residuals. First assign the residuals to our data set:

```{r 10_non10, echo=TRUE}
new.ds$res <- residuals(lm.trump)
```

Now make a density plot of the residuals, but also include a normal curve that has the mean and standard deviation of the residuals:

```{r 10_non11, echo=TRUE}
ggplot(new.ds, aes(res))+
  geom_density(adjust = 3) +
  stat_function(fun = dnorm, args = list(mean = mean(new.ds$res), sd = sd(new.ds$res)), 
                color = "red")
```

The eye test indicates that we might have an issue with non-normality of the residuals. Let's run the Shapiro-Wilk test as well:

```{r 10_non12, echo=TRUE}
shapiro.test(lm.trump$residuals)
```

Recall that the Shapiro-Wilk test tests against the null hypothesis that data are normally distributed. Our test result indicates that the residuals might not be normal, which is corroborated by the visualization. In a future lab we will go over one way to correct this, robust estimators. 

## Multicollinearity

Multicollinearity occurs when one independent variable of a model can be predicted with a high degree of accuracy by another independent variable. Even though perfect multicollinearity is very rare, checking for multicollinearity is an important process. The first way to explore potential multicollinearity is to check the collinearity between independent variables:

```{r 10_mc, echo=TRUE}
new.ds %>%
  dplyr::select(ideol, log.inc, education) %>%
  cor()
```

There does not appear to be any extremely highly-correlated variables. We should find the variance inflation factor, which measures the increase in variance of the other coefficients due to the inclusion of a particular variable:

```{r 10_mc2, echo=TRUE}
vif(lm.trump)
```

Generally speaking, you do not want to have a value greater than 5. This model does not appear to have an issue with multicollinearity. 

Now let's use an example that combines everything we've gone over so far. Let's examine the relationship between square footage of the respondent's home and income, age, and education. Start by selecting the data and removing missing observations:

```{r 10_ex, echo=TRUE}
d <- ds %>%
  dplyr::select("footage", "income", "age", "education") %>% 
  na.omit()

```

Like earlier, we should do a log-transformation of income:

```{r 10_ex3, echo=TRUE}
d$log.inc <- log(d$income)
```

Now build the model:

```{r 10_ex4, echo=TRUE}
mod <- lm(footage ~ age + education + log.inc, data = d)
summary(mod)
```

Check for multicollinearity:

```{r 10_ex5, echo=TRUE}
d %>%
  dplyr::select(age, education, log.inc) %>%
  cor()
```

```{r 10_ex6, echo=TRUE}
vif(mod)
```

Taking all this into account, there does not appear to be a problem with multicollinearity. 

Now let's examine the linearity of the variables. Recall the plot we made earlier that plots the independent variables by the residuals. Let's do that again:

```{r 10_ex7, echo=TRUE}
mod %>%
  augment() %>%
  melt(measure.vars = c("education", "age", "log.inc", ".fitted")) %>%
  ggplot(., aes(value, .std.resid)) +
  geom_point(shape = 1) +
  geom_smooth(method = loess, color = "red") +
  geom_hline(yintercept = 0) +
  facet_wrap(~variable, scales = "free_x")
```

There does not appear to be an issue with non-linearity either, so we have no reason to include exponents in the model. 

The next step is to check for non-normality of the residuals:

```{r 10_ex8, echo=TRUE}
d$res <- residuals(mod)
ggplot(d, aes(res))+
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(d$res), sd = sd(d$res)), 
                color = "red")
```


```{r 10_ex9, echo=TRUE}
shapiro.test(mod$residuals)
```

Our results and visualization indicate that there could be a problem with non-normality. 

Let's take a look at the results of the model again:

```{r 10_ex10, echo=TRUE}
summary(mod)
```

To interpret this model, we would say that a one unit increase in age corresponds with a 9.633 unit increase in square footage of home. Looking at education, we would say that a one unit increase in `log income` corresponds with a 536.9 unit increase in home square footage. Practically speaking though, how large is the difference between the age increase and the log income increase? We can see it is almost a 530 square foot difference, but when thinking about the overall distribution of the square footage variable, is that a lot? 

## Standardizing Coefficients

If you want to compare coefficients of different scales, you need to standardize them. There are three options when standardizing:

1. Standardize the dependent variable.
2. Standardize the independent variables.
3. Standardize all the variables.

Standardizing a variable refers to scaling it in standard deviations. This allows us to compare variables that were originally measured in different units. Let's use our previously developed model, but this time we will standardize the dependent variable only. Use the `scale()` function on the footage variable to standardize it:

```{r 10_stan, echo=TRUE}
d$z.footage <- scale(d$footage)
```

Now build the model and look at the results:

```{r 10_stan2, echo=TRUE}
z.mod1 <- lm(z.footage ~ age + education + log.inc, data = d)
summary(z.mod1)
```

Since we only standardized the dependent variable, we would interpret this as saying that a one unit increase in age corresponds with a .008 standard deviation increase in square footage. For log income, we would say that a one unit increase in log income corresponds with a .45 standard deviation increase in square footage.

Now let's standardize the independent variables only:

```{r 10_stan3, echo=TRUE}
d$z.age <- scale(d$age)
d$z.log.income <- scale(d$log.inc)
d$z.education <- scale(d$education)
```

Next build the model:

```{r 10_stan4, echo=TRUE}
z.mod2 <- lm(footage ~ z.age + z.log.income + z.education, data=d)
summary(z.mod2)
```

Now we would say that a one standard deviation increase in age corresponds with a 134.8 unit increase in square footage, and a one standard deviation increase in log income corresponds with a 376.71 unit increase in square footage. Comparing the coefficients here is rather simple and intuitive. Of course, we next need to standardize all the variables and interpret those.

```{r 10_stan5, echo=TRUE}
z.mod3 <- lm(z.footage ~ z.log.income + z.education + z.age, data = d)
summary(z.mod3)
```

Being careful to interpret this correctly, we would say that a one standard deviation change in log income corresponds with a .32 standard deviation increase in square footage.

<!--chapter:end:10-nonlinear.Rmd-->

# Diagnosing and Addressing Problems in Linear Regression

This lab helps us understand how to diagnose and address potential problems in OLS regression. In the last lab, we addressed the OLS assumptions of linearity, multicollinearity, and normality of residuals. This lab addresses outliers and heteroscedasticity while also providing a refresher on exploring and visualizing your data. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
5. stargazer
6. reshape2
7. stargazer
8. MASS
9. plotly
10. sandwich
11. broom

```{r 11_setup, include=FALSE, message=FALSE}
ds <- read_csv("https://github.com/ripberjt/qrmlabs/raw/master/imdb_2014.csv")
```

## Introduction to the Data

For this lab we will use a data set that contains information on movies from the IMDB website. The initial data set contains almost 60,000 observations, so we will filter the data to make it a little smaller:

```{r 11_int, echo=TRUE}
ds <- filter(ds, year>=1995 & votes>1000 & Short!=1) %>% dplyr::select(rating, length, budget,
                                                                votes, title, year) %>%
  na.omit()
```

Now explore the data. Start with examining the structure of the data set:

```{r 11_int2, echo=TRUE}
str(ds)
```

Look at the first five observations in the data set:

```{r 11_int3, echo=TRUE}
head(ds)
```

To make analysis easier, we can name each row by the title of the movie:

```{r 11_int4, echo=TRUE}
row.names(ds) <- ds$title
```

Now look at the first observations:

```{r 11_int5, echo=TRUE}
head(ds)
```

Explore some descriptive statistics:

```{r 11_int6, echo=TRUE}
describe(ds)
```

We will want to start by cleaning up a couple variables. First, we can scale the budget variable by millions of dollars, scale votes by thousands, and factor the year variable:

```{r 11_int7, echo=TRUE}
ds %>%
  mutate(budget_1m = budget/1000000,
         votes_1k = votes/1000,
         f.year = factor(year)) -> ds
```

The next step should be to look at the univariate distributions. Create histograms for the length, budget, user ratings, and votes variables. First you'll need to melt the data:

```{r 11_int8, echo=TRUE}
melt.ds <- melt(ds, measure.vars = c("length", "budget_1m", "rating", "votes_1k"))
ggplot(melt.ds, aes(value)) +
  geom_histogram(fill="#0000FF") +
  facet_wrap(~variable, scale="free")
```

Now let's look at the bivariate plots for the relationship between length, budget, votes, and rating. This is where we might find potential outliers. Build each visualization and then use `ggplotly()` to create an interactive interface that will allow you to identify individual observations. 

```{r 11_int9, echo=TRUE}
vote <- ggplot(ds, aes(votes_1k, rating, label = title)) +
  geom_point(color = "#0000FF50") +
  geom_smooth(method = "loess", se = FALSE, color = "green") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("# of Votes and Rating")
ggplotly(vote)

```

```{r 11_int10, echo=TRUE}
length <- ggplot(ds, aes(length, rating, label = title)) +
  geom_point(color = "#0000FF50") +
  geom_smooth(method = "loess", se = FALSE, color = "green") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("Length and Rating")
ggplotly(length)

```

```{r 11_int11, echo=TRUE}
budget <- ggplot(ds, aes(budget_1m, rating, label = title)) +
  geom_point(color = "#0000FF50") +
  geom_smooth(method = "loess", se = FALSE, color = "green") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("Budget and Rating")
ggplotly(budget)

```

## Outliers

The next step is to construct a model:

```{r 11_int12, echo=TRUE}
fit1 <- lm(rating ~ length + budget_1m + votes_1k, data = ds)
stargazer(fit1, type = "text", single.row = TRUE)
```

Normally we are primarily interested in how this model explains our data. For this section, we are more interested in the observations that are not explained by this model. Let's identify some outliers. First create predicted ratings based on our model:

```{r 11_out, echo=TRUE}
ds$predicted_rating <- predict(fit1)
```

One simple way to find some possible outliers is to use the `outlierTest()` function:

```{r 11_out2, echo=TRUE}
outlierTest(fit1)
```

We see four potential outliers... let's compare their predicted rating based on their budget, length, and number of votes, to their actual rating:

```{r 11_out3, echo=TRUE}
ds["From Justin to Kelly",c("rating", "predicted_rating")]
ds["You Got Served",c("rating", "predicted_rating")]
ds["Werewolf",c("rating", "predicted_rating")]
ds["Glitter",c("rating", "predicted_rating")]
```

We can see that there is a large discrepancy between these movies' ratings and their predicted ratings. 

There are a variety of ways to visually inspect outliers. Let's start with an influence plot:

```{r 11_out4, echo=TRUE}
influencePlot(fit1)
```

An influence plot shows the residuals by their hat-values. This identifies the Matrix and the first Lord of the Rings as potential outliers, so let's compare their ratings to predicted ratings:

```{r 11_out5, echo=TRUE}
ds["Lord of the Rings: The Fellowship of the Ring, The",c("rating", "predicted_rating")]
ds["Matrix, The",c("rating", "predicted_rating")]
```

These movies are flagged outliers for a different reason than the movies we found earlier. These two movies are rated high, but their predicted ratings are even higher (above the maximum possible value of 10).

Another way to examine outliers is to look at the DFBetas. DFBetas measure the influence of case `i` on the `j` estimated coefficients. Put another way, measuring DFBetas asks how many standard errors a particular beta changes when case `i` is removed. The rule of thumb is if the absolute value of a DFBETA is greater than 2 divided by the square root of n, there could be cause for concern. Let's calculate that value:

```{r 11_out6, echo=TRUE}
df <- 2/sqrt(1261)
df
```

We can find the DFBetas easily:

```{r 11_out7, echo=TRUE}
dfbs.fit1 <- dfbetas(fit1)
head(dfbs.fit1)
```

With a large data set, listing every DFBeta value is not efficient. Instead, we should plot the DFBetas with lines at the calculated value. We will use the `identify()` function to mark specific observations. Each of the coefficients will have its own plot. __Note:__ The `windows()` (or `quartz()`) function does not knit with RMarkdown; however, the function is used to build the plot on the screen as you follow along within the RMD document. 

```{r 11_out8, echo=TRUE, error=TRUE}
windows()
plot(dfbs.fit1[,"length"])
abline(h = c(2/sqrt(1261), -2/sqrt(1261)), lty = 2, col = "red")
identify(dfbs.fit1[,"length"],labels = ds$title)

```


```{r 11_out9, echo=TRUE, error=TRUE}
windows()
plot(dfbs.fit1[,"budget_1m"])
abline(h = c(2/sqrt(1261), -2/sqrt(1261)), lty = 2, col = "red")
identify(dfbs.fit1[,"budget_1m"],labels = ds$title)

```

```{r 11_out10, echo=TRUE, error=TRUE}
windows()
plot(dfbs.fit1[,"votes_1k"])
abline(h = c(2/sqrt(1261), -2/sqrt(1261)), lty = 2, col = "red")
identify(dfbs.fit1[,"votes_1k"],labels = ds$title)

```

All of the diagnostics so far indicate that there are outliers to address. There are a few ways to deal with this: First, you can keep them in the model. This is a perfectly viable method, especially if you don't have a technical or theoretical reason to remove them. Another method of dealing with outliers is to omit them and re-run the model. Let's look at the outliers identified by the outlier test again:

```{r 11_out11, echo=TRUE}
outlierTest(fit1)
```

Omit these using the following operator. Essentially this tells R not to include the rows with the titles of the outlier movies. 

```{r 11_out12, echo=TRUE}
ds.omit <- ds[ !(ds$title %in% c("From Justin to Kelly", "You Got Served", "Werewolf", "Glitter")),]
```

Next make a new model with the `ds.omit` data:

```{r 11_out13, echo=TRUE}
fit.omit <- lm(rating ~ length + budget_1m + votes_1k, data = ds.omit)
```

Compare the two models side by side:

```{r 11_out14, echo=TRUE}
stargazer(fit1, fit.omit, type = "text", single.row = TRUE)
```

Notice the minimal changes. The omitted observations changed 3 of the four coefficients, and increased the adjusted R squared value. 

Another option when dealing with outliers is to use robust regression, which weights the observations based on influence. Make a new model using robust regression using the `rlm()` function. There are two methods, "M" and "MM", and both should be evaluated to determine which model best represents your needs.

```{r 11_out15, echo=TRUE}
fit.m <- rlm(rating ~ length + budget_1m + votes_1k, data = ds, method = "M")
fit.mm <- rlm(rating ~ length + budget_1m + votes_1k, data = ds, method = "MM")
```

Compare the four models:

```{r 11_out16, echo=TRUE}
stargazer(fit1, fit.omit, fit.m, fit.mm, type = "text",single.row = TRUE)
```

The biggest difference here is the residual standard error for the robust models is quite a bit lower. There are also differences in the coefficients. With outliers, there is not a one-size-fits-all solution. Let your theory contribute to what solution you use. 

## Heteroscedasticity

One of the key assumptions of OLS is homoscedasticity (constant error variance). One way to check for this is by making a spread level plot, which allows us to see the spread of the residuals:

```{r 11_het, echo=TRUE}
spreadLevelPlot(fit1)
```

There does not appear to be constant spread of residuals, which could indicate a problem with heteroscedasticity. We can further investigate this by doing a Non-constant Variance Test. This tests the null hypothesis that error variance changes (heteroscedasticity). That is, if you fail to reject the null there exists heteroscedasticity:

```{r 11_het2, echo=TRUE}
ncvTest(fit1)
```

Based on the test and visualization, it is clear there is an issue with heteroscedasticity. There are a couple ways to deal with heteroscedasticity. One method is robust standard errors. Robust standard errors don't change the beta estimates, but rather affect the value of the standard errors, which improve the p-values accuracy. To use robust standard errors for the model:

```{r 11_het3, echo=TRUE}
se.fit1 <- fit1 %>% vcov() %>% diag() %>% sqrt()
vcov.fit1 <- vcovHC(fit1, method = "white1",type = "HC1")
rse.fit1 <- vcov.fit1 %>% diag() %>% sqrt()
```

Now compare the original model to the model using robust standard errors. Use `se=list(se.fit1,rse.fit1)` for R to use the original standard errors for the first model and robust for the second.

```{r 11_het4, echo=TRUE}
stargazer(fit1, fit1, type = "text", single.row = TRUE, se = list(se.fit1, rse.fit1))
```

## Revisiting Linearity

Let's revisit addressing the assumption of linearity by constructing the residual plots that we made in the last lab. Recall that these are made by using `augment()` to predict values and calculate residuals, melt the data into long form and identify the independent variables and fitted values are your measure variables, then pipe it all into `ggplot2` and use `facet_wrap()` to create a visualization for each variable. 

```{r 11_lin, echo=TRUE}
fit1 %>%
  augment() %>%
  melt(measure.vars = c("length", "budget_1m", "votes_1k", ".fitted")) %>%
  ggplot(., aes(value, .std.resid)) +
  geom_point(shape=1) +
  geom_smooth(method = loess) +
  geom_hline(yintercept = 0) +
  facet_wrap(~variable, scales = "free")

```

There appears to be a linearity problem. The budget graphics appears to be the most linear, and the others suggest non-linear relationships. Let's examine some more information about the variables:

```{r 11_lin2, echo=TRUE}
describe(ds$length)
describe(ds$budget_1m)
describe(ds$votes_1k)
```

There is skew for all three variables, so let's respecify the model by using the log of each variable, then create the same visualization as before:

```{r 11_lin3, echo=TRUE}
fit.log <- lm(rating ~ log(length) + log(budget_1m) + log(votes_1k), data = ds)
fit.log %>%
  augment() %>% 
  melt(measure.vars = c("log.length.", "log.budget_1m.", "log.votes_1k.", ".fitted")) %>%
ggplot(., aes(value, .std.resid)) +
  geom_point(shape = 1) +
  geom_smooth(aes(value, .std.resid), method = "loess") +
  geom_hline(yintercept = 0) +
  facet_wrap(~variable, scales = "free")
```

This method fixed some problems and created new ones. The votes graphic suggests a more linear relationship, but problems persist. Perhaps a polynomial model is more appropriate. Let's square every IV in the next model:

```{r 11_lin4, echo=TRUE}
fit.poly <- lm(rating ~ poly(length, 2) + poly(budget_1m, 2) + poly(votes_1k, 2), data = ds)
```

Compare the last three models:

```{r 11_lin5, echo=TRUE}
stargazer(fit1, fit.log, fit.poly, single.row = TRUE, type = "text")
```

The log model has the highest adjusted R squared and lowest residual standard error. 

### Normality 
Let's look at the normality of the residuals for the models:

```{r 11_norm, echo=TRUE}
ds %>%
  mutate(fit1.r = residuals(fit1)) ->ds

ggplot(ds,aes(fit1.r)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(ds$fit1.r), 
                                         sd = sd(ds$fit1.r)), color = "red") +
  ggtitle("First Linear Model")
```

```{r 11_norm2, echo=TRUE}
ds %>%
  mutate(log.r = residuals(fit.log)) -> ds
ggplot(ds, aes(log.r)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(ds$log.r),
                                         sd = sd(ds$log.r)), color = "red") +
  ggtitle("Log Linear Model")

```

```{r 11_norm3, echo=TRUE}
ds %>%
  mutate(poly.r = residuals(fit.poly)) -> ds

ggplot(ds, aes(poly.r)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(ds$poly.r),
                                         sd = sd(ds$poly.r)), color="red") +
  ggtitle("Polynomial Model")

```

The log model has the highest adjusted R squared value, the lowest residual standard error, and its residuals appear to approximate the normal distribution better than the other two models. Let's use it to make predictions and create visualizations. 

First create predicted values for movie ratings by holding all IVs constant at their means except one at at time, using the `augment()` function. Then use `mutate()` to calculate the upper and lower bounds of the confidence interval. Create separate data frames for length, budget, and votes.

```{r 11_pred, echo=TRUE}
df.length <- fit.log %>%
  augment(newdata = data.frame(length = 89:134,
                               budget_1m = mean(ds$budget_1m),
                               votes_1k = mean(ds$votes_1k))) %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit)

df.budget <- fit.log %>% 
  augment(newdata = data.frame(length = mean(ds$length),
                               budget_1m = 5:80,
                               votes_1k = mean(ds$votes_1k))) %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit)

df.votes <- fit.log %>% 
  augment(newdata = data.frame(length = mean(ds$length),
                                        budget_1m = mean(ds$budget_1m),
                                        votes_1k = 1.645:25.964)) %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit)
```

Now make the visualization for each data frame~

```{r 11_pred3, echo=TRUE}
ggplot(df.length, aes(length, .fitted)) +
  geom_line(size = 1, color = "royalblue") +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .2) +
  coord_cartesian(ylim = c(4:8), xlim = c(89:134)) +
  ggtitle("Movie Length and Rating") +
  xlab("Length") +
  ylab("Rating") +
  theme_bw()
```

Now do the same for thse next two IVs:


```{r 11_pred5, echo=TRUE}
ggplot(df.votes, aes(votes_1k, .fitted)) +
  geom_line(size = 1, color = "royalblue") +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .2) +
  ggtitle("IMDB Votes and Rating") +
  xlab("Votes (Thousands)") +
  ylab("Rating") +
  theme_bw()
  
```

```{r 11_pred6, echo=TRUE}
ggplot(df.budget, aes(budget_1m, .fitted)) +
  geom_line(size = 1, color = "royalblue") +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .2) +
  ggtitle("Movie Budget and Rating") +
  xlab("Budget (Millions)") +
  ylab("Rating") +
  theme_bw()
  
```

<!--chapter:end:11-diagnosis.Rmd-->

# Logistic Regression

This lab covers the basics of logistic regression, part of the broader generalized linear model family. GLMs relate a linear model to a response variable that does not have a normal distribution. Often you will use "logit" regression when working with a dependent variable that has limited responses, like a binary DV or an ordered DV. Logit regression uses Maximum Likelihood Estimation, which aims to identify the probability of obtaining the observed data as a function of the model parameters. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
5. stargazer
6. reshape2
7. MASS
8. pscl
9. broom
10. DAMisc

```{r 12_setup, include=FALSE,message=FALSE}
ds<-read_csv("https://github.com/ripberjt/qrmlabs/raw/master/Class%20Data%20Set%20Factored.csv")
```

## Logistic Regression with a Binary DV

Recall from lab ten that we attempted to use OLS regression to explore the relationship between a number of independent variables and a vote for Trump. While using OLS provided useful information, some would consider logistic regression more appropriate in that instance. This is because of the binary DV (voted for Trump or did not) that does not follow the normal distribution. Let's construct a logit regression model that explores how certain IVs predict a vote for Trump. First we need to recode and factor the candidate variable to make it binary and exclude candidates other than Trump and Clinton, where a vote for Trump is 1 and Clinton is 0. We start by factoring the variable:

```{r 12_log, echo=TRUE}
ds$trump <- car::recode(ds$vote_cand, "0 = 1;1 = 0;else = NA;NA = NA")
ds$f.trump <- factor(ds$trump, levels = c(0, 1), labels = c("Clinton", "Trump"))
table(ds$f.trump)
ds$f.party.2 <- factor(ds$f.party.2)
```

Next, select a subset of the data and remove missing observations:

```{r 12_log2, echo=TRUE}
ds.sub <- ds %>% 
  dplyr::select("f.trump", "gender", "ideol", "income", "education", "race") %>%
  na.omit()
```

Build the generalized linear model:

```{r 12_log3, echo=TRUE}
logit1 <- glm(f.trump ~ ideol + gender + education + income, data = ds.sub,
              family = binomial(link = logit), x = TRUE)
summary(logit1)
```

The coefficients returned are logged odds, so there really is not much we can get from looking at them alone; however, from looking at the coefficients alone, we can tell that ideology and education both affect the probability of voting for Trump. In order to understand the sense of magnitude, we need to convert these from logged odds to odds, and then to percentages. To convert logged odds to odds, take the exponent of the coefficients using the _exp()_ function:

```{r 12_log4, echo=TRUE}
logit1 %>%
  coef() %>%
  exp()
```

Odds are difficult to interpret intuitively, but to get a sense of what they're telling us, remember that odds greater than 1 indicate increased probability, and odds less than one indicate a decrease in probability. The statistically significant coefficients from the model are ideology and education. Based on the odds of each IV, we can tell that an increase in ideology improves the probability of a Trump vote, and an increase in education reduces the probability of a Trump vote. To get a more intuitive understanding, we can convert these to percentages. To do this you subtract the odds from 1. Do this for the ideology and education variables only, because those are the only siginificant ones:

Ideology:

```{r 12_log5, echo=TRUE}
1 - exp(logit1$coef[2])
```

Education:

```{r 12_log6, echo=TRUE}
1- exp(logit1$coef[4])
```

This may seem counter-intuitive, but since we subtracted 1 from the odds, a negative percentage is actually an increase in probability. The -2.48 for ideology can be interpreted as a 248% increase in the odds of voting for Trump. The point is that an increasing ideology score (liberal to conservative) drastically increased the probability of a vote for Trump. The .19 for education indicates that an increase in education decreases the odds of voting for Trump by about 19%. 

Notice that even at this point, we are still dealing with some level of abstraction (a 248% increase in odds is hard to understand). Perhaps the best reason to use a logit model is that it allows us to generate predicted probabilities of some outcome. Similar to how we used OLS and the _predict()_ function to describe and predict a relationship, logit regression allows us to obtain a predicted probability that a particular outcome occurs, given a certain set of parameters. In our case, we can generate predicted probabilities of voting for Trump. Let's find the predicted probabilities of voting for Trump as ideology increases and all other IVs are held constant at their means. We first need to generate some simulated data that sequences ideology from 1 to 7 and holds all other values at their means:

```{r 12_log7, echo=TRUE}
ideol.data <- with(ds, data.frame(education = mean(education, na.rm = T),
                                   gender = mean(gender, na.rm = T),
                                   income = mean(income, na.rm = T),
                                   ideol = 1:7))
ideol.data
```

Now use the `augment()` function to calculate predicted probabilities of voting for Trump at the various ideology levels. To do so, include `type.predict = "response"`. This tells `augment()` to generate predicted probabilities:

```{r 12_log8, echo=TRUE}
logit1 %>%
  augment(newdata = ideol.data, predict = "response") 
```

As we would likely expect, increasing ideology increases the probability of voting for Trump. At an ideology level of 7, there is almost a guarantee of voting for Trump. To get a sense of what this would look like, we can visualize these predicted probabilities rather easily. We need to calculate lower and upper bounds of the confidence interval first, which is done just like with other models. Assign the data frame to an object.

```{r 12_log10, echo=TRUE}
logit1 %>%
  augment(newdata = ideol.data, type.predict = "response") %>%
  mutate(upper = .fitted + 1.96 * .se.fit,
         lower = .fitted - 1.96 * .se.fit) -> log.data
```


Visualizing the predicted probabilities is similar to how we have visualized in the past. Use `geom_point()` and `geom_errorbar()`:

```{r 12_log14, echo=TRUE}
ggplot(log.data, aes(ideol, .fitted)) +
  geom_point(size = 1.5) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = .2)
```

### Goodness of Fit, Logit Regression

Determining model fit when performing logit regression is different than when doing OLS. There are three main methods of exploring model fit, Pseudo-R squared, Log-likelihood, and AIC. The best way to understand logit model fit is by comparison, so let's create a null model that tries to predict a Trump vote only by the intercept term:

```{r 12_log15, echo=TRUE}
logit.null <- glm(f.trump ~ 1, data = ds.sub, family = binomial(link = logit))
summary(logit.null)
```

To test model fit via log-likelihood, we can calculate what is called the deviance statistic, or G squared. G squared tests whether the difference in the log-likelihoods of the null model and the demographic model (our initial model) are statistically distinguishable from zero. If so, our demographic model is a better fit. First let's the log-likelihoods for each model:

```{r 12_log16, echo=TRUE}
loglikli.null <- logLik(logit.null)
loglikli.demo <- logLik(logit1)
```

To fing G squared, you subtract the null log-likelihood from the demographic log-likelihood:

```{r 12_log17, echo=TRUE}
G <- loglikli.demo - loglikli.null
```

To test if the G statistic is significant, you use a Chi-Square test with q degrees of freedom, where q is the difference in the number of IVs in the model. Our demographic model has 4 IVs (ideology, age, education, income) and the null model has 1, so q is 3:

```{r 12_log18, echo=TRUE}
pchisq(G, df = 3, lower.tail = FALSE)
```

We can conclude with confidence that the demographic model better explains a vote for Trump than the null model. 

A similar approach can be made to compare nested models, similar to a nested F-test. Using the `anova()` function and specifying chi-squared, we can test if adding or subtracting a particular variable improves model fit. Let's include race in a new model and compare it to our first model:

```{r 12_log19, echo=TRUE}
logit2 <- glm(f.trump ~ ideol + gender + education + income + race, data = ds.sub,
              family = binomial(link = logit), x = TRUE)
summary(logit2)
```

Now compare models:

```{r 12_log20, echo=TRUE}
anova(logit1, logit2, test = "Chisq")
```

The test indicates that including race improves the model.

Another way to examine model fit is pseudo-R squared. This is not completely analogous to R squared, because we're not trying to simply explain the variance in Y. However, pseudo-R squared compares the residual deviance of the null model to that of the actual model and ranges of 0 to 1, with higher values indicating better model fit. Deviance in a logit model is similar to the residual sum of squares in an OLS model. To find pseudo-R squared you take 1 minus the deviance of the actual model divided by the deviance of the null model. Let's use the new model that includes race:

```{r 12_log21, echo=TRUE}
psuedoR2 <- 1 - (logit2$deviance/logit2$null.deviance)
psuedoR2
```

The final method we go over is AIC, or Akaine Information Criteria. AIC is only useful in comparing two models, and like adjusted R squared it penalizes for increased model parameters. Fortunately, AIC is calculated for us when we look at the summary of the model. A smaller AIC value indicates better model fit. Let's again compare the two actual logit models (not the null model)

```{r 12_log22, echo=TRUE}
stargazer(logit1, logit2, type="text", single.row = TRUE)
```

The AIC values indicate the the model including race is a better fit, which our log-likelihood test also indicated. 

### Percent Correctly Predicted

Another way to assess how effective our model is at describing and predicting our data is by looking at the percent correctly predicted. Using the `hitmiss()` function found in the `pscl` package, we can look at how well the model predicts the outcomes for when y and 0 and when y equals 1, and we can immediately compare it to how well a null model predicts outcomes. Let's do this for both the logit model that does not include race and the one that does:

```{r 12_pcp, echo=TRUE}
hitmiss(logit1)
hitmiss(logit2)
```

It appears the model with race better predicts outcomes, which our other diagnostics so far have also suggested. One other method is to examine proportional reduction in error, which looks at how a model reduces error in predictions versus a null model. Let's look at the PRE for the logit model that includes race. To do so, use the `pre()` function from the `DAMisc` package:

```{r 12_pre, echo=TRUE}
pre(logit2)
```
The ? function can help you remember what all of the acronyms mean, but for now, know that PMC is the percent correctly predicted by the null model, PCP is percent correct predicted by the actual model, and PRE is proportional reduction in error. As all of our diagnostics have indicated, the actual model is better at predicting a vote for Trump than the null model. 

### Logit Regression with Groups

Now let's go over logit regression with groups. Let's continue looking into the probability of a vote for Trump, but let's include political party into the mix. We can use logit regression to find the probability of voting for Trump as ideology varies across political parties. Let's pull a new subset of the data that removes missing observations and includes the factored party variable:

```{r 12_log23, echo=TRUE}
ds.sub2 <- ds %>% dplyr::select("f.trump", "gender", "ideol", "income",
                                        "education", "race", "f.party.2") %>%
  drop_na() #%>%
  #mutate(f.part = factor(f.party.2))
```

Notice that we used the factored party variable that only includes Democrats, Independents, and Republicans. Let's build the model:

```{r 12_log24, echo=TRUE}
logit3 <- glm(f.trump ~ ideol + gender + education + income + race + f.party.2,
              family = binomial(link = logit), data = ds.sub2, x = TRUE)
summary(logit3)
```

With Democrats as the reference group, Independents and Republicans have an increased probability of voting for Trump, which makes sense for Oklahoma voters. Next we generate predicted probabilities. First create data frames for each party:

```{r 12_log25, echo=TRUE}
rep.data <- with(ds.sub2, data.frame(gender = mean(gender), 
                                education = mean(education), race = mean(race),
                                income = mean(income), ideol = (1:7),
                                f.party.2 = c("Rep")))

dem.data <- with(ds.sub2, data.frame(gender = mean(gender), 
                                     education = mean(education), race = mean(race),
                                     income = mean(income), ideol = (1:7),
                                     f.party.2 = c("Dem")))

ind.data <- with(ds.sub2, data.frame(gender = mean(gender), 
                                     education = mean(education), race = mean(race), 
                                     income = mean(income), ideol = (1:7),
                                     f.party.2 = c("Ind")))
```

Now we can calculate predicted probabilities of voting for Trump for each party and ideology score, holding all other IVs constant, as well as upper and lower bounds of the confidence intervals:

```{r 12_log26, echo=TRUE}
rep.prob <- augment(logit3, newdata = rep.data, type.predict = "response") %>%
                    mutate(upper = .fitted + 1.96 * .se.fit,
                           lower = .fitted - 1.96 * .se.fit)
dem.prob <-  augment(logit3, newdata = dem.data, type.predict = "response") %>%
                     mutate(upper = .fitted + 1.96 * .se.fit,
                           lower = .fitted - 1.96 * .se.fit)
ind.prob <-  augment(logit3, newdata = ind.data, type.predict = "response") %>%
                     mutate(upper = .fitted + 1.96 * .se.fit,
                           lower = .fitted - 1.96 * .se.fit)
```


Now we combine everything into one data frame using `rbind()`.

```{r 12_log32, echo=TRUE, warning=FALSE}

df.party <- rbind(dem.prob, ind.prob, rep.prob)
```

Start by building the visualization. This will be similar to the last visualization, but we plot predicted probabilities for each ideology score in each party, so 21 points in all. We have everything we need to make a great visualization. We plot the points and error bars just like we did last time, but we assign colors by political party, and specify blue for Democrats, purple for Independents, and red for Republicans:

```{r 12_log33, echo=TRUE}
ggplot(df.party, aes(ideol, .fitted, color = f.party.2)) +
  geom_point(size = 1.5) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = .2) +
  scale_color_manual(values = c("blue", "purple", "red")) +
  ggtitle("Probability of Voting for Trump by Party") +
  scale_x_continuous(breaks=c(1:7),
                     labels = c("Very Liberal", "2", "3", "4", "5",
                                "6", "Very Conservative")) +
  xlab("Ideology") +
  ylab("Probability of Trump Vote") +
  theme_bw()
```

## Ordered Logit and Creating an Index

Logit regression can be used in more than just situations with a binary DV. Ordered logit analysis is done in a similar way, but with an ordered DV. Instead of simply assessing the probability of one outcome, ordered logit analysis gives us the probability of moving from one level of an ordered categorical variable to the next. We're going to use ordered logit analysis to also learn how to create an index and work with one as your dependent variable. 

The class data set survey includes responses that indicate whether or not the participant does a number of energy-saving activities at their home, like turning the lights off, installing insulation, unplugging appliances, etc. Perhaps you are interested in how a variety of IVs influence one's propensity to do these activities. You could use binary logit regression to find the probability of individuals doing one of these particular activities. However, you could use ordered logit regression to include all the activities and use an additive index of them as your dependent variable. Start by creating an index of the energy-saving activities:

```{r 12_ind, echo=TRUE}
energy <- with(ds, cbind(enrgy_steps_lghts, enrgy_steps_heat, enrgy_steps_ac,
                         enrgy_steps_savappl, enrgy_steps_unplug, enrgy_steps_insul,
                         enrgy_steps_savdoor, enrgy_steps_bulbs))
```

Now take a look at the index:

```{r 12_ind2, echo=TRUE}
psych::describe(energy)
```

Add these variables together. This will create an index that scores 1 if an individual does one of the activities, 2 if they do two, and so on and so on:

```{r 12_ind3, echo=TRUE}
ds$s.energy <- with(ds, enrgy_steps_lghts + enrgy_steps_heat + enrgy_steps_ac +
                      enrgy_steps_savappl + enrgy_steps_unplug + enrgy_steps_insul +
                      enrgy_steps_savdoor + enrgy_steps_bulbs)
```

Examine the index:

```{r 12_ind4, echo=TRUE}
psych::describe(ds$s.energy)
```

Make a bar plot of the index:

```{r 12_ind5, echo=TRUE}
ggplot(ds, aes(s.energy)) +
  geom_bar()
```

Start building the model. First select our relevant variables and remove missing observations:

```{r 12_ord, echo=TRUE}
ds.sub3 <- ds %>% dplyr::select("s.energy", "ideol", "age", "glbcc_risk") %>%
  na.omit()
```

In order to use the energy index as a dependent variable, we treat it as a factor:

```{r 12_ord2, echo=TRUE}
ds.sub3$f.energy <- as.factor(ds.sub3$s.energy)
```

There are a number of ways to do ordered logit, but for this example we use the `polr()` function found in the `MASS` package.:

```{r 12_ord3, echo=TRUE}
ord1 <- polr(f.energy ~ ideol + age + glbcc_risk, data = ds.sub3, Hess = TRUE)
```

Use `stargazer()` to look at the results:

```{r 12_ord4, echo=TRUE}
stargazer(ord1, type="text", style="apsr", single.row = T)
```

The results indicate an increased risk associated with climate change corresponds with an an increase in the odds of doing enery-saving techniques at home. When doing ordered logit, coefficient interpretation is even less intuitive than it is with a binary DV. This makes generating predicted probabilities even more important. We are going to generate predicted probabilities of each level of the DV (0 through 8) as perceived climate change risk increases and the other IVs are held constant at their means. It should make sense that we have to do it this way. We can't really explain the relationship in any other way with the information we have. 

First we create a data frame to work with:

```{r 12_ord5, echo=TRUE}
ord.df <- data.frame(ideol = mean(ds.sub3$ideol),
                     age = mean(ds.sub3$age),
                     glbcc_risk = seq(0, 10, 1))
```

Now we use the `predict()` function to generate predicted probabilities of each level of the DV as we sequence climate change risk from 0 to 10. The `augment()` function does not work with the `polr()` function, so that is why we are using `predict()`. 

```{r 12_ord6, echo=TRUE}
prob.df <- cbind(ord.df, predict(ord1, ord.df, type = "probs"))
prob.df
```

The next step is to melt the data. This will allow us to eventually generate a prediction line for each level of the DV:

```{r 12_ord7, echo=TRUE}
m.df <- melt(prob.df, id.vars = c("ideol", "age", "glbcc_risk"),
             variable.name = "Level", value.name = "Probability")
```

Next we will create the visualization. With an ordered logit model, we can visualize predicted probabilities of observing each separate level of the DV (how many energy saving activities), as perceived climate change risk increases. We use `facet_wrap()` to create individaul visualizations for each level of the DV, so that the graphic does not get too hard to interpret. We also create a color scale that goes from red to green. 

```{r 12_ord8, echo=TRUE}
col_scale<-colorRampPalette(c("#FF0000","#228B22"))(9)
ggplot(m.df, aes(x = glbcc_risk, y = Probability, colour = Level)) +
  geom_line(size = 1) +
  scale_color_manual(values = col_scale) +
  facet_wrap( ~ Level, scales = "free")
```

Taking a quick look at these visualizations, we can see that for doing 0 to 2 energy saving activities at home, increasing climate change risk largely corresponds with decreasing probability. This makes sense. Once we reach 4 energy saving activities, increasing climate change risk largely corresponds with increased probabilities.

<!--chapter:end:12-logistic.Rmd-->

# Statistical Simulations

So far we have covered many different techniques of statistical analysis. This lab will cover the basics of statistical simulations. As a foundation for this, we will be using the various functions associated with the Zelig Project. Zelig provides a method of simulating outcomes based on certain parameters, but it also provides a different way of constructing almost any model in R. The following packages are required for this lab: 

1. tidyverse
2. psych
3. car
4. stargazer
5. reshape2
6. MASS
7. zeligverse

## The Basics

The basics of Zelig can be broken down into four steps:

1. `zelig()` function to estimate parameters.
2. `setx()` to set values.
3. `sim()` to simulate the quantities of interest.
4. `plot()` to visualize the simulation results. 

As the lab progresses, we will amend the steps, but these are almost always a good place to start.

First create a subset of data and remove missing observations.

```{r 13_zel, echo=TRUE}
ds.sub <- ds %>%
  dplyr::select("footage", "income", "education", "age") %>%
  na.omit()
```

Use OLS regression to look at the relationship between square footage of a home (DV) and the IVs income, age, and education. First we use `zelig()` to specify the model, indicate `model="ls"`. For the income variable use logged income. As we've shown many times throughout the lab, the income variable has a skew. 

```{r 13_zel2, echo=TRUE}
ds.sub$log.inc <- log(ds.sub$income)
ols1 <- zelig(footage ~ log.inc + age + education, data = ds.sub, model = "ls", cite = FALSE)
```

Just like any other model, we look at the results using the usual methods. Let's start with `summary()`:

```{r 13_zel3, echo=TRUE}
summary(ols1)
```

As we could have guessed, increased income corresponds with increased square footage, as does age and education. Let's further explore education and square footage while holding logged income and age at their means. First take a look at the education variable:

```{r 13_zel4, echo=TRUE}
table(ds.sub$education)
```
According to the code book, a 2 indicates a High School education and 6 indicates a Bachelor's degree. We can set the x value of education to both 2 and 6, and then have Zelig run Monte Carlo simulations, creating quantities of interest that we can compare. Use `setx()` and `setx1()` to set the two x values:

```{r 13_zel5, echo=TRUE}
ols1.ed <- setx(ols1, education = 2)
ols1.ed <- setx1(ols1.ed, education = 6)
ols1.ed
```

The next step is to use the `Zelig::sim()` function. This will use Monte Carlo simulations to generate quantiles of interest at each of the specified levels. In the past we might have predicted a Y value based on a model, but this will allow us to see mean, standard deviation, and more, based on 1000 simulations at each level of x. 

```{r 13_zel6, echo=TRUE}
ols.sim <- Zelig::sim(ols1.ed)
summary(ols.sim)
```

Next we use `plot()` to visualize the various QIs. I recommend clicking the "Show in New Window" button in the top right corner of the space below. __Note:__ This is not visible via the knit PDF.

```{r 13_zel7, echo=TRUE, error=TRUE}
plot(ols.sim)
```

We can use Zelig to simulate based on a range of values, similar to how we would sequence one independent variable when we would predict values in past labs. Let's simulate the data for the whole range of education values, from 1 to 8. We use the pipe operator, %>%, to simplify the syntax. 

```{r 13_zel8, echo=TRUE}
ols2.ed <- setx(ols1, education=1:8) %>%
  Zelig::sim()
plot(ols2.ed)
```

### Plotting Predictions with Zelig

Zelig is useful in another way: plotting predictions. By adding only two more steps to the process, R can return a data frame of information in the tidyverse format that we plot with `ggplot2`. Let's plot predicted values of Y, square footage of home, by each education level. We've got the simulated values, so use `zelig_qi_to_df()` to transform the data into a data frame, and use `qi_slimmer()` to slim the values and generate confidence intervals for each point:

```{r 13_zel9, echo=TRUE}
ols.ed.df <- zelig_qi_to_df(ols2.ed) %>%
  qi_slimmer()
```

Take a look at the new data:

```{r 13_zel10, echo=TRUE}
ols.ed.df
```

Logged income and age are held at their means, education is sequenced from 1 to 8, and there are three other groups of values. `Qi_ci_min` is the lower limit, `qi_ci_median` is the estimate, and `qi_ci_max` is the upper limit. Let's plot them. Make sure to use `factor(education)` to treat each level separately. 

```{r 13_zel11, echo=TRUE}
ggplot(ols.ed.df, aes(factor(education), qi_ci_median)) +
  geom_errorbar(aes(ymin = qi_ci_min, ymax = qi_ci_max)) +
  geom_point()
```

## Other Models

Zelig can be utilized for many different types of models. Let's run through an example of logistic regression. For instance, we can run through the example we used in the last lab, predicting a vote for Trump.

```{r 13_zel12, echo=TRUE}
ds$trump <- car::recode(ds$vote_cand, "0 = 1;1 = 0;else = NA;NA = NA")
```

Subset the data and remove missing observations:

```{r 13_zel13, echo=TRUE}
ds.sub <- ds %>% dplyr::select("footage", "trump", "gender",
                                "ideol", "income", "education", "race", "age") %>%
  na.omit() %>%
  mutate(log.inc = log(income))
```

Build a model that includes gender, ideology, logged income, education, and race. To indicate a logit model, include `model="logit"`:

```{r 13_zel14, echo=TRUE}
z.log <- zelig(trump ~ gender + ideol + log.inc + education + race, data = ds.sub, model="logit", cite=FALSE)
summary(z.log)
```

Since we used ideology in the last lab, let's find the predicted probabilities of voting for Trump based on education levels:

```{r 13_zel15, echo=TRUE}
log.out <- setx(z.log, education = 1:8) %>%
  Zelig::sim() %>%
  zelig_qi_to_df() %>%
  qi_slimmer()
```
Now take a look at the data frame:

```{r 13_zel16, echo=TRUE}
log.out
```

Next make the visualization. 

```{r 13_zel17, echo=TRUE}
ggplot(log.out, aes(factor(education), qi_ci_median))+
  geom_errorbar(aes(ymin = qi_ci_min, ymax = qi_ci_max), width = .2) +
  geom_point(size = 2) +
  ylab("Predicted Probability of Trump Vote") +
  xlab("Education Level")
```

### Ordered Logit 

Recall that in the last lab, we created an index of energy-saving activities and used ordered logit to assess the probability of individuals doing the activities based on their perceived climate change risk. Let's revisit that model and go one step farther than we did last time. Instead of looking at predicted probabilities, we will use Zelig to simulate predicted values, actually predicting the number of energy-saving activities individuals do based on their perceived climate change risk. 

First create the index again:

```{r 13_ord, echo=TRUE}
energy <- with(ds, cbind(enrgy_steps_lghts, enrgy_steps_heat, enrgy_steps_ac, 
                         enrgy_steps_savappl, enrgy_steps_unplug, enrgy_steps_insul,
                         enrgy_steps_savdoor, enrgy_steps_bulbs))

ds$s.energy <- with(ds, enrgy_steps_lghts + enrgy_steps_heat + enrgy_steps_ac +
                        enrgy_steps_savappl + enrgy_steps_unplug + enrgy_steps_insul +
                        enrgy_steps_savdoor + enrgy_steps_bulbs)
```

Subset the data and remove missing observations:

```{r 13_ord2, echo=TRUE}
ds.sub3 <- ds %>%
  dplyr::select("s.energy", "ideol", "age", "glbcc_risk") %>%
  na.omit()
```

Create a factored version of the index:

```{r 13_ord3, echo=TRUE}
ds.sub3$f.energy <- factor(ds.sub3$s.energy)
```

Build the model using `model="ologit"`:

```{r 13_ord4, echo=TRUE}
logit1 <- zelig(f.energy ~ ideol + age + glbcc_risk, data=ds.sub3,
               model = "ologit", cite=FALSE)
```

Let's review the results:

```{r 13_ord5, echo=TRUE}
summary(logit1)
```

We're primarily interested in the relationship between climate change risk and energy-saving activities. Normally the next step would be to sequence climate change risk from one to ten and generate predicted probabilities, but we already did that in the last lab. This time, let's use Zelig to generate predicted values. These next steps might get a little messy, so here they are:

1. Use `setx()` and `Zelig::sim()` to simulate values for each level of climate change risk. Then use `get_qi()` to extract the predicted values. We have to do this for each level separately.
2. Put all the predicted values into a data frame:
3. Use `melt()` to melt the data frame into long form:
4. Use ggplot2 and `facet_wrap()` to create bar plots of the predicted values.

We can do steps one and two together in one line of code by piping:

```{r 13_ord6, echo=TRUE}
pv.0 <- setx(logit1, glbcc_risk = 0) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.1 <- setx(logit1, glbcc_risk = 1) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.2 <- setx(logit1, glbcc_risk = 2) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.3 <- setx(logit1, glbcc_risk = 3) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.4 <- setx(logit1, glbcc_risk = 4) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.5 <- setx(logit1, glbcc_risk = 5) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.6 <- setx(logit1, glbcc_risk = 6) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.7 <- setx(logit1, glbcc_risk = 7) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.8 <- setx(logit1, glbcc_risk = 8) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.9 <- setx(logit1, glbcc_risk = 9) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")

pv.10 <- setx(logit1, glbcc_risk = 10) %>% Zelig::sim() %>% get_qi(qi = "pv", xvalue = "x")
```

Put the predicted values into a data frame:

```{r 13_ord7, echo=TRUE}
pv.df <- data.frame(pv.0, pv.1, pv.2, pv.3, pv.4, pv.5,
                    pv.6, pv.7, pv.8, pv.9, pv.10)
```

Melt the data:

```{r 13_ord8, echo=TRUE}
pv.m <- melt(pv.df, measure.vars = c("pv.0", "pv.1", "pv.2","pv.3","pv.4","pv.5",
                                     "pv.6","pv.7","pv.8","pv.9","pv.10"))
```

Plot the predicted values. Remember, these are the predicted values that Zelig found by doing 1000 simulations at each level, not just one predicted value. Use `geom_bar()` to bar plots:

```{r 13_ord9, echo=TRUE}
ggplot(pv.m, aes(value)) +
  geom_bar() +
  facet_wrap( ~ variable, scales = "fixed") +
  scale_x_continuous(breaks = c(0:10))
```

We can deduce from this visualization that the skew shifts more negative as climate change risk increases, indicating that individuals more concerned about climate change are doing more energy-saving activities. 

### Another Example

Let's go back to the example model that regressed home square footage on logged income, age, and education. Recall the model:

```{r 13_ex, echo=TRUE}
ols1 <- zelig(footage ~ log.inc + age + education, data = ds.sub,
              model = "ls", cite = FALSE)
summary(ols1)
```

So far in the labs, we would often sequence one IV while holding the rest constant at their means. But that is not the only way to go about this. Perhaps you were interested in the relationship between income and square footage for people who have a Bachelor's degree, or maybe the relationship between education and square footage for individuals with a specific income. You can hold IVs constant at values other than their means. If you do so, in it important that you make note of it and are transparent about the data you are presenting. Let's use Zelig to generate simulations and predictions for respondents who went to college by their logged income. A Bachelor's degree is indicated by a 6 on the education scale. We need to know the range of logged income as well:

```{r 13_ex2, echo=TRUE}
describe(ds.sub$log.inc)
```

```{r 13_ex3, echo=TRUE}
inc.out <- setx(ols1, education=6,
                log.inc = seq(min(ds.sub$log.inc), max(ds.sub$log.inc))) %>%
  Zelig::sim() %>%
  zelig_qi_to_df() %>%
  qi_slimmer()
inc.out
```

Plot the predictions:

```{r 13_ex4, echo=TRUE}
ggplot(inc.out, aes(factor(log.inc), qi_ci_median)) +
  geom_errorbar(aes(ymin=qi_ci_min, ymax=qi_ci_max),width=.2) +
  geom_point(size=2)
```

## Zelig with non-Zelig Models:

There are some models that can be specified outside of Zelig but that you can use the Zelig functions on. The complete list can be found on the Zelig website, but we can demonstrate with the `lm()` function. Here's a classic model from our labs:

```{r 13_ex5, echo=TRUE}
ds$log.inc <- log(ds$income)
lm1 <- lm(glbcc_risk ~ glbcc_cert + log.inc + education + gender + ideol, data=ds)
summary(lm1)
```

We pass this model along to `setx()` and go from there. Let's sequence ideology from 1 to 7:

```{r 13_ex6, echo=TRUE}
lm1.out <- setx(lm1, ideol=1:7) %>%
  Zelig::sim() %>%
  zelig_qi_to_df() %>%
  qi_slimmer()
```

```{r 13_ex7, echo=TRUE}
ggplot(lm1.out, aes(x=factor(ideol), y = qi_ci_median)) +
    geom_errorbar(aes(ymin = qi_ci_min, ymax = qi_ci_max), width = .2) +
    geom_point(size = 2)
```

Let's move onto a different model that might provide some interesting findings. In a previous lab we looked the relationship between ideology and support for the candidate an individual voted for. Recall that we used polynomial terms to find a better model fit and concluded that the relationship was not strictly linear. Let's take that question one step further and break it down by political party. Perhaps there are linear relationships when we look at candidate support by ideology and party. We use Zelig to run simulations and predict values of candidate support for each of the ideology levels based on political party. 

First subset the data:

```{r 13_ex8, echo=TRUE}
d <- filter(ds) %>%
  dplyr::select("ideol", "education", "vote_cand_spt", "income", "age", "gender", "f.party.2") %>%
  na.omit() %>%
  mutate(log.inc = log(income),
         f.part = as.factor(f.party.2))
```

Build the model:

```{r 13_ex9, echo=TRUE}
lm2 <- lm(vote_cand_spt ~ ideol + education + log.inc + age + gender + f.part, data=d)
summary(lm2)
```

Within the `setx()` function we can sequence ideology from 1 to 7 and sequence the three party options, Democrats, Independents, and Republicans. Then use `Zelig::sim()`. Doing so will perform 1000 simulations for each level of ideology and each party, so 1000 for Democrats with an ideology score of 1, 1000 for Democrats with an ideology score of 2, and so on. 

```{r 13_ex10, echo=TRUE}
lm2.out <- setx(lm2, ideol=1:7, f.part=c("Dem", "Ind", "Rep")) %>%
  Zelig::sim()
```

Now get the data into tidyverse data frame form, slim the QIs:

```{r 13_ex11, echo=TRUE}
lm2.out <- zelig_qi_to_df(lm2.out) %>%
  qi_slimmer()

```

Next plot the results of the simulations:

```{r 13_ex12, echo=TRUE}
ggplot(lm2.out, aes(factor(ideol), qi_ci_median, color = f.part)) +
  geom_errorbar(aes(ymin = qi_ci_min, ymax = qi_ci_max), width = .2) +
  geom_point(size = 2) +
  scale_color_manual(values = c("blue", "purple", "red"),
                     name = c("Party")) +
  facet_wrap( ~ f.part, scales = "fixed") +
  ggtitle("Candidate Support by Party and Ideology") +
  xlab("Ideology") +
  ylab("Candidate Support") +
  theme_bw()
```

This visualization tells us some interesting information: Independents do not appear to be have as much support for the candidate they voted for, regardless of their ideology. It also appears that Democrats and Republicans follow the same trend, with more conservative Democrats tending to support the candidate they voted for a little more, and the same for Republicans. 

For more on Zelig, make sure to check out the website: zeligproject.org

<!--chapter:end:13-simulations.Rmd-->

# Appendix: Guide to Data Visualization

Throughout these labs, we have created numerous high quality data visualizations using our class data sets. While you can always use the code we have created there - it may even help refresh your memory on the data analysis skills you've learned - this guide is designed to be a quick, additional resource for the creation and customization of data visualizations using the Tidyverse and `ggplot2`. 

These examples will use the class dataset to demonstrate how to add and customize elements of your data visualizations. 

__Note:__ While this quick guide is hopefully helpful, it is by no means complete. Remember that R and the Tidyverse packages are open source; you can always find help online, such as this [Data Visualization Cheat Sheet](https://resources.rstudio.com/rstudio-cheatsheets/data-visualization-2-1-cheat-sheet) from RStudio.

## Deciding Which Visualization to Use

Remember - the type of data you are using _matters_ for what kind of plot you make. 

### For Exploring a Single Variable {-}
When you are first exploring your data, it is often helpful to visualize a single variable.

For __continuous__ data, you may be interested in producing 

  - density curves (geom_density), or 
  - histograms (geom_histogram).

For __discrete__ data, you will likely be using a bar plot (geom_bar or geom_col).

### For Displaying Two (or more) Variables {-}
After you get a feel for your data, you are likely then interested in visualizing their relationships. These examples are geared towards two variables (one x and one y), but you can always add more!

With a __continuous__ x and a __continuous__ y, you have numerous options available. The most common include:

  - a scatter plot (geom_point + geom_jitter), and 
  - a function curve(geom_line or geom_smooth)
  
With a __discrete__ x variable and a __continuous__ y variable, you might produce:

  - a bar plot (geom_bar or geom_col),
  - a box blot (geom_boxplot), or
  - a violin plot (geom_violin)
  
__Note:__ While these are not your only options, they are some of the most commonly used (and match what we've covered in these labs!)

Once you have the basic plot made, the customization can begin. Below, we'll cover how to add labels and titles, scale your axes, visualize error, add color, and using a theme.

## Adding Labels
There are numerous ways to add titles and labels to your plot. The simplest is to use the functions:

  $+$ xlab("X-Axis Title"),
  $+$ ylab("Y-Axis Title"), and
  $+$ ggtitle("Plot Title")

## Scale and Limits
You may also want to change the range of values that appear on either axis, as well as the number and nature of data breaks. 

To set limits on the range of data that appears in your plot, you can use the function:

  $+$ coord_cartesian(ylim =, xlim =)

In essence, this allows you to "zoom in" on your data by specify the range of the x and y axes.

You may also want to change the number and nature of the breaks along your axes. For continuous data, you can use the functions:
  $+$ scale_x_continuous(breaks =, labels = c()), and
  $+$ scale_y_continuous(breaks =, labels = c())

__Note:__ For discrete data, you will use these functions in the same way:
  $+$ scale_x_discrete()
  $+$ scale_y_discrete()

## Visualizing Error

Often, you will need to include a representation of estimated error in your data visualizations. The primary ways to do this are through confidence intervals or error bars. 

### Confidence Intervals {-}

To add confidence intervals to your function lines, you can use the geom_ribbon() function.
The arguments for this function are:

  $+$ geom_line() $+$ geom_ribbon(aes(ymin =, ymax =), alpha = )

__Note:__ Specifying the alpha value here changes how opaque the confidence interval is.

### Error Bars {-}

With a discrete point, you will need error bars rather than confidence intervals. To include error bars on your plot, you can use the geom_errorbar() function. The necessary arguments include:

  $+$ geom_errorbar(aes(ymin, ymax), width =)

__Note:__ Prior to using either of these functions, it helps to add the upper and lower limits of your confidence interval to your data frame. This can be done via the mutate function.

## Adding Color

Adding color to your plots helps you distinguish between variables and categories. In the `ggplot` aesthetic, setting a variable to "color" or "fill" will automatically produce predefined color variations. However, these won't always make sense with your data and you may want to specify your own.

__Note:__ `R` allows you to use HEX color codes for customization. [Color Brewer 2](http://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3) is a great tool for picking out colors.

For setting the color of an individual variable, you can add the argument within the geom you are using. Here you will specify the desired color, rather than the variable. Remember that you use "color" for points and lines, and "fill" for columns and bars. For example,

  $+$ geom_point(color = "blue"), or
  $+$ geom_col(fill = "red")

__Note:__ You can also specify the size and alpha here, too.

You may also want to add multiple colors to a plot. These functions allow yoou to specify your own mappings from levels in the data to color values.

  $+$ scale_colour_manual(values = c())
  $+$ scale_fill_manual(values = c())

## Position Adjustments
When using multiple variables, you may often want to adjust their placement on the graph. Below are some common position adjustments you can use.

- Arrange elements side by side

  $+$ geom_bar(position = "dodge")

- Stack elements on top of each other

  $+$ geom_bar(position = "stack") or,
  $+$ geom_bar(position = "fill")
  
__Note:__ "Fill" normalizes the height of the bars, while "stack" does not

- Move labels away from data points

  $+$ geom_label(position = "nudge")

## Using Themes

Outside of the basic plot elements pertaining to your data, you can still further customize your plots by using themes. 

A complete guide to the existing `ggplot2` themes is available [here](https://ggplot2.tidyverse.org/reference/ggtheme.html). These primarily control the non-data display elements, which include the background shading and internal lines of the plot. Different themes may be best suited for different kinds of projects; you can even tweak the premade themes by adjusting their arguments.

Additionally, you can create and save your own custom theme for future use! Here is an example of a custom theme:
 
     theme_custom <- theme_minimal() + 
      theme(plot.title = element_text(family="serif",
                                      face = "bold",
                                      hjust = .5,
                                      size = 15),
            axis.title = element_text(family = "serif",
                                      face = "bold",
                                      size=15),
            axis.text = element_text(family = "serif",
                                     face = "bold",
                                     color = "black"),
            panel.grid = element_line(color = "grey75"),
            legend.background = element_rect(color = "grey94",
                                             fill = "grey94"),
            legend.text = element_text(family = "serif",
                                       face = "bold",
                                       size = 9),
            legend.title = element_text(family = "serif",
                                        face = "bold",
                                        size = 9),
            legend.title.align = .5)

## Putting it All Together

Below, we have created three plots to provide an example of how these visual elements might all interact.

First, we will establish a subset of the class dataset to use for the examples.
```{r 14_subset, echo = FALSE}
ds %>%
  dplyr::select("glbcc_risk", "wtr_comm", "ideol", "gender", "party", "glbwrm_risk_fed_mgmt") %>%
  na.omit -> sub.ds

sub.ds$f.gender <- factor(sub.ds$gender, levels = c(0, 1), 
                      labels = c("Women", "Men"))
```

```{r 14_ex1, echo=TRUE}
#create the model
lm1 <- lm(glbcc_risk ~ party + f.gender, data = sub.ds)
lm1 %>%
  augment(newdata = data.frame(party = 1:2,
                               f.gender = "Men")) -> fit1.m
lm1 %>%
  augment(newdata = data.frame(party = 1:2,
                               f.gender = "Women")) -> fit1.w

fit1.df <- full_join(fit1.m, fit1.w)

#visualize the model
ggplot(fit1.df, aes(party, .fitted, color = f.gender)) +
  geom_point(stat = "identity", position = position_dodge(width = 0))  +
  scale_x_continuous(breaks = 1:2, labels = c("Democrat", "Republican")) +
  geom_errorbar(aes(ymin = .fitted - 1.96 * .se.fit, 
                    ymax = .fitted + 1.96 * .se.fit), width = 0.25) +
  scale_colour_manual(name="",  values =c("#468499", "#f6546a")) +
  ylab("Perceived Risk of Climate Change") +
  xlab("Party Affiliation") +
  ggtitle("Climate Change Risk By Gender and Party") +
  labs(color="Gender") +
  scale_fill_discrete(guide=FALSE) +
  theme_minimal()
```

```{r 14_ex2, echo=TRUE}
#creating the model
lm2 <- lm(glbcc_risk ~ wtr_comm + f.gender, sub.ds)
lm2 %>%
  augment(newdata = data.frame(wtr_comm = 0:10,
                               f.gender = "Women")) -> fit2.w
lm2 %>%
  augment(newdata = data.frame(wtr_comm = 0:10,
                               f.gender = "Men")) -> fit2.m
fit2.df <- full_join(fit2.w, fit2.m)
fit2.df <- fit2.df %>%
  mutate(up = .fitted + 1.96 * .se.fit,
         low = .fitted - 1.96 * .se.fit)

#visualizing the model
fit2.df %>%
ggplot(., aes(x = wtr_comm, y = .fitted, color = f.gender)) +
  geom_line() +
  geom_ribbon(aes(ymin = low, ymax = up, fill = f.gender), alpha = .3) +
  coord_cartesian(ylim = c(2, 9), xlim = c(1, 5)) +
  scale_x_continuous(breaks=c(1, 2 ,3 ,4 ,5), labels=c("Definitely No", 
                                                       "Probably No", "Unsure",
                                                       "Probably Yes", "Definitely Yes")) +
  theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggtitle("Concern for Water Supply and Climate Change") +
  xlab("Considers Water Supply Adequate") +
  ylab("Perceived Climate Change Risk") +
  labs(color="Gender") +
  scale_fill_discrete(guide=FALSE)
```

```{r 14_ex3, echo=TRUE}
lm3 <- lm(glbwrm_risk_fed_mgmt ~ ideol * glbcc_risk, data = sub.ds)
lm3 %>%
  augment(newdata = data.frame(ideol = 1, glbcc_risk = seq(1, 10, 1))) -> id1
lm3 %>%
  augment(newdata = data.frame(ideol = 2, glbcc_risk = seq(1, 10, 1))) -> id2
lm3 %>%
  augment(newdata = data.frame(ideol = 3, glbcc_risk = seq(1, 10, 1))) -> id3
lm3 %>%
  augment(newdata = data.frame(ideol = 4, glbcc_risk = seq(1, 10, 1))) -> id4
lm3 %>%
  augment(newdata = data.frame(ideol = 5, glbcc_risk = seq(1, 10, 1))) -> id5
lm3 %>%
  augment(newdata = data.frame(ideol = 6, glbcc_risk = seq(1, 10, 1))) -> id6
lm3 %>%
  augment(newdata = data.frame(ideol = 7, glbcc_risk = seq(1, 10, 1))) -> id7
full_join(id1, id2) %>%
  full_join(., id3) %>%
  full_join(., id4) %>%
  full_join(., id5) %>%
  full_join(., id6) %>%
  full_join(., id7) -> fit3.df

#create a color scale
col_scale<-colorRampPalette(c("#0200bd50","#FF000050"))(7)

#visualizing the model
ggplot(fit3.df, aes(glbcc_risk, .fitted, color = as.factor(ideol))) +
  geom_line(size = 2) +
  scale_color_manual(values = c(col_scale[1], col_scale[2], col_scale[3],
                       col_scale[4], col_scale[5], col_scale[6], col_scale[7]),
                     labels = c("1", "2", "3", "4", "5", "6", "7"),
                     name = "Ideology") +
  ggtitle("Federal Climate Change Management by Ideology") +
  xlab("Climate Change Risk") +
  ylab("Preferred Level of Federal Involvement") +
  theme_bw()
```

There are an infinite number of ways you might use these (and more) elements to customize your own data visualizations. Have fun with it!

<!--chapter:end:14-appendix.Rmd-->

