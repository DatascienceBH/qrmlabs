---
title: 'Lab 9: Categorical Explanatory Variables, Dummy Variables, and Interactions'
author:
 - Alex Davis
 - Cody Adams
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(psych)
library(car)
library(memisc)
library(stargazer)
library(gridExtra)
library(interplot)
options(scipen = 999)
setwd("~/Methods Labs")
ds <- read.csv("Class Data Set Factored.csv")
```

This lab will focus on the ways in which we can use and understand categorical independent variables. So far the independent variables we have worked with have mainly been interval or ordinal data. When working with categorical data, there are different approaches and techniques of interpretation. The following packages are required for this lab: 

1. ggplot2
2. psych
3. memisc
4. stargazer
5. gridExtra
6. interplot

## Part I: Dummy Variables

As the textbook mentions, we often have situations in the social sciences that require analyzing variables that our categorical. Dummy variables are one way of doing so. A dummy variable is a dichotomous variable (0 or 1) in which 1 represents some present quality and 0 represents its absence. There is not any inherent organization or hierarchy, just the 1 is a particular presence and 0 is the absence. For example, A dummy variable for whether someone is a Democrat would be the value of 1 for Democrats and 0 for anyone else. There is no numeric difference, just a categorical difference. With a dummy variable, the 0 is called the "referent group". Often times dummy variables are used for race, gender, and political party. 


Up to this point we have not included categorical variables in any of our models. Now that we are going to utilize dummy variables, we can start including some in our models. The simplest one to start with is gender. In the class data set, the gender variable is coded as a 0 for women and 1 for men. This technically makes it a dummy variable for men, with women as the referent group. If we wanted to costruct a model that looked at how certainty of climate change varied by gender, ideology, education, income, and age, our equation would look like this:

$$Y_i=\alpha + \beta_{ideol} + \beta_{educ} + \beta_{inc} + \beta_{age} + \beta_{gend} + \epsilon_i$$
where B_gend is a binary indicator of gender, 0 for female and 1 for male. This means that when gender is female, gender equals 0, and therefore that coefficient is 0.

Let's pull the data, omit missing variables, and look at the gender variable we are going to use.

```{r dum, echo=TRUE}
ds.sub <- subset(ds, select = c("ideol", "education", "income", "age", "gender", "f.gender", "glbcc_cert", "f.party", "glbcc_risk", "glbwrm_risk_fed_mgmt"))
ds.sub <- na.omit(ds.sub)
table(ds.sub$f.gender)
```

Notice that the factored gender variable actually lists men as 0 and women as 1. If you look at a table of the non-factored version, it shows the opposite. This is because R tends to read factored variables in alphabetical order, which can be quite frustrating at times if you don't notice it. Luckily it's easy to change the order of the factored variable. All we need to do is factor the gender variable again:

```{r dum2, echo=TRUE}
ds.sub$f.gender <- factor(ds.sub$gender, levels = c(0,1), labels=c("Women", "Men"))
table(ds.sub$f.gender)
table(ds.sub$gender)
```

When working with a binary categorical explanatory variable (like our gender variable), you can technically use the numeric version of the variable. However, when working with categorical variables with more than two categories, it is often easier to use the factored version of the variable, for reasons we will get into shortly. So for consistencies sake, we will use the factored gender variable in our model. 

```{r dum3, echo=TRUE}
lm1 <- lm(glbcc_cert ~ ideol + education + income + age + f.gender, data=ds.sub)
summary(lm1)
```

Some of these results we should know by heart by now, like ideology and education having an effect on someone's certainty of climate change. However, now we can look at the role that gender plays. We used the factored version of gender in our model, so we need to interpret the results as such. The summary table says "f.genderMen", which means the variable is a dummy variable for men, with the refernt category being women. The coefficient is interpreted as- the difference in in dependent variable from the referent category to the dummy category. In this case, the coefficient is statistically significant. To interpret it, one would say that men are on average .406 units more convinced of climate change, on a scale of 0 to 10, all else held constant. The rest of the coefficients are interpreted as they have been in the past. But now you likely have a more accurate model, since you are "controlling" for gender. 

Visualizing this should likely make the picture more clear. If we wanted to visualize the relationship between ideology and climate change risk in our model **by gender**, we would go about it in a very similar way to previous visualizations. To separate men and women, there are a couple ways you can choose from. You can use *group=DummyVariable*, which separates the two groups but does not give you a way to really distinguish between them. You can also use *color=DummyVariable*, which will separate the groups, but also assign them colors and include a legend. 

```{r dum4, echo=TRUE}
ggplot(lm1, aes(x=ideol, y=glbcc_cert, color=f.gender)) +
  geom_smooth(aes(ideol, glbcc_cert, fill=f.gender), method = "lm", se=FALSE)
```

In this visualization we turned off the confidence intervals to emphasize that you can think of the effect of dummy variables as a change in the value of the intercept. In this case our dummy variable for men is about 0.41, and you'll notice that the line for men looks about that much above the women line. 

### Multiple Dummy Variables

Sometimes it is necessary to include multiple dummy variables in your model. This could be the case when you need to include multiple dummies that have two possible categories (like our gender variable), but it could also be the case that you need to inlcude multiple dummies to account for a variable that has multiple different options (e.g. Republican/Democrat/Independent/Other), or race, or a gender variable that includes more than two possiblities. When working with these types of dummy variables, you need to select one of the groups to be the reference group. Sometimes this is a decision driven by your theory and other times convenience. R will automatically select a reference group, but you can change it. When using dumym variables that have multiple categories, you are in essence creating a separate dummy variable for each of the groups versus the reference. For example, including political party in your model as an independent variable, with Republicans as the reference group, you would be creeating a dummy variable for Democrats (with Republicans as the 0), Independents (with Republicans as the 0), and Other (with Republicans as the zero). It is important to note that these variables are interpreted as the diffference between the reference group and each other group, but you can't interpret the other groups against each other. If you wanted to do that, you would need to change the reference category. 

Let's get into an example using political party as a dummy variable. Start by looking at a table of the factored party variable.



```{r dum5, echo=TRUE}
table(ds.sub$f.party)
```


First thing to take note of: Democrats are listed first, so they will be the reference category. The second thing to note is that, if you were to write out an equation for the regression model, you would include dummy variables for each of the other party options. In R, you can just include the factored party variable. R will read it as a factored variable and treat every party option as an independent dummy variable with Democrats as the reference category. Let's create a model based on the model we used earlier, but let's include the factored party variable as an independent variable. Due to potential multicollinearity issues, let's omit the ideology variable from the model. To make calculations simpler, we're also going to use the non-factored version of gender. Since its a binary group, this will not change any of the coefficients. 

```{r dum6, echo=TRUE}
lm2 <- lm(glbcc_cert ~ f.party  +education + income + age + gender, data=ds.sub)
summary(lm2)
```

We can see our model suggests that independents and Republicans are, on average, less certain about climate change, all other variables held constant. The coefficeint for Other is not significant, which makes sense given Other could indicate a panoply of political parties spanning the ideological spectrum. 

Let's suppose we wanted to visualize this model. How could we go about visualizing how climate change certainty varies by political party. You might have realized that we already covered this! In essence dummy variables are similar to performing t tests, but with statistical controls. We can visualize them as such. Recall that in lab 5 we went over the basics of visualizing difference in means. If you need a refresher, check it out! Here we're going to combine our knowledge of that with our knowledge of regression analysis. First we need to predict values based on party affiliation. Like the last lab, we'll use the *predict()* function and tell R to return predicted climate change certainty values based on politicla party, holding each other variable at its mean. We'll assign it to a data frame called "values". We'll also generate standard error values for each of the estimates. 

```{r dum7, echo=TRUE}
values <- predict(lm2, newdata =  data.frame((f.party=c("Dem", "Ind", "Rep", "Other")), education=mean(ds.sub$education), income = mean(ds.sub$ideol), age=mean(ds.sub$age),
                                   gender = mean(ds.sub$gender)), se.fit = T)
values
```

In the next code chunks we are going to construct a data frame that contains the following vectors:

1. A vector containing the names of the parties.
2. A vector containing the estimated values.
3. A vector containing the upper confidence interval estimates.
4. A vector containing thew lower confidence interval estimates.

Doing so will provide an intuitive way behind the visualizaiton. 

First the names:

```{r dum8, echo=TRUE}
party <- c("Democrat", "Independent", "Republican", "Other")
party
```

Now construct a vector with the point estimates. We'll call it "cert" since we are measuring climate change certainty. The order of our party name vector was determined by the order that we predicted the estimated values. 

```{r dum9, echo=TRUE}
cert <- values$fit
cert
```

Next we need to calculate the upper end of the confidence interval. Remember, you do this by adding the estimate to 1.96 (the 95% t value) times the standard error. We'll call it "upper":

```{r dum10, echo=TRUE}
upper <- cert + 1.96*values$se.fit
upper
```

Now do the same for the lower end. 

```{r dum11, echo=TRUE}
lower <- cert - 1.96*values$se.fit
lower
```

We now have all the information we need. First combine all the vectors into a data frame:

```{r dum12, echo=TRUE}
df <- data.frame(party, cert, upper, lower)
```

With the data frame constructed, the next step is to build the visualization. Our x value is party and the y values is climate change certainty. We'll use *geom_point()* and *geom_errorbar* to build the points and confidence intervals:

```{r dum13, echo=TRUE}
ggplot(df, aes(x=party, y=cert)) +
  geom_point() +
  geom_errorbar(ymin=lower, ymax=upper, width=.1) +
  ylim(4,8) +
  ggtitle("Climate Change Certainty by Political Party") +
  ylab("Climate Change Certainty") +
  xlab("Political Party")
```


Perhaps you noticed that our model suggests that gender also plays a role. Next we are going to breakdown the relationship by party and gender simultaneous, by creating different predictions for each gender within each party. First we will create a new model, this time including the factored gender variable:

```{r dum14, echo=TRUE}
lm3 <- lm(glbcc_cert ~ f.party  +education + income + age + f.gender, data=ds.sub)

```

The only difference between the models is that the factored gender variable is used, which does not change any of the results. Now we are going to follow a similar process in constructing the graphic, except we'll predict different values for men and women, and build vectors and confidence intervals separately for men and women before combining them.

First let's predict values. We'll do everything the same except this time we'll indicate a gender, not hold it at its mean:

```{r dum15, echo=TRUE}
men  <- predict(lm3, newdata =  data.frame((f.party=c("Dem", "Ind", "Other", "Rep")), education=mean(ds.sub$education), income = mean(ds.sub$ideol), age=mean(ds.sub$age),
                                                        f.gender = "Men"), se.fit = T)
women  <- predict(lm3, newdata =  data.frame((f.party=c("Dem", "Ind", "Other", "Rep")), education=mean(ds.sub$education), income = mean(ds.sub$ideol), age=mean(ds.sub$age),
                                                      f.gender = "Women"), se.fit = T)
```

Now let's create the party vector. This time we'll need R to repeat the order of the parties:

```{r dum16, echo=TRUE}
party2 <- rep(c("Dem", "Ind", "Other", "Rep"),2)
```

Now the climate change certainty values for men and women separately.

```{r dum17, echo=TRUE}
m.cert <- men$fit
w.cert <- women$fit
```

Now combine the two values into one vector:

```{r dum18, echo=TRUE}
cert2 <- c(m.cert, w.cert)
```

The next step is to create the upper and lower confidence intervals for men and women, then combine them into two vectors:

```{r dum19, echo=TRUE}
m.up <- m.cert + 1.96*men$se.fit
m.low <- m.cert - 1.96*men$se.fit
w.up <- w.cert + 1.96*women$se.fit
w.low <- w.cert - 1.96*women$se.fit
up <- c(m.up, w.up)
low <- c(m.low, w.low)
```

Next we need to create a gender vector. It needs to repeeat men four times and then women four times:

```{r dum20, echo=TRUE}
gend <- rep(c("Men", "Women"), each=4)
```

Now assign each vector to a data frame:

```{r dum21, echo=TRUE}
df2 <- data.frame(party2, gend, cert2, up, low)
df2
```

The next step is to build the visualization:

```{r dum22, echo=TRUE}
ggplot(df2, aes(x=party2, y=cert2, fill=gend)) +
  geom_bar(stat = "identity", position = position_dodge())

```


It appears that there might be a difference in climate change risk of the politicla parties by gender. If we really were to test how political beliefs vary as a function of gender and are related to opinions about climate change certainty, we would need to explore interaction terms.

## Part II: Interactions

Interactions estimate the effect of a variable x on y as a function of z. We measure this by multiplying x and z together in the model. Let's suppose we wanted to explore the effect of ideology on climate change certainty as a function of gender, meaning the relationship might be different for men and women. Let's include the other predictors (minus political party) from earlier and specify this model: 

$$Y_i=\alpha + \beta_{ideol} + \beta_{ideol*gend} + \beta_{educ} + \beta_{inc} + \beta_{age} + \beta_{gend} + \epsilon_i$$

where gender is a binary indicator of men (1) or women(0), When gender is 0, you can remove the interaction term because it would equal zero. When specifying this model in R, you would do this:

```{r int, echo=TRUE}
lm3 <- lm(glbcc_cert ~ ideol*gender + education + income + age , data=ds.sub)
summary(lm3)
```

Before interpreting these results, notice that the formula includes ideology times gender but does not have the ideology and gender variables separately. This is an R syntax point. You don't include them because R reads the interaction and automatically includes the separate variables. Now to interpret the results, notice that the ideol:gender coefficient is not significant. This suggests that there is not an interaction effect. The rest of the results should be known by now!

Let's build a new model that looks at cimate change risk, not certainty. Inlcude the same independent variables and the same interaction:


```{r int2, echo=TRUE}
lm4 <- lm(glbcc_risk ~ ideol*f.gender + education + income + age , data=ds.sub)
summary(lm4)
```

We've got a lot of results to interpret! As would be expected, ideology is very signficant and the coeffienct is large. Education and income al;so exert influence. This time our interaction is significant. To interpret these results, we would say that there is an interaction (ideology effects climate change risk as a function of gender). We also know that the slop of the lines will be negative. Often times the most intuitive way to understand interactions is to make predictions and visualize them, so let's do so. 

Visualizing an interaction effect when the interaction term is binary is rather simple. There are only two possible lines, when z=0 and when z=1, in this case when the gender is female or male. This makes the visualizing process similar to the first visualization we need, with the dummy variable:

```{r int3, echo=TRUE}
ggplot(lm4, aes(x=ideol, y=glbcc_risk, color=f.gender)) +
  geom_smooth(aes(ideol, glbcc_risk, fill=f.gender), method = "lm")

```

Notice how the slopes are different for men and women. The slope is steeper for men, suggesting that there is more of an interaction for men. Let's make some predictions:


First men:

```{r int4, echo=TRUE}
men.p <- predict(lm4, newdata = data.frame(ideol=seq(1:7), f.gender="Men", education=mean(ds.sub$education),
                                           income=mean(ds.sub$income), age=mean(ds.sub$age)), se.fit = T)
men.p

```

Now women:

```{r int5, echo=TRUE}
women.p <- predict(lm4, newdata = data.frame(ideol=seq(1:7), f.gender="Women", education=mean(ds.sub$education),
                                           income=mean(ds.sub$income), age=mean(ds.sub$age)), se.fit = T)
women.p
```

The difference between the first predicted value and the last is called the "first difference". We can tell that the first difference is larger for men. They have a higher first value and a higher last value.

### Interactions with Two Non-binary Variables

Often times our theory and hypotheses dictate that we need to include an interaction between two variables when neither of them are binary. This makes the intuitive interpretation of the interaction coefficient more difficult, but nonetheless the process is still the same. Suppose you wanted to explore people's attitudes about the role of the federal government in climate change management. We can theorize that two primary predictors of these attitudes are ideology and climate change risk. Conservatives tend to be more likely to oppose federal government intervention, and someone more concerned about climate change should likely support more government management of the situation. Perhaps, though, we should include an interaction between the two variables. Let's suppose we wrote a whole research paper full of literature and theory supporting the idea that attitudes about the role of the federal government and perceived risk of climate change will be different for liberals and conservatives. We could theorize that the relationship between federal climate change management and climate change risk will be positive for everyone, with individuals perceving greater risk from climate change supporting more government management of it, but that the relationship will be weaker for conservatives, due to their general greater opposition to federal government intervention. Therefore we will specify the following hypothesis:

The relationship between perceived climate change risk and support for federal government management of climate change will be positive, but conditional on ideology. The relationship will be more pronounced for liberals and less prounced for conservatives. 


First let's take a look at the federal climate change management variable:

```{r int7, echo=TRUE}
describe(ds.sub$glbwrm_risk_fed_mgmt)
```

We see that it is an ordinal variable ranging from 0 (not involved) to 10 (very involved).

Now we should specify the model, including appropriate controls:

```{r int8, echo=TRUE}
lm5 <- lm(glbwrm_risk_fed_mgmt ~ ideol*glbcc_risk + education + gender + income + age, data=ds.sub)
summary(lm5)
```

Right from the start we see that ideology and climate change risk both play very significant roles! These coefficients are both statistically significant (p values barely distinguishable from 0) and substantively significant. A one unit change in either of the variables corresponds with more than a half point change in opinions about federal climate change management. Education and income also play slight roles. And notice that the interaction is significant! There is not a lot of intuitive interpretation we can gather from the coefficient alone. However, we see that it is very small, .026, and will likely not alter the slopes too much. The best way to understand an interaction of two non-binary variables is to make predictions and visualize. Let's start with predictions.

First predict values of Y for extreme liberals:

```{r int9, echo=TRUE}
lib <- predict(lm5,newdata=data.frame(ideol=(1), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)

```

Now conservatives:

```{r int10, echo=TRUE}
con <- predict(lm5,newdata=data.frame(ideol=(7), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
```

Now compare them:


Liberals:
```{r int11, echo=TRUE}
lib$fit
```

Conservatives:

```{r int12, echo=TRUE}
con$fit

```

Find the first difference of each:

Liberals: 
```{r int13, echo=TRUE}
lib$fit[10] - lib$fit[1]
```

Conservatives:

```{r int14, echo=TRUE}
con$fit[10] - con$fit[1]

```

There is a greater first difference for conservatives! This combined with a significant interaction coefficient that is positive, we can start to see that perhaps the slopes of the lines are steeper for extreme conservatives. Let's build a visualization that includes a prediction line for every ideology level. To do so we need to:

1. Generate predictions for every ideology score.
2. Put those predictions in data frames.
3. Visualize each individaul line on a single plot.

Start with an ideology of 1 and then go to 7:

```{r int15, echo=TRUE}
id1 <-predict(lm5,newdata=data.frame(ideol=(1), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
id2 <- predict(lm5,newdata=data.frame(ideol=(2), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
id3 <- predict(lm5,newdata=data.frame(ideol=(3), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
id4 <- predict(lm5,newdata=data.frame(ideol=(4), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
id5 <- predict(lm5,newdata=data.frame(ideol=(5), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
id6 <- predict(lm5,newdata=data.frame(ideol=(6), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
id7 <- predict(lm5,newdata=data.frame(ideol=(7), gender=mean(ds.sub$gender), education=mean(ds.sub$education),
                                     income=mean(ds.sub$income), age=mean(ds.sub$age), glbcc_risk=seq(1,10,1)), se.fit = T)
```

Now let's put all the predicted values into their own data frames:

```{r int16, echo=TRUE}
p1 <- data.frame(glbcc_risk=1:10, id1)
p2 <- data.frame(glbcc_risk=1:10, id2)
p3 <- data.frame(glbcc_risk=1:10, id3)
p4 <- data.frame(glbcc_risk=1:10, id4)
p5 <- data.frame(glbcc_risk=1:10, id5)
p6 <- data.frame(glbcc_risk=1:10, id6)
p7 <- data.frame(glbcc_risk=1:10, id7)
```

Next we are going to create a color scale. This will help our visualization because we will be visualizing multiple lines. We'll create a scale with 7 values, one for each ideology score, that goes from blue (liberal) to red (conservative):


```{r int17, echo=TRUE}
col_scale<-colorRampPalette(c("#0200bd50","#FF000050"))(7)
```

Now to build the visualzation. We have been using geom_smooth(), but this time we will use geom_line(), because we already predicted the values we want to plot. Let's make this a complete visualization, including axis labels, and a title.




```{r int18, echo=TRUE}
ggplot() +
  geom_line(data=p1, aes(glbcc_risk, fit), size=2, color= col_scale[1]) +
  geom_line(data=p2, aes(glbcc_risk, fit), size=2, color= col_scale[2]) +
  geom_line(data=p3, aes(glbcc_risk, fit), size=2, color= col_scale[3]) +
  geom_line(data=p4, aes(glbcc_risk, fit), size=2, color= col_scale[4]) +
  geom_line(data=p5, aes(glbcc_risk, fit), size=2, color= col_scale[5]) +
  geom_line(data=p6, aes(glbcc_risk, fit), size=2, color= col_scale[6]) +
  geom_line(data=p7, aes(glbcc_risk, fit), size=2, color= col_scale[7]) +
  ggtitle("Climate Change Risk and Federal Management by Ideology") +
  xlab("Climate Change Risk") +
  ylab("Level of Federal Management") +
  theme_bw()

```

Consider everything we've found so far. The positive interaction coefficient, the larger first difference for conservatives, and now this visualization. It is quite clear that the relationship between climate change risk and preferred level of federal climate change management is appears to be stronger for conservatives. The slopes of the lines become steeper as the idelogy score increases. This was not what we hypothesized, and therefore we cannot reject the null hypothesis. However, this is an interesting finding!


## Part III: Releveling Variables

It was mentioned earlier in the lab that R can sometimes re-order a varaible. This is espeically the case the for factored variables, when R automatically reads them alphabetically. Sometimes you need to relevel a variable by necessity, sometimes by preference. Let's look at the factored party variable:

```{r rel, echo=TRUE}
table(ds$f.party)
```

Notice the variable is in alphabetical order. If we included this variable in our model, R would read the Democrat category as the reference group. Perhaps you wanted Republicans to be the reference group. There are a couple ways you could do this. The first way would be to refactor the numeric version of the variable. First look at the unfactored version:

```{r rel2, echo=TRUE}
table(ds$party)
```

We can compare this to the factored version and extrapolate that the 2 value indicates Republicans. So when factoring the variable we will list 2 first:

```{r rel3, echo=TRUE}
ds$f.party2 <- factor(ds$party, levels = c(2,1,3,4), labels = c("Rep", "Dem", "Ind", "Other"))
table(ds$f.party2)
```


This is one way to go about it. This way has its merits, such as wanting to re-order every level of the variable, not just the reference group. But there is another way, one that changes the reference group onle. Using the *relevel()* function, and indicating the reference group with the *ref=* argument. Let's do so, and make Republicans the reference group:

```{r rel4, echo=TRUE}
ds$f.party3 <- relevel(ds$f.party, ref = "Rep")
table(ds$f.party3)
```


## Part IV: Interaction Plots

Earlier in our exploration of interactions we plotted estimated Y values as X varies as a function of Z. Specifically, we looked at the relationship between climate change management and climate change risk as a function of ideology. We plotted prediction lines for every level of ideology. However, there is another way we can explore interaction effects. Using the *interplot()* function, we can calculate and visualize esitmates of the estimated coefficient of an independent variable in an interaction. Meaning, instead of looking at predicted values of a dependent variable, we are looking at the estimated effect of an independent variable on a dependent variable in a model that includes a two-way interaction. 

Let's create an interaction plot for our earlier model. We need to specify the two variables in the interaction. The first variable we specify is the variable of interest, the one for which we want estimated coefficients. The second is the other variable in the interaction.

```{r ip, echo=TRUE}
interplot(lm5, var1="glbcc_risk", var2 = "ideol")
```

This plot graphs the estimated coefficient of climate change risk by ideology score. As we can see, the estimated effect of climate change risk on federal climate change management appears stronger for more conservative individuals. This is completely consistent with our earlier findings. 

We can futher customize this plot, though. Instead of plotting individual estimates, we could plot a line that also visualizes the relationship. To do this we would specify *hist=T*, which plots a histogram on the bottom and a line for the estimated coefficients. Let's add titles on this plot as well:

```{r ip2, echo=TRUE}
interplot(m=lm5, var1="glbcc_risk", var2="ideol", hist=T) +
  ggtitle("Estimated Coefficient of Climate Change Risk by Ideology") +
  theme_bw() +
  xlab("Ideology: Liberal to Conservative") +
  ylab("Climate Change Risk Coefficient")


```


Good work!




