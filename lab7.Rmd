---
title: 'Lab Seven: Bivariate Regression'
author:
 - Alex Davis
 - Cody Adams
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Methods Labs")
ds <- read.csv("Class Data Set Factored.csv")
library(ggplot2)
library(psych)
library(car)
library(memisc)
library(stargazer)
library(grid)
library(gridExtra)
options(scipen = 999)
```

This lab will cover the basics of bivariate linear regression. The aim is to estimate a line that minimizes the error term. The deep mathematical specifics of regression is already covered in the book and so we won't spend too much time on it, but we will start with understanding how to do bivariate regression by hand. The following packages are required for this lab: 

1. ggplot2
2. psych
3. car
4. memisc
5. stargazer
6. grid
7. gridExtra

## Part I: Bivariate Regression by Hand

Suppose you wanted to explore the relationship between ideology and concern for natural resources. We will use this as our example research. First we should create a subset of our data and remove missing observations:

```{r 1, echo=TRUE}
ds.sub <- subset(ds, select = c("ideol","cncrn_natres"))
ds.sub <- na.omit(ds.sub)
```

Now let's look at some descriptive information about our variables:

```{r 2, echo=TRUE}
describe(ds.sub$ideol)
describe(ds.sub$cncrn_natres)
```


Our ideology variable runs from 1 to 7, with one being very liberal and seven being very conservative. The cncrn_natres variable measures concern for natural resources ranging from 0 to 10, with 0 being not concerned for natural resources and 10 being extremely concerned. We need to identify our dependent and independent variables:

DV: Ideology
IV: Concern for natural resources




Recall the proper formula for bivariate linear regression:

$$y_i=\alpha+\beta{x}_i+\varepsilon_i$$

When dealing with samples, though, this is the formula we use:

$$Y_i=A+B{X}_i+E_i$$
Our first step need to be calculating A and B. Recall that the formular for finding B:

$$B=r*s{y}/s{x}$$
where r is the correlation coefficient of the variables and sy and sx are the sample standard deviations of our variables. First let's create x and y variables to make the calculations simpler, with x being the independent variable and y being the dependent variable:

```{r biv, echo=TRUE}
ds.sub$x <- ds.sub$ideol
ds.sub$y <- ds.sub$cncrn_natres
```

Now find the correlation and respective standard deviations. Review the previous lab for a refresher on calculating correlation the long way!

```{r biv2, echo=TRUE}
r <- cor(ds.sub$x, ds.sub$y)
sd.y <- sd(ds.sub$y, na.rm = T)
sd.x <- sd(ds.sub$x, na.rm = T)
```

We now have all the information we need to calculate B:

```{r biv3, echo=TRUE}
B <- r * (sd.y/sd.x)
```

Now that we have B, we can calculate A, or the intercept (also called the constant). Recall the formula for calculating A:

$$A=\bar{Y} - B\bar{X}$$
Where Y bar and X bar are the sample means. Let's find those first:

```{r biv4, echo=TRUE}
ybar <- mean(ds.sub$y, na.rm = T)
xbar <- mean(ds.sub$x, na.rm = T)
```

Now calculate A:

```{r biv5, echo=TRUE}
A <- ybar - (B*xbar)
```

Take a look at our calculated A and B:

```{r biv6, echo=TRUE}
A
B
```

Now let's create some predicted values of Y, concern for natural resources, based on our regression formula:

```{r biv7, echo=TRUE}
yhat <- A + B*ds.sub$x
head(yhat) # Returns the first 5 values
```

Recall that residuals are calculated by taking the difference in the observed value of the data and the predicted value of the regression line. We can calculate residuals simply by subtracting the predicted y values we just calculated from the observed y values:

```{r biv8, echo=TRUE}
res <- ds.sub$y - yhat
```


Now that we have the residuals, we need to calculate the residual standard error, which measures the spead of observations around the regression line we just calculated. We also need to know the residual standard error so that we can find the standard errors for the regression coefficients, which are then used to calculate the t scores of the coefficients. To calculate the residual standard error we need to find:

1. The residual sum of squares
2. The degrees of freedom for our model

To find the residual sum of squares, first square the residuals, and then take the sum:

```{r rss, echo=TRUE}
res.sqr <- res^2
RSS <- sum(res.sqr, na.rm=T)
```

Now we need to calculate the degrees of freedom. Take your n size minus one degree of freedom for every parameter you estimate. In this case, we have the intercept and the ideology variable, so we lose two. Since we subset our data to remove all missing observations, we know the n size for x and y are the same. 


```{r df, echo=TRUE}
df <- length(ds.sub$y)-2
df
```

With the residual sum of squares and the degrees of freedom, we have what we need to find the residual standard error. The formula is the square root of the residual sum of squares divided by the degrees of freedom:

```{r rse, echo=TRUE}
RSE <- sqrt(RSS/df)
RSE
```


With the residual standard error, we can now start to calculate the standard errors for our coefficients, the intercept and the Beta coefficient for ideology. To calculate these, we need to find the total sum of squares of the independent variable, x. To do this you take the sum of the distances to the mean squared:

```{r tssx, echo=TRUE}
TSSx <- sum((ds.sub$x - xbar)^2)
TSSx
```

Now that we have the total sum of squares for the independent variable, we can find the standard errors of our coefficients. To find the standard error of B, take the residual standard error divided by the square root of the total sum of squares of x:

```{r SEb, echo=TRUE}
SEB <- RSE/sqrt(TSSx)
SEB
```

For the standard error of the intercept, the calculation is different. Take the residual standard error and multiply it by the square root of 1 over n plus the sample mean squared divided by the total sum of squares for x. That might sound terrible but it is relatively straight-forward.

```{r SEa, echo=TRUE}
SEA <- RSE*sqrt((1/2508)+(xbar^2/TSSx))
SEA
```


With the standard errors calculated, we can now find the corresponding t scores. To do this you take the coefficient and divide it by its standard error:

```{r t, echo=TRUE}
t.B <- B/SEB
t.B
```

```{r t2, echo=TRUE}
t.A <- A/SEA
t.A
```

These t scores tell us how many standard errors the coefficients are away from 0. 

### Calculating Goodness of Fit

Now we'll turn to measuring how well our model explains the variation in the dependent variable. Measuring goodness of fit allows us to better understand how much substantive significance our model provides. First we will calculate R squared, known as the coefficient of determination. R squared is a coefficient that tells you how much of the variance in the dependent variable is explained by your model. To find R squared, you need to know

1. The Residual sum of squares
2. The Total sum of squares
3. The Explained sum of squares.

We already have the residual sum of squares. We found it by taking the sum of the residuals squared. Earlier we calculated the total sum of squares for our independent variable, x , but now we need to find the total sum of squares for our dependent variable, y. We do this by taking the sum of the distances to the mean, ybar, squared.

```{r r, echo=TRUE}
TSS <- sum((ds.sub$y - ybar)^2)
TSS
```

Now that we have the residual sum of squares and the total sum of squares, we can find the explained sum of squares. The explained sum of squares tells us how much of the variance of the dependent variable is accounted for by our model. This is found simply by subtracting the residual sum of squares from the total sum of squares. 

```{r ess, echo=TRUE}
ESS <- TSS - RSS
ESS
```

R squared is found by dividing the explained sum of squares by the total sum of squares:

```{r r2, echo=TRUE}
r.sqr <- ESS/TSS
r.sqr
```


Hopefully you didn't get too bogged down in that alphabet soup! What we have here is an R squared value of 0.04, telling us that our model accounts for about 4 percent of the variation of the dependent variable.

#### Adjusted R Squared

There is a slightly more accurate measure of model fit, though, known as adjusted R squared. Adjusted R squared addresses some problems that are inherent in the R squared calculation, like the realtiy that R squared tends to increase as you add more predictors to your model, even if it's more due to chance than actual predicting power. Adjusted R squared addresses this issue by penalizing the model for an increased number of predictors. Use this formula to find adjusted R squared

$$1-\frac{(1-R^{2})(n-1)}{n-k-1}$$

where k is the number of predictors in our model, not including the intercept(A). We can actually find this pretty simply. Recall that we are working with an n size of 2508. Our k value is 1, becuase we only have one predictor in the model (ideology). Find the adjusted R squared value:

```{r adjr, echo=TRUE}
adj.r2 <- 1-(((1-r.sqr)*(2508-1))/(2508-1-1))
adj.r2
```


### Checking Our Work

Congratulations! Now it's time for us to check our work by telling R to do all the calculations we just did. Performing linear regression in R is pretty simple. Essentially you tell R to contruct a linear model using the *lm()* function, and you put the tilda, ~, in between your dependent and indepedent variables. Recall that we have been examining the relationship between iddeology and concern for natural resources. Let's build the model here:

```{r ols, echo=TRUE}
model <- lm(ds.sub$cncrn_natres~ds.sub$ideol)
```

Now we can use the *summary()* function to see the results of the model:

```{r ols2, echo=TRUE}
summary(model)
```

With all this information, we can check if we performed our calculations correctly. Let's look at our coefficients, residual standard error, coefficient standard errors, t scores, r squared, and adjusted r squared.

```{r ols3, echo=TRUE}
stats <- data.frame(name=c("Intercept", "Beta", "RSE", "IntSE", "BetaSE", "IntT", "BetaT", "Rsqr", "AdjRsqr"),
                    values=c(A,B,RSE,SEA,SEB,t.A,t.B,r.sqr,adj.r2))
stats
```

Looks like our calculations were correct!

## Part II: Bivariate Regression in R

Moving onto a section on doing bivariate regression in R, let's suppose you wanted to explore the relationship between ideology and one's opinions about how much of Oklahoma's electricity should come from renewable sources. The survey data we use for class asked respondents what percentage of the state's electricity should come from renewable sources. This will be our dependent variable. Our independent variable will be ideology.

Let's start by createing a subset data set with the variables we need and then removing missing observations

```{r br, echo=TRUE}
sub.ds <- subset(ds, select=c("okelec_renew", "ideol"))
sub.ds <- na.omit(sub.ds)
```

Since we're working with a variable that we haven't used yet, we should examine its structure:

```{r br2, echo=TRUE}
str(sub.ds$okelec_renew)
```

Notice how R tell us it is a factor? That is not going to work. Fortunately this is a good time to brush up on our coercing skills. Using *as.numeric*, create a new variable that is a numeric verson of the renewable energy variable:

```{r br3, echo=TRUE}
sub.ds$renew <- as.numeric(sub.ds$okelec_renew)
```

Now look at the structure:

```{r br4, echo=TRUE}
str(sub.ds$renew)
```

With the variable now numeric, examine it using the *describe* function:

```{r br5, echo=TRUE}
describe(sub.ds$renew)
```

Let's now specify a hypothesis. It's reasonable to theorize that concervatives would want a lower percentage of renewable energy than liberals. So thinking of the renewable energy variable as a function of ideology, we can specify a hypothesis that a more conservative ideology will correspond with a decreased preference for renewable energy. The null hypothesis, what we're testing against, is that there is no difference in preference for renewable enrgy by ideological score. 

First we should examine the normality of both variables. In the past we used different functions, but now that we're using GGplot, let's visualize normality in GGplot. We'll create a histogram and overlay it with a normal distribution curve, with the correct mean and standard deviation. 

```{r br6, echo=TRUE}
ggplot(sub.ds, aes(renew))+
  geom_histogram(aes(y=..density..), bins = 10) +
  stat_function(fun = dnorm, args = list(mean=mean(sub.ds$renew), sd = sd(sub.ds$renew)))
```


```{r br7, echo=TRUE}
ggplot(sub.ds, aes(ideol))+
  geom_histogram(aes(y=..density..), bins = 7) +
  stat_function(fun = dnorm, args = list(mean=mean(sub.ds$ideol), sd = sd(sub.ds$ideol)))
```


Now let's construct the model:

```{r br8, echo=TRUE}
model1 <- lm(sub.ds$renew~sub.ds$ideol)
summary(model1)
```

Based on this calculation, it appears that the relationship between ideology and preference for renewable energy is extremely statistically significant. But how should we interpret the results beyond that? To begin, look at the intercept. An intercept coefficient of 43.94 suggests that if ideology were zero, not counted in the equation, the model predicts a dependent variable score of 43.94, meaning a preference that 43.94% of the state's electricity comes from renewable energy sources. 

Now look at the ideology coefficient. With a value of -2.45, this suggests that a one unit increase in ideology(meaning one unit more conservative), corresponds with a -2.45 unit decrease in preferred percentage of elextricity coming from renewble energy. Take a look at the residual standard error and r squared values, as well.

We should always visualize a relationship that we're trying to convey. Let's start with constructing a scatter plot, like we did in the previous lab, and adding a regression line:

```{r br9, echo=TRUE}
ggplot(sub.ds, aes(x=sub.ds$ideol, y=sub.ds$renew)) +
  geom_point(shape=1) +
  geom_smooth(method = lm) +
  geom_jitter(shape=1)
```


Sometimes it is more benefitial when visualizing the relationship to plot the regression line without first showing the scatterplot. To do this with GGplot, simply do not include the *geom_point* or *geom_jitter* functions in the visualization.

```{r br10, echo=TRUE}
ggplot(sub.ds, aes(x=sub.ds$ideol, y=sub.ds$renew)) +
  geom_smooth(method = lm) 
```

## Part III: The Residuals

Recall that when using Ordinary Least Squares regression, there are three assumptions made about the error terms:

1. Errors have identitcal distributions
2. Errors are independent of X and other error terms
3. Errors are normally distributed

Since the error term is unobserved, we use the residuals in their place. 

To look at the values your model created, use the *names* function:

```{r res, echo=TRUE}
names(model)
```

In R we can examine the distribution of the residuals relatively simply. First let's assign the residuals to an object:

```{r res2, echo=TRUE}
resid <- model$residuals
```

Now we can plot a histogram of the residuals, we'll also add a normal density curve with the mean and standard deviation of our residuals:

```{r res3, echo=TRUE}
ggplot(model, aes(model$residuals)) +
  geom_histogram(aes(y= ..density..)) +
   stat_function(fun = dnorm, args = list(mean=mean(model$residuals), sd = sd(model$residuals)))
```

We can also look at a QQplot of the residuals:

```{r res4, echo=TRUE}
qqPlot(model1$residuals)
```

## Part IV: Comparing Models

Suppose you wanted to create multiple bivariate models and compare them to each other. Using the *mtable()* function from the memisc package, this is a simple thing to do. First decide what you want to compare. Let's create three models, each looking at the relationship between an independent variable and concern about global climate change. The three independent variables will be ideology, certainty that humans cause climate change, and age. We'll start by creating a subset of our data and removing missing observations, then we'll create the three models:

```{r comp, echo=TRUE}
sub <- subset(ds, select=c(glbcc_risk, glbcc_cert, age, ideol))
sub <- na.omit(sub)
model1 <- lm(sub$glbcc_risk~sub$ideol)
model2 <- lm(sub$glbcc_risk~sub$glbcc_cert)
model3 <- lm(sub$glbcc_risk~sub$age)
```

Using the *mtable()* function, we can create regression tables that compare all three of the models:

```{r comp2, echo=TRUE}
mtable(model1, model2, model3)
```

Alternatively, you can use the *stargazer()* function to create tables for your models. The *stargazer()* function is different in that you can specify a table created with text or with LaTex code that you can subsequently paste into a LaTex document. We'll create a text table, but if you wanted to create a Latex table, you would change the *type=* argument to *"text"*:

```{r com3, echo=TRUE}
stargazer(model1, model2, model3, type="text", style="apsr")
```
 
### Visualizing Multiple Models

To visualize all three of the models, you need to create three separate plots. Similar to the previous lab, let's create three visualizations and use the *grid.arrange()* function:

```{r vis, echo=TRUE}
ideol <- ggplot(sub, aes(x=sub$ideol, y=sub$glbcc_risk)) +
  geom_smooth(method = lm) +
  ggtitle("Ideology and Concern for Climate Change")

cert <- ggplot(sub, aes(x=sub$glbcc_cert, y=sub$glbcc_risk)) +
  geom_smooth(method = lm) +
  ggtitle("Climate Change Certainty and Concern for Climate Change")

age <- ggplot(sub, aes(x=sub$age, y=sub$glbcc_risk)) +
  geom_smooth(method = lm) +
  ggtitle("Age and Concern for Climate Change")

grid.arrange(ideol, cert, age)
```


## Part V: Hypothesis Testing

Let's do one more example of how we would hypothesis test with bivaraite regression. First let's build a make-shift theory. In our class data set there is a variable, wtr_comm, that asks the respondent if they think the supplies of water in their region will be adequate to meet their community's needs over the next 25 years. In essence, this measures concern about water supply for the community.

Start with a research question: What is the relationship between concern for water supply and concern about climate change?

Now build a theory: We could reasonably theorize that individuals who are more concerned about water supply are also likely more concerned about climate change. There is likely a link in their head between climate change and a shortened water supply. 

Built on this "theory", we can specify a hypothesis that individuals more concerned about climate change will be more concerned about water supply for their community. The null hypothesis is that there is no relationship between climate change concern and water concern. Remember that we are really testing against the null hypothesis.

We need to create a subset of the dataset that includes out two variables and removes missing observations:

```{r hyp, echo=TRUE}
new.ds <- subset(ds, select = c("wtr_comm", "glbcc_risk"))
new.ds <- na.omit(new.ds)
```

Now let's examine our variables:

```{r hyp2, echo=TRUE}
describe(new.ds$wtr_comm)
```

```{r hyp3, echo=TRUE}
describe(new.ds$glbcc_risk)
```

Note that the climate change risk varible goes from 0 to 10 and the water supply concern variable ranges from 1 to 5, with 1 being definitely no (the supplies of water are NOT enough) and 5 being definitely yes.
 
Now let's visualize the normality of the variables:

```{r hyp4, echo=TRUE}
ggplot(new.ds, aes(wtr_comm)) +
  geom_density(adjust=3) +
  stat_function(fun = dnorm, args = list(mean=mean(new.ds$wtr_comm), sd = sd(new.ds$wtr_comm)), color="blue")
```

```{r hyp5, echo=TRUE}
ggplot(new.ds, aes(glbcc_risk)) +
  geom_density(adjust=3) +
  stat_function(fun = dnorm, args = list(mean=mean(new.ds$glbcc_risk), sd = sd(new.ds$glbcc_risk)), color="blue")
```


The next step is to create the model. Recall that our dependent variable is concern for water supply, and the independent variable is climate change risk.

```{r hyp6, echo=TRUE}
lm1 <- lm(new.ds$wtr_comm~new.ds$glbcc_risk)
```

Now examine and interpret the results:

```{r hyp7, echo=TRUE}
summary(lm1)
```

These results tell us that when climate change risk is zero (at the intercept), the average concern for water supply is about 3.73. That is the Alpha coefficient. Now to examine the Beta coefficient. The coefficient is about -.09, with a corresponding p value of essentially 0. This means that a one unit change in climate change risk corresponds with a -0.09 unit change in water supply concern. Remember that the water supply variable goes from 1 (there is definitely not enough water) to 5 (there definitely is enough water). These findings suggest that an individual more concerned about climate change is also more concerned about water supply. 

We should examine the normality of the residuals:

```{r hyp8, echo=TRUE}
ggplot(lm1, aes(lm1$residuals)) +
  geom_density(adjust=3) +
  stat_function(fun = dnorm, args = list(mean=mean(lm1$residuals), sd = sd(lm1$residuals)), color="blue")
```

Now let's build a good visualization that would be worthy of being in a paper:

```{r hyp9, echo=TRUE}
ggplot(new.ds, aes(x=wtr_comm, y=glbcc_risk)) +
  geom_smooth(method = lm) +
  coord_cartesian(ylim = c(2,9), xlim = c(1,5)) +
  ggtitle("Concern for Water and Climate Change") +
  xlab("Considers Water Supply Adequate") +
  ylab("Perceived Climate Change Risk") +
  scale_x_continuous(breaks=c(1,2,3,4,5), labels=c("Definitely No", "Probably No", "Unsure", "Probably Yes", "Definitely Yes")) +
  theme_bw()
  
```

Taking our findings into account, we can reject the null hypothesis. It is clear that individuals less concerned about climate change are also less concerned about their community's water supply. 


