---
title: 'Lab Four: Foundations for Inference'
author:
 - Alex Davis
 - Cody Adams
output: pdf_document
---

This lab introduces the tools for the foundation of inference by examining the normal distribution, standard errors, confidence errors, and single sample t-tests. The following packages are required:

1. car
2. psych
3. sm
4. HistData
5. ggplot2
6. dplyr

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Methods Labs")
ds <- read.csv("Class Data Set Factored.csv")
library(car)
library(psych)
library(sm)
library(HistData)
library(ggplot2)
library(dplyr)
options(scipen=999)
```

## Part I: Testing for Normality

Lab three introduced the basics of normal distributions. Much of the statistical work done is built on the condition of data that follows a normal distribution. Inspecting data for normality is done via numerous methods.

Recall the _rnorm()_ function to generate random values that follow a normal distribution given specified parameters. The following _random_ object will consist of 1000 random values given $\mu$ = 50 and $\sigma$ = 5:

```{r norm1, echo=TRUE}
random <- rnorm(1000, mean = 50, sd = 5)
```

Visualizing data is an important first step to inspecting data for normality. The previous lab introduced density plots, box plots, and QQ plots as means of visualizing data and their relationships to a normal distribution. Recall that the QQ plot graphs the quantiles of data against quantiles of a normal distribution, given the $\mu$ and $\sigma$ of the data.

```{r qq, echo=TRUE}
ggplot(as.data.frame(random), aes(sample = random)) +
  stat_qq()
```

The QQ plot of the _random_ object demonstrates the data closely follows a normal distribution. This is expected given the _random_ object was created via the _rnorm()_ function. __Note:__ Visualizing data for normality is an informal approach to inspecting for normality. Various empirical methods exist to test whether data follows a normal distribution, the most popular of which are:

1. Shapiro–Wilk test
2. Anderson–Darling test
3. Kolmogorov–Smirnov test
4. Pearson's chi-squared test

The Shapiro-Wilk test is the most popular method, as the test has been demonstrate as providing the most power for a given significance.

### Shapiro-Wilk Test

The _shapiro.test()_ function in R employs the Shapiro-Wilk test on data to test whether the data are normally distributed. Use of the Shapiro–Wilk test is contingent on univariate and continuous data. The hypotheses for the Shapiro-Wilk test are:

$H_0$: The data are normally distributed  
$H_1$: The data are not normally distributed

__Note:__ The Shapiro-Wilk test for normality is a statistical test that provides a p-value of the test statistic, _W_. This lab will focus on the p-value approach for statistical tests, using an $\alpha$ value of 0.05 as the desired significance level.

```{r sw test, echo=TRUE}
shapiro.test(random)
```

The Shapiro-Wilk test p-value is greater than $\alpha$ = 0.05, therefore failing to reject $H_0$ concluding the data are normally distributed. Again, this is expected given the _random_ object was created via the _rnorm()_ function.

### Testing Normality 

R provides data pertaining to Yellowstone National Park's Old Faithful geyser in the _faithful_ object. Old Faithful eruptions are recorded as duration, in minutes, between events. First, the _describe()_ function provides valuable information for the _eruptions_ variable in the _faithful_ object.

```{r faith, echo=TRUE}
describe(faithful$eruptions)
```

Comparing the _eruptions_ data to a normal distribution, given $\mu$ and $\sigma$ from the _eruptions_ data, is available via the *geom_density()* and *stat_function()* functions.

```{r faith2, echo=TRUE}
ggplot(faithful, aes(eruptions)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(faithful$eruptions), 
                                         sd = sd(faithful$eruptions)), color = "blue")
```

The black line represents the _eruptions_ data, and the blue line represents the normal distribution given the $\mu$ and $\sigma$ values of _eruptions_. __Note:__ The _eruptions_ data appears bimodal, and does not fit the normal distribution model given the parameters calculated via the _eruptions_ data. 

The _eruptions_ data is further examined using QQ plots via the _qqPlot()_ function:

```{r faith4, echo=TRUE}
qqPlot(faithful$eruptions)
```

Most of the points in the QQ plot fall outside the region defined by the dashed lines, further suggesting the _eruptions_ data is likely not normally distributed. Lastly, a Shapiro-Wilk test can confirm whether the _eruptions_ data is normally distributed:

```{r faith3, echo=TRUE}
ggplot(faithful, aes(sample = eruptions)) +
  stat_qq()
```

The Shapiro-Wilk test p-value is less than $\alpha$ = 0.05, leading to reject $H_0$: data are normally distributed. In conclusion, the _eruptions_ data is not normally distributed. __Note:__ The visual plots are likely enough to confirm the _eruptions_ data are not normally distributed.

The _MacdonellDF_ variable within the _HistData_ package consists of finger length data. Again, visualizing the data for normality is performed via various methods. A box plot is generated for the _MacdonellDF_ data:

```{r f, echo=TRUE}
ggplot(MacdonellDF, aes(x = "", y))
```

The box plot appears balanced, indicating normality. The _sm.density()_ function should show the distribution of _MacdonellDF_ variable as similar to the projected normal distribution given the $\mu$ and $\sigma$ parameters of the _MacdonellDF_ variable.

```{r f2, echo=TRUE}
sm.density(MacdonellDF$finger, model = "Normal")
```

Additionally, a QQ plot:

```{r f3, echo=TRUE}
qqPlot(MacdonellDF$finger)
```

The visualizations all suggest the data is normally distributed; however, a Shapiro-Wilk test is still useful to test for normality.

```{r f4, echo=TRUE}
shapiro.test(MacdonellDF$finger)
```

__Note:__ Despite the visualizations suggesting normality, the Shapiro-Wilk test p-value is less than $\alpha$ = 0.05, resulting in rejecting the null hypothesis that the data are normally distributed. A closer examination of the QQ plot yields that most points exist outside the region, though at first glance nothing appears suspect. This merits a conversation on judgment for assessing whether the data could be treated as normally distributed.

## Part II : Standard Errors

Recall that the standard error is the standard deviation of the sample distribution, calculated as the square root of the standard deviation divided by the square root of the sample size.

- Population: $\frac{\sigma}{\sqrt{n}}$
- Sample: $\frac{s}{\sqrt{n}}$

R does not provide a single purpose function to calculate the standard error. As a result, the following demonstrates the previous formula to calculate the standard error:

```{r se, echo=TRUE}
sd(ds$age, na.rm=T)/sqrt(length(ds$age)-sum(is.na(ds$age)))
```

Alternatively, the _describe()_ function includes the standard error statistic for a given variable, as follows:

```{r se2, echo=TRUE}
describe(ds$age)
```

__Note:__ The previous two methods return the same result.

## Part III: Confidence Intervals

The standard error is vital to inferential statistics. For example, the standard error is required to calculate confidence intervals. Confidence intervals employ standard error to assist with inferring information from a sample of a larger population, through inclusion of uncertainty.

Confidence intervals synthesize  knowledge of standard error and z-scores. Recall that with the normal distribution, a z-score standardizes standard deviations values are from the mean. To calculate a level of confidence (90%, 95%, 99%), the z-score associated with those levels of confidence is necessary:

 - 90% confidence = 1.645
 - 95% confidence = 1.960
 - 99% confidence = 2.576
 
The formula for confidence level is:

$$CI=\bar{x} \pm z\frac{s}{\sqrt{n}},$$, or $CI=\bar{x} \pm z*SE$, 

Put simply, the confidence interval is the sample mean plus/minus the product of the z-score and standard error. 

Using the _age_ variable within the _ds_ data set, the a 95% confidence interval is calculated as follows: 

First calculate the mean:

```{r ci1, echo=TRUE}
mean(ds$age, na.rm = T)
```

Second, find the standard error:

```{r ci2, echo=TRUE}
describe(ds$age)$se ## this is a way to pull the standard error out without having to look at everything else
```

Given a 95% confidence interval, the z-score is 1.96. The upper bound of the confidence interval is calculated as follows:

```{r ci3, echo=TRUE}
60.38 + 1.96*.28
```

The lower bound of the confidence interval is calculated as follows:

```{r ci4, echo=TRUE}
60.38 - 1.96*.28
```

With 95% confidence, the population mean of _age_ is between 59.83 and 60.93 (rounded).

Increasing confidence will increase the interval of the confidence interval. For example, the z-score associated to 99% is 2.58, so the confidence interval for 99% is calculated as follows:

```{r ci5, echo=TRUE}
se <- describe(ds$age)$se
```

__Note:__ The standard error of the _age_ variable was stored as to the _se_ object via specifying only to return the standard error from the _describe()_ function. Using the _se_ object simplifies the confidence interval calculation as exemplified:

```{r ci6, echo=TRUE}
mean(ds$age, na.rm = T) + 2.58*se
mean(ds$age, na.rm = T) - 2.58*se
```

With 99% confidence, the population mean of _age_ is between 59.64 and 61.09. This interval is larger than the interval calculated for 95% confidence.

Alternatively, the _t.test()_ function in R provides the 95% confidence interval for a given variable.

```{r ci7, echo=TRUE}
t.test(ds$age)
```

The results of _t.test()_ is similar to the manual calculations performed previously. By default, the _t.test()_ tests whether $\mu$ for the variable is equal to zero. The hypotheses for the _t.test()_ function are:

- $H_0$: $\bar{x}$ = $\mu$
- $H_1%: $\bar{x}$ $\neq$ $\mu$

Note the returned p-value of the previous _t.test()_ performed is less than $\alpha$ = 0.05; therefore, the null hypothesis is rejected. That is, given the sample data, with 95% confidence the $\mu$ is not zero.

To explain a different way, using the same variable, but with a defined $\mu$ of 50, the _t.test()_ function can test whether $\mu$ is 50.

```{r ci8, echo=TRUE}
t.test(ds$age, mu=50)
```

The same result occurs: the null hypothesis is rejected. Once again, the _t.test()_ function is employed to test whether $\mu$ = 60:

```{r ci9, echo=TRUE}
t.test(ds$age, mu=60)
```

The result differs, such that the null hypothesis is not rejected as the p-value is greater than $\alpha$ = 0.05. That is, with 95% confidence, $\mu$ = 60.

__Note:__ The p-values can be calculated manually from z-scores to test population means using confidence intervals. This requires:

1. Calculate $\bar{x}$
2. Calculate confidence intervals (as previously shown)
3. Calculate the p-value associated with $H_0$

Objects are employed to simplify the calculation:

```{r p, echo=TRUE}
xbar <- mean(ds$age, na.rm = T)
a <- 60
s <- sd(ds$age, na.rm = T)
n <- 2547
```

The p-value for if the population mean $\neq$ 60:

```{r p2, echo=TRUE}
2*(1-pnorm(xbar, mean=a, sd=s/sqrt(n)))
```

The p-value of 0.191 is greater than $\alpha$ = 0.05; therefore, failing to reject $H_0$: $\mu$ = 60.

## Part IV: More on Single Sample T-tests

The previous confidence interval section introduced a preliminary discussion of t-tests. The Student's t distribution is based on sample estimates, whereas the normal distribution is used when $\sigma$ and $\mu$ are known. 

Performing a t-test requires:

1. n
2. s
3. $\bar{x}$
4. t statistic

To demonstrate, the following tests whether the population mean is 58 for the given data by defining the above parameters:

```{r t test, echo=TRUE}
nAge <- length(ds$age)-sum(is.na(ds$age))
sdAge <- sd(ds$age, na.rm=TRUE) # Standard deviation of age
seAge <- sdAge/(sqrt(nAge)) # Standard error of age
meanAge <- mean(ds$age, na.rm = TRUE)
```

To calculate the t statistic, the population mean is subtracted from the sample mean, and divided by the standard error as follows: 

```{r t test2, echo=TRUE}
t_value <- (meanAge - 58)/seAge   
t_value
```

To calculate the two-sided p value under the t distribution:

```{r t test3, echo=TRUE}
2*(1-pt(abs(t_value),df=nAge-1))
```

The returned p-value is less than $\alpha$ = 0.05, rejecting $H_0$: $\mu$ = 60. Put another way, $H_1%: $\mu$ $\neq$ 60.

```{r t test4, echo=TRUE}
t.test(ds$age, mu=58)
```

With 95% confidence, $\mu$ $\neq$ 60.