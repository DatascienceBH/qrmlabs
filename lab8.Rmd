---
title: 'Lab Eight: Multiple Linear Regression'
author:
 - Alex Davis
 - Cody Adams
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(psych)
library(car)
library(memisc)
library(stargazer)
library(gridExtra)
options(scipen = 999)
setwd("~/Methods Labs")
ds <- read.csv("Class Data Set Factored.csv")
```


This lab will cover the basics of multiple linear regression. We will begin by covering the basics of working with matrices in R, then move to doing OLS regression in matrix form. After that, we will cover an introduction to multiple linear regression in R, followed by visualizing it. The following packages are required for this lab: 

1. ggplot2
2. psych
3. car
4. memisc
5. stargazer
6. gridExtra

## Part I: Working with Matrices

### The Basics of the Matrix

We first need to go over some basics of matrix algebra. Matrix algebra provives the building blocks for multiple linear regression analysis, which we will learn all about throughout this lab. We will start by learning the basic parts of working with matrices. 

A Matrix is a rectangular array of numbers with rows and columns, with each individual number in a matric called an element. When working with matrices, all operations performed on a matrix are performed simultaneously. To create a matrix in R, use the *matrix()* command. Take note of the syntax:

*matrix(c(elements), #of rows, #of columns)*

Let's create a matrix called A with two rows and three columns. We'll fill it with some random values. When entering the elements of the matrix, you count down the columns, not across the rows.

```{r mat, echo=TRUE}
A <- matrix(c(10,-12,5,1,8,0),2,3)
A
```

When working with matrices, you often have to transpose the matrix. When you transpose a matrix, you are make the first column become the first row, the second column become the second row, and so on and so on. We refer to a transposed matrix as the prime of the original matrix. So our matrix A transposed is A'. Let's transpose it. Simply use the *t()* function:

```{r mat2, echo=TRUE}
Aprime <- t(A)
```

Now compare A and A':

```{r mat3, echo=TRUE}
A
Aprime
```

Notice how the first column in A became the first row in A', the second column in A became the second row in A', and so on? We'll come back to transposed matrices shortly.

There are a variety of operations you cna perform on matrices. When you add matrices, the matrices must have the same dimensions (number of rows and columns). We have already created matrix A with two rows and three comumns, so let's create a matrix B with the same dimensions:

```{r mat4, echo=TRUE}
B <- matrix(c(6,2,8,4,7,2),2,3)
```

When adding matrices, you add each element in A to its corresponding element in B. R does this for us when we tell it to add the matrices together:

```{r mat5, echo=TRUE}
AplusB <- A+B
AplusB
```

Multiplying matrices is a little more complicated. To multiple two matrices, they must be conformable, which means the number of columns in the first matrix must equal the number of rows in the second. The two matrices we already created are not conformable, so let's create two new ones, C and D. C will have 2 rows and 3 columns and D will have 3 rows and 2 columns:

```{r mat6, echo=TRUE}
C <- matrix(c(2,1,6,5,0,-2),3,2)
D <- matrix(c(4,5,2,7,1,2),2,3)
C
D
```

When multiplying matrices, you multiple the first element of the first column in A by the first element of the first row in B and add them, then move onto the second elements, and so on. Fortunately R does this for us with the %*% command:


```{r mat7, echo=TRUE}
CxD <- C %*% D
CxD
```

You might have realized that transposing a matrix creates a conformable version of that matrix. This is in essence how to square a matrix. You multiple the matrix by the transposed version of the matrix. Let's transponse matrix C and then square it by multiplying it by its prime:

```{r mat8, echo=TRUE}
Cprime <- t(C)
Csquared <- C %*% Cprime
Csquared
```

In matrix algebra there is also something called the *identity matrix*. The identity matrix is a square matrix with 1s on the diagonal and 0s elsewhere. In essence, it acts like a 1 in algebra. In algebra, anything multiplied by 1 is itself. In matrix algebra, a matrix multiplied by its identity matrix is itself. For example:

```{r mat9, echo=TRUE}
E <- matrix(c(5,2,8,1),2,2)
E
I <- matrix(c(1,0,0,1),2,2)
I
E %*% I
```

An understanding of the identity matrix is needed to understand how to work with matrix inversion. The matrix inversion operates similar to the process of dividing a number by itself in algebra. A matrix multiplied by its inverse if equal to the identity matrix. **This is only applicable to square matrices**. The notation for an inverse matrix:

$$A^{-1}$$

Therefore a matrix A multipled by its inverse is equal to the identity matrix, like this:

$$A^{-1}A = I$$

To actually solve for the inverse matrix by hand requires a lot of matrix algebra, which the book details richly. Fortunately, we can find the inverve matrix in R using the *solve()* function. We need to create a new matrix that is square. 

```{r mat10, echo=TRUE}
M <- matrix(c(3,2,1,4),2,2)
M
```

Now find the inverse of M:

```{r mat11, echo=TRUE}
M.inv <- solve(M)
M.inv
```

Now to show that a matrix multiplied by its inverse is equal to the identity matrix:

```{r mat12, echo=TRUE}
M %*% M.inv
```

### OLS Regression and Matrices

Recall the scalar formula for multiple linear regression:

$$Y= A + BX_1 + BX_2 + BX_3 + E$$
When working in matrix notation, the formula is shortened:

$$y=Xb + e$$
Therefore the formula for e:

$$e=y-Xb$$
Just like in bivariate regression, the aim in multiple linear regression is to minimize the sum of squared errors. If the error matrix is e, then the sum of squared errors would be the sum of e'e. The book provides a very thorough mathematical proof, but in essence we want to find the b that minimizes the sum of squared errors. Through a great proof that can be found in Chapter 11, the formular to find this b is

$$b=(X'X)^{-1}X'y$$

We can solve this by hand in R! We need to create two matrices right now, X and y:

```{r ols, echo=TRUE}
y <- matrix(c(6,11,4,3,5,9,10),7,1)
y
X <- matrix(c(1,1,1,1,1,1,1,4,7,2,1,3,7,8,5,2,6,9,4,3,2,4,3,4,6,5,4,5),7,4)
X
```

Now we need to find X'. Recall that to do this, we need to transpose X:

```{r ols2, echo=TRUE}
Xprime <- t(X)
Xprime
```

Next we have to calculate (X'X):

```{r ols3, echo=TRUE}
XprimeX <- Xprime%*%X
XprimeX
```

Now to find the inverse of (X'X):

```{r ols4, echo=TRUE}
XprimeXinv <- solve(XprimeX)
XprimeXinv
```

Remember the formula we are solving:

$$b=(X'X)^{-1}X'y$$
We have calculated the inverse of (X'X), so not we need to multiply it by X':

```{r ols5, echo=TRUE}
XprimeXinvXprime <- XprimeXinv%*%Xprime
XprimeXinvXprime
```

The final step is to multiple by y to find b:

```{r ols6, echo=TRUE}
b <- XprimeXinvXprime %*% y
b
```

So what did we just do? We just calculated a matrix of regression coefficients by hand! Based on our regression coefficients, let's calculate some predicted y values. Recall the formula:

$$\hat{y}=Xb$$

We already have everything we need to calculate a matrix of predicted y values:

```{r ols7, echo=TRUE}
y.hat <- X %*% b
y.hat
```

Now to calculate the vector of residuals. Rememeber that the residual is the distance beetween the actual observation and the predicted observation, so we substract the predicted values of y from the observed values of y:

```{r ols8, echo=TRUE}
e <- y - y.hat
e
```

Just like that, we've done multiple linaer regression by hand. Let's check our work. Just like in bivariate regression, use the *lm()* function:

```{r ols9, echo=TRUE}
ols <- lm(y ~ 0 + X)
ols
b
```


Looks like our b matrix is correct. Now let's check the predicted values:

```{r ols10, echo=TRUE}
ols$fitted.values
y.hat
```

Right again! Now to look at the residuals:

```{r ols11, echo=TRUE}
ols$residuals
e
```

Excellent work. Now that we've covered the basics of matrices and OLS regression in matrix form, let's move onto learrning how to utilize R to efficiently do multiple linear regression analysis.

## Part II: Multiple Regression in R

The R syntax for multiple linear regression is similar to what we used for bivariate regression. Essentially you add independent variables to the *lm()* function. Let's construct a model that looks at climate change certainty as the dependent variable and age and ideology as the independent variables:

```{r mlr, echo=TRUE}
sub.ds <- subset(ds, select = c("glbcc_cert", "ideol", "age"))
sub.ds <- na.omit(sub.ds)
model1 <- lm(sub.ds$glbcc_cert ~ sub.ds$ideol + sub.ds$age)
```

Now look at the model:

```{r mlr2, echo=TRUE}
summary(model1)
```

Before really understanding what these results tell us, we need to understand partial effects. Chapter 12 of the textbook goes into partial effects in great depth, so make sure you've read that. Essentially, multiple regression "controls" for the effects of other dependent variables when reporting the effect of one particular variable. This is not accidental. If multiple regression were not calculated in such a way, we would not be able to tease out the effects of variables without not knowing if the effect is actually caused by another variable. Let's explore this for our model. First construct a bivairate regression model for each of the independent variables in our multiple regression model:

```{r mlr3, echo=TRUE}
model2 <- lm(sub.ds$glbcc_cert ~ sub.ds$ideol)
model3 <- lm(sub.ds$glbcc_cert ~ sub.ds$age)
mtable(model2, model3)
```

Notice how the ideology coefficient in the bivariate model is larger than the ideology coefficient in the multiple regression model. The same is also true for the age variable. This is because the bivariate model reports the total effects of X on Y, but the multiple regression model reports the effects of X on Y when "controlling" for the other Xs. Let's look at the multiple regression model again:

```{r mlr4, echo=TRUE}
summary(model1)
```

To interpret these results, we would say that a one unit increase in ideology corresponds with a -0.392 unit decrease in climate change certainty, with all other variables held constant at their means. This is significant, as well. Given the large p value of the age coefficient, we would report that the partial effect of age on climate change certainty is not statisticially significant. You  might have noticed that the p value for the age coefficient was much smaller  
in the bivariate model. Age lost its statistical significance when we included ideology. This is a good example of why, in most cases, multiple regression will give you a clearer picture of a relationship. Only looking at age and climate change risk, we could potentially conlcude that there is a significant relationship. However, when we include ideology in the model, it becomes clear that ideology is the actual cause of the change in climate change certainty.

## Part III: Hypothesis Testing with Multiple Regression

Perhaps we wanted to explore the relationship between people's opinions about fossil fuels and ideology. Our class data set includes a question asking respondents what percentage of Oklahoma's electrcity should come from fossil fuels. It is reasonable to posit that more conservative individuals will want a higher percentage of the state's electricty to come from fossil fuels. For this test, we are going to include other indendentr variables, like age, income, and education. Let's establish a hypothesis that the more conservative a respondent is, the more electricity they want to come from fossil fuels, all other variables held constant. The null hypothesis is that there is not difference in preferred percentage of electricity coming from fossil fuels by ideology. First we need to look at our variables:

```{r mlr5, echo=TRUE}
str(ds$okelec_foss)
```

Notice that R reads the fossil fuels variable as a factor. We need to change this to a numeric variable. We will coerce it and create a new variable:

```{r mlr6, echo=TRUE}
ds$foss <- as.numeric(ds$okelec_foss)
```

Now let's look at all our variables. First we will create a subset of the data and remove missing observations, then use the *describe()* function:

```{r mlr7, echo=TRUE}
ds.sub <- subset(ds, select = c("income", "education", "ideol", "foss", "age"))
ds.sub <- na.omit(ds.sub)
describe(ds.sub$foss)
describe(ds.sub$income)
table(ds.sub$education)
describe(ds.sub$age)
```

Now construct the model:

```{r mlr8, echo=TRUE}
model <- lm(foss ~ income + education + age + ideol, data = ds.sub)
```

Note the different syntaax in constructing this model. Instead of writing *dataset$variable* every time, you can write the variable names, and at the end of the model include *data=dataset*. 

Now examine the model:

```{r mlr9, echo=TRUE}
summary(model)
```

To interpret these results, let's start with the intercept. In a regression model, the intercept is the expexted mean of our dependent variable when our independent variables are 0. In this case the expected mean is 5.83. Now examine the variable that our hypothesis is concerned with, ideology. Looking at the ideology variable, we can see that there is a statistically significant coefficient of 3.07. We can interpret this by saying that a one unit increase in ideology (from liberal to conservative) corresponds with a 3.07 unit increase in preferred percentage of electricty coming from fossil fuels, all other variables held constant. There are also significant relationships for age and income, suggesting an increase in those correspond with an increase in the dependent variable as well. The adjusted R squared vlue of .15 suggests that our model accounts for about 15 percent of the variation in the dependent variable. 

As is almost always the case, we need to visualize the model. Visualizing multiple linear regression isn't as simple as visualizing bivariate relationships. There is no intuitive way for us to visualize, in one graphic, how all of these variables look while all others are held constant simultaneously. For a relationship with one dependent variable and two independent variables, we can make a three dimensional scatterplot and regression plane. However, these still don't provide a very intuitive visualization. 

### Visualizing Multiple Linear Regression

The best way to visualize multiple linear regression is to create a visualization for each independent variable while holding the other independent variables constant. Doing this allows us to see how each relationship between the DV and IV looks. Constructing each graph in GGplot is essentially the same as going so for bivariate regression. Specify the data source (in this case the model itself), then the variables in the aesthetic (x,y), then use *geom_smooth(method="lm")* to construct a regression line. We'll use the *grid.arrange()* function to put each visualization in a grid together. 

```{r mlr10, echo=TRUE}
id <-ggplot(model, aes(ideol,foss)) + # Ideology visualization
  geom_smooth(aes(ideol,foss), method = lm)
age <- ggplot(model, aes(age,foss)) + # Age visualization
  geom_smooth(aes(age,foss), method = lm)
educ <- ggplot(model, aes(education,foss)) + # Education visualization
  geom_smooth(aes(education,foss), method = lm)
inc <- ggplot(model, aes(income,foss)) + # Income visualization
  geom_smooth(aes(income,foss), method = lm)
grid.arrange(id,age,educ,inc)
```

Now we can see the general relationship between each independent variable and the dependent variable. The *geom_smooth()* function defaults to showing 95% confidence intervals. You can technically turn them off with the *se=FALSE* argument, but that is not recommended. Notice the very large confidence interval in the income visualization, especially at the higher income levels. This happens because there are fewer observations with verg high incomes. A smaller sample size leads to a larger standard error, and in turn larger confidence intervals. 

Recall that our hypothesis stated that the more conservative a respondent, the more electricty they will prefer comes from fossil fuels. We can safely reject the null hypothsis, given the clear relationship and overall evidence that there is a positive relationship. Let's build a solid, paper-worthy visualization of the relationship betweeen ideology and opinions on fossil fuels from our model.

```{r mlr11, echo=TRUE}

ggplot(model, aes(ideol,foss)) +
  geom_smooth(aes(ideol, foss), method="lm") +
  ggtitle("Fossil Fuel Energy by Ideology") +
  ylab("% of State's Electricity Should Come From Fossil Fuels") +
  theme(axis.title.y = element_text(size = rel(.8), angle = 90)) + # Allows you to change font size and angle. 
  xlab("Ideology: Liberal to Conservative") +
  scale_x_continuous(breaks=c(1:7), labels = c("1", "2", "3", "4", "5", "6", "7")) +
  coord_cartesian(ylim = c(15,40), xlim = c(1,7)) +
  theme_bw()
  

```

## Part IV: Predicting with OLS Regression

Regression analysis is performed not only to explain relationships, but also to predict. In R, we can use the model we built to predict values of the dependent variable based on the values of the independent variables. Doing so is rather simple. Recall that our model attempts to explain how much of the state's electricity a respondent thinks should come from fossil fuels as a function of their ideology, age, education, and income. 

To start predicting, first we need to identify what values we want to assign to our independent variables. Perhaps you wanted to know the preferred percentage of energy coming from fossil fuels for an individual with a bachelor's degree, an income of 45000, 40 years old, and a moderate (3) ideology. First we need to know the beta coefficients for each variable:

```{r predict, echo=TRUE}
coef(model)
```


Now recall the scalar formula for multiple linear regression:

$$\hat{Y}_= A + BX_1 + BX_2 + BX_3 + E$$

Therefore for our model, the formula would be

$$Y= \alpha + \beta_{income} + \beta_{educ} + \beta_{age} + \beta_{ideol} + \epsilon$$
To find predicted values, the method is rather simple. Multiply each coefficient by the value we specified for each independent variable, add them together, and add the intercept value. Note that for a bachelor's degree, the value of the education variable is 6. 

```{r predict2, echo=TRUE}
(5.83 + (.000014*(45000)) + (-.19*(6)) + (.19*(40)) + (3.07*(3)))
```

Based on our calculation, an individual with out specification is predicted to think that about 22% of the state's electricity should come from fossil fuels. There is a more precise way to do this calculation, as is often the case. Using the *predict()* function, we can tell R to return predicted values based on specifications of variable values. 

```{r predict3, echo=TRUE}
predict(model, newdata = data.frame(ideol=3, income=45000, education=6, age=40))
```

Notice our original estimate is a little off. This is because we rounded. However, the numbers are very similar!

We can use the *predict()* function to find more than one value at a time. If we use the *seq()* argument, we can tell R to predict values for a sequence of values:

```{r predict4, echo=TRUE}
predict(model, newdata = data.frame(ideol=seq(1:7), income=45000, education=5, age=45))
```

We've been specifying values for the income, education, and age variables that are not the means. However, it is often appropriate to predict values when all other variables are held constant at their means. We can use the *predict()* function for this too!

```{r predict5, echo=TRUE}
p <- predict(model, newdata = data.frame(ideol=seq(1:7), income=mean(ds.sub$income, na.rm = T), education=mean(ds.sub$education, na.rm = T),
                                    age=mean(ds.sub$age, na.rm = T)), se.fit = T)
p
```

For a moment we'll switch to using base visualizations to make a connection between the prediction line we make earlier and the values we just calculated. Let's say we wanted to plot a line with the predicted values above. We can use the *plot()* function and the *lines()* function to do so.

```{r predict6, echo=TRUE}
plot(ds.sub$foss~ds.sub$ideol)
lines(p$fit~seq(1,7,1))
```

Now scroll up and look at our earlier visualization. Notice that the lines are the same. By plotting the values we predicted, we did what GGplot does for us. When we calculate predicted values with all other variables held constant, we are creating points in a prediction line.

When we use GGplot, we normally end up having confidence intervals around the prediction line. Let's take the long way in making confidence inetervals so you get a better sense of what is going on when GGplot makes them for us. Recall that confidence intervals are calculated using t scores (or z scores, depending on the distribution) and standard errors. When working with the t distribution like we are, we default to 95% confidence intervals, which is derived from a t score of 1.96. Earlier in the *predict()* function that gave us our values, we included the *se.fit=T* argument. This calculated standard errors for the points as well. To calculate the confidence interval for each point, we sinply need to add the predicted value to 1.96 multiplied by the standard error for that point.

We can visualize confidence intervals using the *polygon()* function, in which we rell R to make a polygon up to a point and then reverse it from that point, and include the values of the confidence intervals.  

```{r predict7, echo=TRUE}
plot(ds.sub$foss~ds.sub$ideol)
lines(p$fit~seq(1,7,1), col="red", lwd=2)
polygon(c(seq(1,7,1), rev(seq(1,7,1))), c(p$fit+1.96*p$se.fit, rev(p$fit-1.96*p$se.fit)), col = "#0052d250", border = FALSE )

```

Creating confidence intervals for a regression line is simply a matter of calculating the confidence intervals for each point, based on the standard error for each point. Fortunately GGplot does this for us, but it is important to understand the calculations behind it. 










