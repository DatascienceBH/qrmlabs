<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>11 Non-linearity, Non-normality, and Multicollinearity | Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration.</title>
  <meta name="description" content="11 Non-linearity, Non-normality, and Multicollinearity | Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="11 Non-linearity, Non-normality, and Multicollinearity | Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration." />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Non-linearity, Non-normality, and Multicollinearity | Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration." />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="categorical-explanatory-variables-dummy-variables-and-interactions.html">
<link rel="next" href="diagnosing-and-addressing-problems-in-linear-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#requirements"><i class="fa fa-check"></i>Requirements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html"><i class="fa fa-check"></i><b>1</b> Introduction to R, RStudio, and R Markdown</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html#introduction-to-r-the-language"><i class="fa fa-check"></i><b>1.1</b> Introduction to R, the language</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html#benefits-of-r"><i class="fa fa-check"></i><b>1.1.1</b> Benefits of R</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html#packages"><i class="fa fa-check"></i><b>1.1.2</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html#rsudio"><i class="fa fa-check"></i><b>1.2</b> RSudio</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html#navigating-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Navigating RStudio</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-r-rstudio-and-r-markdown.html"><a href="introduction-to-r-rstudio-and-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>1.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basics-of-r.html"><a href="basics-of-r.html"><i class="fa fa-check"></i><b>2</b> Basics of R</a><ul>
<li class="chapter" data-level="2.1" data-path="basics-of-r.html"><a href="basics-of-r.html#r-as-a-calculator"><i class="fa fa-check"></i><b>2.1</b> R, as a Calculator</a><ul>
<li class="chapter" data-level="2.1.1" data-path="basics-of-r.html"><a href="basics-of-r.html#order-of-operations"><i class="fa fa-check"></i><b>2.1.1</b> Order of Operations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basics-of-r.html"><a href="basics-of-r.html#objects"><i class="fa fa-check"></i><b>2.2</b> Objects</a></li>
<li class="chapter" data-level="2.3" data-path="basics-of-r.html"><a href="basics-of-r.html#functions"><i class="fa fa-check"></i><b>2.3</b> Functions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="basics-of-r.html"><a href="basics-of-r.html#object-types"><i class="fa fa-check"></i><b>2.3.1</b> Object Types</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="basics-of-r.html"><a href="basics-of-r.html#packages-1"><i class="fa fa-check"></i><b>2.4</b> Packages</a><ul>
<li class="chapter" data-level="2.4.1" data-path="basics-of-r.html"><a href="basics-of-r.html#installing-packages"><i class="fa fa-check"></i><b>2.4.1</b> Installing Packages</a></li>
<li class="chapter" data-level="2.4.2" data-path="basics-of-r.html"><a href="basics-of-r.html#loading-packages"><i class="fa fa-check"></i><b>2.4.2</b> Loading Packages</a></li>
<li class="chapter" data-level="2.4.3" data-path="basics-of-r.html"><a href="basics-of-r.html#updating-packages"><i class="fa fa-check"></i><b>2.4.3</b> Updating Packages</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basics-of-r.html"><a href="basics-of-r.html#r-help"><i class="fa fa-check"></i><b>2.5</b> R Help</a></li>
<li class="chapter" data-level="2.6" data-path="basics-of-r.html"><a href="basics-of-r.html#setting-a-working-directory"><i class="fa fa-check"></i><b>2.6</b> Setting a Working Directory</a></li>
<li class="chapter" data-level="2.7" data-path="basics-of-r.html"><a href="basics-of-r.html#importing-your-data"><i class="fa fa-check"></i><b>2.7</b> Importing Your Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html"><i class="fa fa-check"></i><b>3</b> Formatting, Describing, and Visualizing Data</a><ul>
<li class="chapter" data-level="3.1" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#factoring"><i class="fa fa-check"></i><b>3.1</b> Factoring</a><ul>
<li class="chapter" data-level="3.1.1" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#coerce-factoring"><i class="fa fa-check"></i><b>3.1.1</b> Coerce Factoring</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#recoding"><i class="fa fa-check"></i><b>3.2</b> Recoding</a><ul>
<li class="chapter" data-level="3.2.1" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#factoring-and-recoding"><i class="fa fa-check"></i><b>3.2.1</b> Factoring and Recoding</a></li>
<li class="chapter" data-level="3.2.2" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#creating-a-dummy-variable"><i class="fa fa-check"></i><b>3.2.2</b> Creating a Dummy Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#part-iii-building-and-sorting-your-data"><i class="fa fa-check"></i><b>3.3</b> Part III: Building and Sorting Your Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#the-tidyverse"><i class="fa fa-check"></i><b>3.3.1</b> The Tidyverse</a></li>
<li class="chapter" data-level="3.3.2" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#other-methods-of-exploring-your-data"><i class="fa fa-check"></i><b>3.3.2</b> Other Methods of Exploring Your Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#working-with-nominal-data"><i class="fa fa-check"></i><b>3.4</b> Working with Nominal Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#finding-the-mode"><i class="fa fa-check"></i><b>3.4.1</b> Finding the Mode</a></li>
<li class="chapter" data-level="3.4.2" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#visualizing-nominal-data"><i class="fa fa-check"></i><b>3.4.2</b> Visualizing Nominal Data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#working-with-ordinal-data"><i class="fa fa-check"></i><b>3.5</b> Working with Ordinal Data</a></li>
<li class="chapter" data-level="3.6" data-path="formatting-describing-and-visualizing-data.html"><a href="formatting-describing-and-visualizing-data.html#working-with-interval-data"><i class="fa fa-check"></i><b>3.6</b> Working with Interval Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><i class="fa fa-check"></i><b>4</b> Visualizing Data, Probability, the Normal Distribution, and Z Scores</a><ul>
<li class="chapter" data-level="4.1" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#histograms-and-density"><i class="fa fa-check"></i><b>4.1</b> Histograms and Density</a><ul>
<li class="chapter" data-level="4.1.1" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#normal-distribution-and-histograms"><i class="fa fa-check"></i><b>4.1.1</b> Normal Distribution and Histograms</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#probability-and-distributions"><i class="fa fa-check"></i><b>4.2</b> Probability and Distributions</a></li>
<li class="chapter" data-level="4.3" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#visualizing-normality"><i class="fa fa-check"></i><b>4.3</b> Visualizing Normality</a></li>
<li class="chapter" data-level="4.4" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#z-scores"><i class="fa fa-check"></i><b>4.4</b> Z-Scores</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html"><i class="fa fa-check"></i><b>5</b> Foundations for Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#testing-for-normality"><i class="fa fa-check"></i><b>5.1</b> Testing for Normality</a><ul>
<li class="chapter" data-level="5.1.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#shapiro-wilk-test"><i class="fa fa-check"></i><b>5.1.1</b> Shapiro-Wilk Test</a></li>
<li class="chapter" data-level="5.1.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#testing-normality"><i class="fa fa-check"></i><b>5.1.2</b> Testing Normality</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#standard-errors"><i class="fa fa-check"></i><b>5.2</b> Standard Errors</a></li>
<li class="chapter" data-level="5.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#more-on-single-sample-t-tests"><i class="fa fa-check"></i><b>5.4</b> More on Single Sample T-tests</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html"><i class="fa fa-check"></i><b>6</b> Inference for Two Populations</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#proportions"><i class="fa fa-check"></i><b>6.1</b> Proportions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#two-populations"><i class="fa fa-check"></i><b>6.1.1</b> Two Populations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#cross-tabulations"><i class="fa fa-check"></i><b>6.2</b> Cross Tabulations</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#other-coefficients"><i class="fa fa-check"></i><b>6.2.1</b> Other Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#independent-t-tests"><i class="fa fa-check"></i><b>6.3</b> Independent t-tests</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#other-independent-sample-tests"><i class="fa fa-check"></i><b>6.3.1</b> Other Independent Sample Tests</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#paired-t-test"><i class="fa fa-check"></i><b>6.4</b> Paired t-test</a></li>
<li class="chapter" data-level="6.5" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#visualizing-differences-in-means"><i class="fa fa-check"></i><b>6.5</b> Visualizing Differences in Means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html"><i class="fa fa-check"></i><b>7</b> Covariance and Correlation</a><ul>
<li class="chapter" data-level="7.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance"><i class="fa fa-check"></i><b>7.1</b> Covariance</a><ul>
<li class="chapter" data-level="7.1.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance-by-hand"><i class="fa fa-check"></i><b>7.1.1</b> Covariance by Hand</a></li>
<li class="chapter" data-level="7.1.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance-in-r"><i class="fa fa-check"></i><b>7.1.2</b> Covariance in R</a></li>
<li class="chapter" data-level="7.1.3" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance-in-class-data-set"><i class="fa fa-check"></i><b>7.1.3</b> Covariance in Class Data Set</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation"><i class="fa fa-check"></i><b>7.2</b> Correlation</a><ul>
<li class="chapter" data-level="7.2.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation-by-hand"><i class="fa fa-check"></i><b>7.2.1</b> Correlation by Hand</a></li>
<li class="chapter" data-level="7.2.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation-tests"><i class="fa fa-check"></i><b>7.2.2</b> Correlation Tests</a></li>
<li class="chapter" data-level="7.2.3" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation-across-groups"><i class="fa fa-check"></i><b>7.2.3</b> Correlation Across Groups</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#visualizing-correlation"><i class="fa fa-check"></i><b>7.3</b> Visualizing Correlation</a><ul>
<li class="chapter" data-level="7.3.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#another-example-political-party"><i class="fa fa-check"></i><b>7.3.1</b> Another Example: Political Party</a></li>
<li class="chapter" data-level="7.3.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#one-more-visualization"><i class="fa fa-check"></i><b>7.3.2</b> One More Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html"><i class="fa fa-check"></i><b>8</b> Bivariate Linear Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#bivariate-linear-regression-by-hand"><i class="fa fa-check"></i><b>8.1</b> Bivariate Linear Regression by Hand</a><ul>
<li class="chapter" data-level="8.1.1" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#calculating-goodness-of-fit"><i class="fa fa-check"></i><b>8.1.1</b> Calculating Goodness of Fit</a></li>
<li class="chapter" data-level="8.1.2" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#checking-our-work"><i class="fa fa-check"></i><b>8.1.2</b> Checking Our Work</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#bivariate-regression-in-r"><i class="fa fa-check"></i><b>8.2</b> Bivariate Regression in R</a></li>
<li class="chapter" data-level="8.3" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#the-residuals"><i class="fa fa-check"></i><b>8.3</b> The Residuals</a></li>
<li class="chapter" data-level="8.4" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#comparing-models"><i class="fa fa-check"></i><b>8.4</b> Comparing Models</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#visualizing-multiple-models"><i class="fa fa-check"></i><b>8.4.1</b> Visualizing Multiple Models</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>8.5</b> Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html"><i class="fa fa-check"></i><b>9</b> Multivariable Linear Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#calculating-least-squared-estimates"><i class="fa fa-check"></i><b>9.1</b> Calculating Least-Squared Estimates</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#matrix-algebra"><i class="fa fa-check"></i><b>9.1.1</b> Matrix Algebra</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#representing-system-of-linear-equations-as-matrices"><i class="fa fa-check"></i><b>9.1.2</b> Representing System of Linear Equations as Matrices</a></li>
<li class="chapter" data-level="9.1.3" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#ols-regression-and-matrices"><i class="fa fa-check"></i><b>9.1.3</b> OLS Regression and Matrices</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#multiple-regression-in-r"><i class="fa fa-check"></i><b>9.2</b> Multiple Regression in R</a></li>
<li class="chapter" data-level="9.3" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#hypothesis-testing-with-multivariable-regression"><i class="fa fa-check"></i><b>9.3</b> Hypothesis Testing with Multivariable Regression</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#visualizing-multivariable-linear-regression"><i class="fa fa-check"></i><b>9.3.1</b> Visualizing Multivariable Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#predicting-with-ols-regression"><i class="fa fa-check"></i><b>9.4</b> Predicting with OLS Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html"><i class="fa fa-check"></i><b>10</b> Categorical Explanatory Variables, Dummy Variables, and Interactions</a><ul>
<li class="chapter" data-level="10.1" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#dummy-variables"><i class="fa fa-check"></i><b>10.1</b> Dummy Variables</a><ul>
<li class="chapter" data-level="10.1.1" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#multiple-dummy-variables"><i class="fa fa-check"></i><b>10.1.1</b> Multiple Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#interactions"><i class="fa fa-check"></i><b>10.2</b> Interactions</a><ul>
<li class="chapter" data-level="10.2.1" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#interactions-with-two-non-binary-variables"><i class="fa fa-check"></i><b>10.2.1</b> Interactions with Two Non-binary Variables</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#releveling-variables"><i class="fa fa-check"></i><b>10.3</b> Releveling Variables</a></li>
<li class="chapter" data-level="10.4" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#interaction-plots"><i class="fa fa-check"></i><b>10.4</b> Interaction Plots</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html"><i class="fa fa-check"></i><b>11</b> Non-linearity, Non-normality, and Multicollinearity</a><ul>
<li class="chapter" data-level="11.1" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#non-linearity"><i class="fa fa-check"></i><b>11.1</b> Non-linearity</a><ul>
<li class="chapter" data-level="11.1.1" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#exploring-non-linearity"><i class="fa fa-check"></i><b>11.1.1</b> Exploring Non-linearity</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#non-normality"><i class="fa fa-check"></i><b>11.2</b> Non-normality</a></li>
<li class="chapter" data-level="11.3" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#multicollinearity"><i class="fa fa-check"></i><b>11.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="11.4" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#standardizing-coefficients"><i class="fa fa-check"></i><b>11.4</b> Standardizing Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html"><i class="fa fa-check"></i><b>12</b> Diagnosing and Addressing Problems in Linear Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#introduction-to-the-data"><i class="fa fa-check"></i><b>12.1</b> Introduction to the Data</a></li>
<li class="chapter" data-level="12.2" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#outliers"><i class="fa fa-check"></i><b>12.2</b> Outliers</a></li>
<li class="chapter" data-level="12.3" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>12.3</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="12.4" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#revisiting-linearity"><i class="fa fa-check"></i><b>12.4</b> Revisiting Linearity</a><ul>
<li class="chapter" data-level="12.4.1" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#normality"><i class="fa fa-check"></i><b>12.4.1</b> Normality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-with-a-binary-dv"><i class="fa fa-check"></i><b>13.1</b> Logistic Regression with a Binary DV</a><ul>
<li class="chapter" data-level="13.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#goodness-of-fit-logit-regression"><i class="fa fa-check"></i><b>13.1.1</b> Goodness of Fit, Logit Regression</a></li>
<li class="chapter" data-level="13.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#percent-correctly-predicted"><i class="fa fa-check"></i><b>13.1.2</b> Percent Correctly Predicted</a></li>
<li class="chapter" data-level="13.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logit-regression-with-groups"><i class="fa fa-check"></i><b>13.1.3</b> Logit Regression with Groups</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="logistic-regression.html"><a href="logistic-regression.html#ordered-logit-and-creating-an-index"><i class="fa fa-check"></i><b>13.2</b> Ordered Logit and Creating an Index</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="statistical-simulations.html"><a href="statistical-simulations.html"><i class="fa fa-check"></i><b>14</b> Statistical Simulations</a><ul>
<li class="chapter" data-level="14.1" data-path="statistical-simulations.html"><a href="statistical-simulations.html#the-basics"><i class="fa fa-check"></i><b>14.1</b> The Basics</a><ul>
<li class="chapter" data-level="14.1.1" data-path="statistical-simulations.html"><a href="statistical-simulations.html#plotting-predictions-with-zelig"><i class="fa fa-check"></i><b>14.1.1</b> Plotting Predictions with Zelig</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="statistical-simulations.html"><a href="statistical-simulations.html#other-models"><i class="fa fa-check"></i><b>14.2</b> Other Models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="statistical-simulations.html"><a href="statistical-simulations.html#ordered-logit"><i class="fa fa-check"></i><b>14.2.1</b> Ordered Logit</a></li>
<li class="chapter" data-level="14.2.2" data-path="statistical-simulations.html"><a href="statistical-simulations.html#another-example"><i class="fa fa-check"></i><b>14.2.2</b> Another Example</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="statistical-simulations.html"><a href="statistical-simulations.html#zelig-with-non-zelig-models"><i class="fa fa-check"></i><b>14.3</b> Zelig with non-Zelig Models:</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html"><i class="fa fa-check"></i><b>15</b> Appendix: Guide to Data Visualization</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#deciding-which-visualization-to-use"><i class="fa fa-check"></i><b>15.1</b> Deciding Which Visualization to Use</a></li>
<li class="chapter" data-level="15.2" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#adding-labels"><i class="fa fa-check"></i><b>15.2</b> Adding Labels</a></li>
<li class="chapter" data-level="15.3" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#scale-and-limits"><i class="fa fa-check"></i><b>15.3</b> Scale and Limits</a></li>
<li class="chapter" data-level="15.4" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#confidence-intervals-1"><i class="fa fa-check"></i><b>15.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="15.5" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#adding-color"><i class="fa fa-check"></i><b>15.5</b> Adding Color</a></li>
<li class="chapter" data-level="15.6" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#creating-a-legend"><i class="fa fa-check"></i><b>15.6</b> Creating a Legend</a></li>
<li class="chapter" data-level="15.7" data-path="appendix-guide-to-data-visualization.html"><a href="appendix-guide-to-data-visualization.html#using-themes"><i class="fa fa-check"></i><b>15.7</b> Using Themes</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-linearity-non-normality-and-multicollinearity" class="section level1">
<h1><span class="header-section-number">11</span> Non-linearity, Non-normality, and Multicollinearity</h1>
<p>This lab focuses on issues that arise with non-linearity, non-normality, and multicollinearity. We begin with non-linearity. The following packages are required for this lab:</p>
<ol style="list-style-type: decimal">
<li>tidyverse</li>
<li>psych</li>
<li>car</li>
<li>stargazer</li>
<li>reshape2</li>
<li>skimr</li>
<li>broom</li>
</ol>
<div id="non-linearity" class="section level2">
<h2><span class="header-section-number">11.1</span> Non-linearity</h2>
<div id="exploring-non-linearity" class="section level3">
<h3><span class="header-section-number">11.1.1</span> Exploring Non-linearity</h3>
<p>A critical assumption of OLS is that the relationship between variables is linear in their functional form, therefore it is imperative to inspect whether a model consist of non-linear relationships between variables of interest. As the book describes, regression diagnostics is largely an art. To demonstrate, suppose you want to examine the relationship between ideology and candidate support in the 2016 presidential election. The survey associated with the class data set asked respondents to indicate on a scale of 1 to 5 the level of support they have for the candidate they voted for in the 2016 presidential election. We will start by exploring data in preparation for a linear regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sub &lt;-<span class="st"> </span>ds <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="st">&quot;vote_cand_spt&quot;</span>, <span class="st">&quot;ideol&quot;</span>,<span class="st">&quot;education&quot;</span>,
                                    <span class="st">&quot;income&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;gender&quot;</span>, <span class="st">&quot;f.gender&quot;</span>,
                                    <span class="st">&quot;glbcc_cert&quot;</span>, <span class="st">&quot;f.party&quot;</span>, <span class="st">&quot;glbcc_risk&quot;</span>,
                                    <span class="st">&quot;cncrn_econ&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()</code></pre></div>
<p>Now examine the candidate support variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psych<span class="op">::</span><span class="kw">describe</span>(sub<span class="op">$</span>vote_cand_spt)</code></pre></div>
<pre><code>##    vars    n mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 1980 3.56 1.15      4    3.65 1.48   1   5     4 -0.45    -0.52
##      se
## X1 0.03</code></pre>
<p>Now a table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(sub<span class="op">$</span>vote_cand_spt)</code></pre></div>
<pre><code>## 
##   1   2   3   4   5 
## 117 209 600 560 494</code></pre>
<p>We can see that the modal value is 3, a response indicating moderate support for the candidate the respondent voted for. We also observe a negative skew indicating more responses in the higher values.</p>
<p>Next build the linear regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>(vote_cand_spt <span class="op">~</span><span class="st"> </span>ideol <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender, <span class="dt">data =</span> sub)
<span class="kw">summary</span>(model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = vote_cand_spt ~ ideol + education + income + age + 
##     gender, data = sub)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8895 -0.6401  0.2183  0.8652  1.8780 
## 
## Coefficients:
##                  Estimate    Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  3.2245955848  0.1628686111  19.799 &lt; 0.0000000000000002 ***
## ideol        0.0215255996  0.0148191732   1.453              0.14651    
## education   -0.0462020061  0.0153075522  -3.018              0.00257 ** 
## income      -0.0000002871  0.0000004401  -0.652              0.51420    
## age          0.0085719712  0.0018812530   4.557           0.00000552 ***
## gender      -0.0601777317  0.0525893860  -1.144              0.25264    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.134 on 1974 degrees of freedom
## Multiple R-squared:  0.02201,    Adjusted R-squared:  0.01953 
## F-statistic: 8.884 on 5 and 1974 DF,  p-value: 0.00000002359</code></pre>
<p>Based on the p-values for the model variables, the ideology variable does not appear to help us understand what candidate the responded supported; however, we should examine the linearity of the variables. One way to do this is to plot the residuals by the values of the independent variables. Residual relationships should constitute a straight line with the residuals spread around a line at zero. To build this visualization we need to:</p>
<ol style="list-style-type: decimal">
<li>Use <code>augment()</code> to predict values and create a data frame containing fitted values and residuals</li>
<li>Melt the data into long form, sorted by independent variables.</li>
<li>Visualize the relationship between the residuals and IVs simultaneously using <code>facet_wrap()</code>.</li>
</ol>
<p>First the residuals and fitted values. Use <code>head()</code> to look at the first five observations in the data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>() -&gt;<span class="st"> </span>m.df
<span class="kw">head</span>(m.df)</code></pre></div>
<pre><code>## # A tibble: 6 x 14
##   .rownames vote_cand_spt ideol education income   age gender .fitted
##   &lt;chr&gt;             &lt;int&gt; &lt;int&gt;     &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;
## 1 1                     5     6         4 160000    50      1    3.49
## 2 2                     2     3         4  45000    58      1    3.53
## 3 3                     4     6         6  40000    52      0    3.51
## 4 5                     4     4         4  55000    40      0    3.45
## 5 7                     3     2         6  40000    60      0    3.49
## 6 8                     4     4         6  50000    67      1    3.53
## # ... with 6 more variables: .se.fit &lt;dbl&gt;, .resid &lt;dbl&gt;, .hat &lt;dbl&gt;,
## #   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;</code></pre>
<p>Now we need to melt the data into rows with unique id-variable combinations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.df <span class="op">%&gt;%</span>
<span class="kw">melt</span>(<span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;income&quot;</span>, <span class="st">&quot;age&quot;</span>,
                                       <span class="st">&quot;gender&quot;</span>, <span class="st">&quot;.fitted&quot;</span>)) -&gt;<span class="st"> </span>m.df
<span class="kw">head</span>(m.df)</code></pre></div>
<pre><code>##   .rownames vote_cand_spt    .se.fit     .resid        .hat   .sigma
## 1         1             5 0.06292522  1.5085812 0.003078242 1.133935
## 2         2             2 0.05491383 -1.5284394 0.002344319 1.133922
## 3         3             4 0.04930230  0.4892061 0.001889677 1.134391
## 4         5             4 0.05455346  0.5470242 0.002313651 1.134378
## 5         7             3 0.05215890 -0.4932672 0.002114999 1.134390
## 6         8             4 0.04636595  0.4667270 0.001671289 1.134396
##         .cooksd .std.resid variable value
## 1 0.00091331373  1.3321858    ideol     6
## 2 0.00071294152 -1.3492255    ideol     3
## 3 0.00005881882  0.4317469    ideol     6
## 4 0.00009012074  0.4828766    ideol     4
## 5 0.00006696004 -0.4353802    ideol     2
## 6 0.00004732953  0.4118630    ideol     4</code></pre>
<p>The next step is to plot the residuals by the values of the independent variables. We’re going to use <code>geom_smooth()</code> to create a loess line that approximates the spread of our data, and we will use <code>geom_hline()</code> to plot a horizontal line at 0. Then we will use <code>facet_wrap()</code> to tell R to create different plots for each independent variable of the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(m.df, <span class="kw">aes</span>(value, .std.resid)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(value, .std.resid), <span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>variable, <span class="dt">scales =</span> <span class="st">&quot;free_x&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_lin4-1.png" width="672" /></p>
<p>We can see there are some potential non-linear relationships; for instance, the ideology variable. The next step is to consider adding an exponent the ideology variable. <strong>Note:</strong> We want to avoid over fitting our model to the data. Therefore it is important to have an understanding of why you are including an exponent.</p>
<p>For example, when thinking about how ideology might influence candidate support, you could imagine this might be an instance when a quadratic model would be appropriate. Think about it: it wouldn’t make sense to theorize that the more conservative an individual, the more enthusiastic they were for the candidate they voted for (regardless of who the candidate was), but it <strong>does</strong> make sense to theorize that the more ideologically extreme an individual is (very liberal or very conservative) the more they supported the candidate they voted for. Perhaps the moderates voting in the 2016 election felt left our or alienated by the polarized political environment, and therefore might have had less support for the candidate they voted for.</p>
<p>With this in mind, let’s build a new model to include ideology as a squared term. <strong>Note:</strong> The syntax to use a polynomial is: <code>poly(var,# of powers)</code>. The <code>poly</code> function creates an independent variable for each of the powers as required to create orthogonal power terms. Let’s construct the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pol &lt;-<span class="st"> </span><span class="kw">lm</span>(vote_cand_spt <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(ideol,<span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender, <span class="dt">data =</span> sub)
<span class="kw">summary</span>(pol)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = vote_cand_spt ~ poly(ideol, 2) + education + income + 
##     age + gender, data = sub)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2697 -0.6101  0.0023  0.7914  2.2046 
## 
## Coefficients:
##                      Estimate    Std. Error t value             Pr(&gt;|t|)
## (Intercept)      3.4716343110  0.1404083490  24.725 &lt; 0.0000000000000002
## poly(ideol, 2)1  1.6130128925  1.1096206250   1.454                0.146
## poly(ideol, 2)2 14.8590944999  1.0888870633  13.646 &lt; 0.0000000000000002
## education       -0.0586338098  0.0146646212  -3.998            0.0000661
## income          -0.0000001029  0.0000004210  -0.245                0.807
## age              0.0071042635  0.0018019703   3.942            0.0000835
## gender          -0.0760264761  0.0502966844  -1.512                0.131
##                    
## (Intercept)     ***
## poly(ideol, 2)1    
## poly(ideol, 2)2 ***
## education       ***
## income             
## age             ***
## gender             
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.084 on 1973 degrees of freedom
## Multiple R-squared:  0.1064, Adjusted R-squared:  0.1036 
## F-statistic: 39.13 on 6 and 1973 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>We see that our squared ideology term is statistically significant, but the coefficient may not provide us an intuitive interpretation. Before visualizing the model, it may be helpful to compare the new model to the previous, via the <code>anova()</code> function, to compare their fit to the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(model, pol)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: vote_cand_spt ~ ideol + education + income + age + gender
## Model 2: vote_cand_spt ~ poly(ideol, 2) + education + income + age + gender
##   Res.Df    RSS Df Sum of Sq      F                Pr(&gt;F)    
## 1   1974 2539.2                                              
## 2   1973 2320.2  1    218.99 186.22 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The significant p-value for the second line implies the second model is a better fit. We can also compare the adjusted <span class="math inline">\(R^2\)</span> values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stargazer</span>(model, pol, <span class="dt">single.row =</span> <span class="ot">TRUE</span>, <span class="dt">type =</span> <span class="st">&quot;text&quot;</span>)</code></pre></div>
<pre><code>## 
## ====================================================================
##                                   Dependent variable:               
##                     ------------------------------------------------
##                                      vote_cand_spt                  
##                               (1)                     (2)           
## --------------------------------------------------------------------
## ideol                    0.022 (0.015)                              
## poly(ideol, 2)1                                  1.613 (1.110)      
## poly(ideol, 2)2                                14.859*** (1.089)    
## education              -0.046*** (0.015)       -0.059*** (0.015)    
## income                -0.00000 (0.00000)       -0.00000 (0.00000)   
## age                    0.009*** (0.002)         0.007*** (0.002)    
## gender                  -0.060 (0.053)           -0.076 (0.050)     
## Constant               3.225*** (0.163)         3.472*** (0.140)    
## --------------------------------------------------------------------
## Observations                 1,980                   1,980          
## R2                           0.022                   0.106          
## Adjusted R2                  0.020                   0.104          
## Residual Std. Error    1.134 (df = 1974)       1.084 (df = 1973)    
## F Statistic         8.884*** (df = 5; 1974) 39.134*** (df = 6; 1973)
## ====================================================================
## Note:                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>The coefficients for the individual variables in the quadratic model are statistically significant, suggesting they help us understand our dependent variable. Let’s move onto the visualization. Since ideology is our variable of interest, we will visualize the relationship between ideology and candidate support while holding the other variables constant at their means.</p>
<p>Start by looking at a scatter plot. To assist we need to jitter the data because ideology is a discrete variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sub, <span class="kw">aes</span>(ideol, vote_cand_spt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">shape =</span> <span class="dv">1</span>) </code></pre></div>
<p><img src="_main_files/figure-html/10_poly5-1.png" width="672" /></p>
<p>Now add the regression line and confidence interval. To do this we will:</p>
<ol style="list-style-type: decimal">
<li>Create predicted values and standard error values of candidate support using the <code>augment()</code> function, while holding all other values constant at their mean.</li>
<li>Generate upper and lower limits of the confidence interval using <code>mutate()</code></li>
<li>Visualize.</li>
</ol>
<p>First: predict. We are going to sequence ideology from 1 to 7 by 0.1 instead of by 1 to produce a smoother line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pol <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">ideol =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">7</span>,.<span class="dv">1</span>),
                               <span class="dt">education =</span> <span class="kw">mean</span>(sub<span class="op">$</span>education),
                               <span class="dt">income =</span> <span class="kw">mean</span>(sub<span class="op">$</span>income),
                               <span class="dt">age =</span> <span class="kw">mean</span>(sub<span class="op">$</span>age),
                               <span class="dt">gender=</span> <span class="kw">mean</span>(sub<span class="op">$</span>gender))) -&gt;<span class="st"> </span>pol.df</code></pre></div>
<p>Now create the upper and lower limits of the confidence interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pol.df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">upper =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit,
         <span class="dt">lower =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit) -&gt;<span class="st"> </span>pol.df</code></pre></div>
<p>The next step is to visualize. Use <code>geom_line()</code> to create the line, and <code>geom_ribbon()</code> to create the confidence interval. For practice, let’s label the axes and add a title.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(pol.df, <span class="kw">aes</span>(ideol, .fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;dodgerblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> lower, <span class="dt">ymax =</span> upper), <span class="dt">fill =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">alpha =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Ideology&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Support for Candidate&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Ideology and Support for Candidate&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="_main_files/figure-html/10_poly9-1.png" width="672" /></p>
<p>This visualization shows the relationship specified in our theory: that the relationship between ideology and candidate support is quadratic, where more ideologically extreme individuals have more support for the candidate they voted for than moderates.</p>
<p>Let’s use an example where a cubed exponent would be appropriate. A very common model we’ve used has been exploring the relationship between ideology and climate change risk. Perhaps the relationship between the two is not best described linearly. Let’s first make a model we’ve looked at many times before:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(glbcc_risk <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>ideol <span class="op">+</span><span class="st"> </span>education, <span class="dt">data =</span> sub)
<span class="kw">summary</span>(lm1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + gender + ideol + education, data = sub)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.0014 -1.6496  0.1836  1.4960  6.8115 
## 
## Coefficients:
##              Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 10.788953   0.348172  30.987 &lt;0.0000000000000002 ***
## age         -0.001105   0.004013  -0.275              0.7832    
## gender      -0.264746   0.111988  -2.364              0.0182 *  
## ideol       -1.062645   0.031745 -33.474 &lt;0.0000000000000002 ***
## education    0.052300   0.031618   1.654              0.0983 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.436 on 1975 degrees of freedom
## Multiple R-squared:  0.3796, Adjusted R-squared:  0.3784 
## F-statistic: 302.1 on 4 and 1975 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Now plot the residuals by the independent variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm1 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">melt</span>(<span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;age&quot;</span>,
                        <span class="st">&quot;gender&quot;</span>, <span class="st">&quot;.fitted&quot;</span>)) <span class="op">%&gt;%</span>
<span class="kw">ggplot</span>(., <span class="kw">aes</span>(value, .std.resid)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(value, .std.resid), <span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>variable, <span class="dt">scales =</span> <span class="st">&quot;free_x&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_cube2-1.png" width="672" /></p>
<p>Looking at the ideology variable, we can see that it is likely not linear. It looks more like the loess line moves above and below the line at zero. This may imply that a cube term might be appropriate. Build a model that cubes ideology:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cubed &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> glbcc_risk <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span><span class="kw">poly</span>(ideol, <span class="dv">3</span>)  <span class="op">+</span><span class="st"> </span>education, <span class="dt">data =</span> sub)
<span class="kw">summary</span>(cubed)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + gender + poly(ideol, 3) + education, 
##     data = sub)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.8042 -1.7562  0.2032  1.5035  7.3212 
## 
## Coefficients:
##                     Estimate   Std. Error t value             Pr(&gt;|t|)    
## (Intercept)       5.71039034   0.31056558  18.387 &lt; 0.0000000000000002 ***
## age              -0.00006485   0.00399923  -0.016               0.9871    
## gender           -0.25936550   0.11136612  -2.329               0.0200 *  
## poly(ideol, 3)1 -83.16325764   2.47050241 -33.662 &lt; 0.0000000000000002 ***
## poly(ideol, 3)2 -12.07796545   2.42949486  -4.971          0.000000722 ***
## poly(ideol, 3)3  -3.53484785   2.42400684  -1.458               0.1449    
## education         0.06113881   0.03146973   1.943               0.0522 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.421 on 1973 degrees of freedom
## Multiple R-squared:  0.3879, Adjusted R-squared:  0.3861 
## F-statistic: 208.4 on 6 and 1973 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Now compare the two models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stargazer</span>(lm1, cubed, <span class="dt">single.row =</span> <span class="ot">TRUE</span>, <span class="dt">type =</span> <span class="st">&quot;text&quot;</span>)</code></pre></div>
<pre><code>## 
## =======================================================================
##                                     Dependent variable:                
##                     ---------------------------------------------------
##                                         glbcc_risk                     
##                                (1)                       (2)           
## -----------------------------------------------------------------------
## age                      -0.001 (0.004)            -0.0001 (0.004)     
## gender                  -0.265** (0.112)          -0.259** (0.111)     
## ideol                   -1.063*** (0.032)                              
## poly(ideol, 3)1                                  -83.163*** (2.471)    
## poly(ideol, 3)2                                  -12.078*** (2.429)    
## poly(ideol, 3)3                                    -3.535 (2.424)      
## education                0.052* (0.032)            0.061* (0.031)      
## Constant                10.789*** (0.348)         5.710*** (0.311)     
## -----------------------------------------------------------------------
## Observations                  1,980                     1,980          
## R2                            0.380                     0.388          
## Adjusted R2                   0.378                     0.386          
## Residual Std. Error     2.436 (df = 1975)         2.421 (df = 1973)    
## F Statistic         302.117*** (df = 4; 1975) 208.430*** (df = 6; 1973)
## =======================================================================
## Note:                                       *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>It actually looks like the cube term does not describe the data very well. The adjusted R squared appears to marginally increase in the cubed model, but that does not tell us much. Let’s use ANOVA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(lm1, cubed)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: glbcc_risk ~ age + gender + ideol + education
## Model 2: glbcc_risk ~ age + gender + poly(ideol, 3) + education
##   Res.Df   RSS Df Sum of Sq      F     Pr(&gt;F)    
## 1   1975 11720                                   
## 2   1973 11562  2    157.55 13.443 0.00000159 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The ANOVA test tells us that our cubed model is a better fit. This is likely due to the square term, which is statistically significant. Nonetheless, let’s visualize this so you have experience seeing cubed lines.</p>
<p>Follow the same steps as last time, predicting values, calculating the confidence interval, and plotting using <code>geom_line()</code> and <code>geom_ribbon()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cubed <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">ideol =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">7</span>,.<span class="dv">1</span>),
                               <span class="dt">education =</span> <span class="kw">mean</span>(sub<span class="op">$</span>education),
                               <span class="dt">age =</span> <span class="kw">mean</span>(sub<span class="op">$</span>age),
                               <span class="dt">gender=</span> <span class="kw">mean</span>(sub<span class="op">$</span>gender))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">upper =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit,
         <span class="dt">lower =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit) -&gt;<span class="st"> </span>cube.df

<span class="kw">ggplot</span>(cube.df, <span class="kw">aes</span>(ideol, .fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;dodgerblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> lower, <span class="dt">ymax =</span> upper), <span class="dt">fill =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">alpha =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Ideology&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Climate Change Risk&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Ideology and Climate Change Risk&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="_main_files/figure-html/10_cube6-1.png" width="672" /></p>
</div>
</div>
<div id="non-normality" class="section level2">
<h2><span class="header-section-number">11.2</span> Non-normality</h2>
<p>This section will go over the problem of non-normality and how to deal with it. One of the key assumptions of OLS is that the residuals of the model are normally distributed. It is also important to make sure your variables are not skewed too far either negatively or positively. Let’s cover how to examine whether residuals are normally distributed, as well as how to handle greatly-skewed variables.</p>
<p>To begin, suppose you want to examine how different independent variables are related to a vote for Donald Trump in the 2016 presidential election. We need to clean up the class data set variable on 2016 presidential votes. Let’s look at it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(ds<span class="op">$</span>vote_cand)</code></pre></div>
<pre><code>## 
##    0    1    2    3    4 
## 1272  722  125    7   39</code></pre>
<p>The codebook for the class data set tells us that a response of 0 indicates a vote for Trump, 1 is for Clinton, 2 for Gary Johnson, 3 for Jill Stein, and 4 for a different candidate. Change the variable so that it only includes Trump and Clinton. To transform this variable into a binary indicator of a vote for Trump or Clinton, we need to recode the variable so that responses of 1 equals 0, a response of 0 equals 1, with all else an NA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ds<span class="op">$</span>v.trump &lt;-<span class="st"> </span>car<span class="op">::</span><span class="kw">recode</span>(ds<span class="op">$</span>vote_cand, <span class="st">&quot;0 = 1;1 = 0;else = NA;NA = NA&quot;</span>)
<span class="kw">table</span>(ds<span class="op">$</span>v.trump)</code></pre></div>
<pre><code>## 
##    0    1 
##  722 1272</code></pre>
<p>Now pull the variables into a new data set and remove missing observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.ds &lt;-<span class="st"> </span>ds <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="st">&quot;income&quot;</span>, <span class="st">&quot;gender&quot;</span>, <span class="st">&quot;ideol&quot;</span>,
                                       <span class="st">&quot;v.trump&quot;</span>, <span class="st">&quot;education&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()</code></pre></div>
<p>Now review independent variables that are not binary:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psych<span class="op">::</span><span class="kw">describe</span>(new.ds<span class="op">$</span>income)</code></pre></div>
<pre><code>##    vars    n     mean       sd median  trimmed     mad   min    max  range
## X1    1 1819 72889.47 58269.83  60000 64601.34 41512.8 10000 900000 890000
##    skew kurtosis      se
## X1 4.13    37.96 1366.24</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psych<span class="op">::</span><span class="kw">describe</span>(new.ds<span class="op">$</span>education)</code></pre></div>
<pre><code>##    vars    n mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 1819 5.18 1.78      6    5.28 1.48   1   8     7 -0.45    -0.79
##      se
## X1 0.04</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psych<span class="op">::</span><span class="kw">describe</span>(new.ds<span class="op">$</span>ideol)</code></pre></div>
<pre><code>##    vars    n mean  sd median trimmed  mad min max range skew kurtosis   se
## X1    1 1819 4.69 1.8      5    4.81 1.48   1   7     6 -0.5    -0.89 0.04</code></pre>
<p>Look at the density distributions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.ds <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">melt</span>(<span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;income&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;ideol&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(., <span class="kw">aes</span>(value)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">adjust =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>variable, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_non5-1.png" width="672" /></p>
<p>It is clear that the income variable has a large positive skew. Education and ideology are slightly skewed. One way to fix a skewed variable is to transform it, often by a log. Let’s try that:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.ds<span class="op">$</span>log.inc &lt;-<span class="st"> </span><span class="kw">log</span>(new.ds<span class="op">$</span>income)</code></pre></div>
<p>Now review it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psych<span class="op">::</span><span class="kw">describe</span>(new.ds<span class="op">$</span>log.inc)</code></pre></div>
<pre><code>##    vars    n  mean  sd median trimmed  mad  min   max range  skew kurtosis
## X1    1 1819 10.96 0.7     11   10.97 0.74 9.21 13.71   4.5 -0.12    -0.03
##      se
## X1 0.02</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(new.ds, <span class="kw">aes</span>(log.inc))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">adjust =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_non8-1.png" width="672" /></p>
<p>The skew appears to be reduced. Transforming a variable does not really change how you use it in a model, but it does change the interpretation. Now the variable is ordered in logs. So a mean of 10.96 indicates 10.96 natural logged income. Now let’s build the model, using the log of income instead of the income variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm.trump &lt;-<span class="st"> </span><span class="kw">lm</span>(v.trump <span class="op">~</span><span class="st">  </span>log.inc <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>ideol, <span class="dt">data =</span> new.ds)
<span class="kw">summary</span>(lm.trump)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = v.trump ~ log.inc + education + gender + ideol, 
##     data = new.ds)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.11760 -0.12826  0.05173  0.16066  1.09163 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -0.273694   0.129610  -2.112               0.0349 *  
## log.inc      0.015387   0.012458   1.235               0.2169    
## education   -0.024763   0.004906  -5.048          0.000000492 ***
## gender       0.015522   0.016451   0.944               0.3455    
## ideol        0.184062   0.004517  40.748 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3389 on 1814 degrees of freedom
## Multiple R-squared:  0.5044, Adjusted R-squared:  0.5033 
## F-statistic: 461.6 on 4 and 1814 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Regarding the ideology variable, there really should not be any surprises. More conservative individuals voted for Trump. Education is significant, but the coefficient is rather small. Our model suggests that education helps us explain voting, with more education tending to go with voting for Clinton. Our log.income variable does not help us to explain voting.</p>
<p>Let’s now look at the normality of the residuals. First assign the residuals to our data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.ds<span class="op">$</span>res &lt;-<span class="st"> </span><span class="kw">residuals</span>(lm.trump)</code></pre></div>
<p>Now make a density plot of the residuals, but also include a normal curve that has the mean and standard deviation of the residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(new.ds, <span class="kw">aes</span>(res))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">adjust =</span> <span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(new.ds<span class="op">$</span>res), <span class="dt">sd =</span> <span class="kw">sd</span>(new.ds<span class="op">$</span>res)), 
                <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_non11-1.png" width="672" /></p>
<p>The eye test indicates that we might have an issue with non-normality of the residuals. Let’s run the Shapiro-Wilk test as well:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(lm.trump<span class="op">$</span>residuals)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  lm.trump$residuals
## W = 0.97524, p-value &lt; 0.00000000000000022</code></pre>
<p>Recall that the Shapiro-Wilk test tests against the null hypothesis that data are normally distributed. Our test result indicates that the residuals might not be normal, which is corroborated by the visualization. In a future lab we will go over one way to correct this, robust estimators.</p>
</div>
<div id="multicollinearity" class="section level2">
<h2><span class="header-section-number">11.3</span> Multicollinearity</h2>
<p>Multicollinearity occurs when one independent variable of a model can be predicted with a high degree of accuracy by another independent variable. Even though perfect multicollinearity is very rare, checking for multicollinearity is an important process. The first way to explore potential multicollinearity is to check the collinearity between independent variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.ds <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(ideol, log.inc, education) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cor</span>()</code></pre></div>
<pre><code>##                  ideol     log.inc  education
## ideol      1.000000000 0.009092405 -0.1700741
## log.inc    0.009092405 1.000000000  0.3732611
## education -0.170074114 0.373261119  1.0000000</code></pre>
<p>There does not appear to be any extremely highly-correlated variables. We should find the variance inflation factor, which measures the increase in variance of the other coefficients due to the inclusion of a particular variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(lm.trump)</code></pre></div>
<pre><code>##   log.inc education    gender     ideol 
##  1.193055  1.206849  1.039107  1.043909</code></pre>
<p>Generally speaking, you do not want to have a value greater than 5. This model does not appear to have an issue with multicollinearity.</p>
<p>Now let’s use an example that combines everything we’ve gone over so far. Let’s examine the relationship between square footage of the respondent’s home and income, age, and education. Start by selecting the data and removing missing observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>ds <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="st">&quot;footage&quot;</span>, <span class="st">&quot;income&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;education&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">na.omit</span>()</code></pre></div>
<p>Like earlier, we should do a log-transformation of income:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d<span class="op">$</span>log.inc &lt;-<span class="st"> </span><span class="kw">log</span>(d<span class="op">$</span>income)</code></pre></div>
<p>Now build the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod &lt;-<span class="st"> </span><span class="kw">lm</span>(footage <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>log.inc, <span class="dt">data =</span> d)
<span class="kw">summary</span>(mod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = footage ~ age + education + log.inc, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1951.5  -526.0  -176.6   296.6 18199.8 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -4598.212    403.704 -11.390 &lt; 0.0000000000000002 ***
## age             9.633      1.695   5.683          0.000000015 ***
## education      32.968     14.126   2.334               0.0197 *  
## log.inc       536.881     36.366  14.763 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1111 on 2232 degrees of freedom
## Multiple R-squared:  0.1186, Adjusted R-squared:  0.1174 
## F-statistic: 100.1 on 3 and 2232 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Check for multicollinearity:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(age, education, log.inc) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cor</span>()</code></pre></div>
<pre><code>##                   age   education    log.inc
## age        1.00000000 -0.04921609 -0.1327773
## education -0.04921609  1.00000000  0.3720851
## log.inc   -0.13277728  0.37208510  1.0000000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(mod)</code></pre></div>
<pre><code>##       age education   log.inc 
##  1.017946  1.160695  1.178663</code></pre>
<p>Taking all this into account, there does not appear to be a problem with multicollinearity.</p>
<p>Now let’s examine the linearity of the variables. Recall the plot we made earlier that plots the independent variables by the residuals. Let’s do that again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">melt</span>(<span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;education&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;log.inc&quot;</span>, <span class="st">&quot;.fitted&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(., <span class="kw">aes</span>(value, .std.resid)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> loess, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>variable, <span class="dt">scales =</span> <span class="st">&quot;free_x&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_ex7-1.png" width="672" /></p>
<p>There does not appear to be an issue with non-linearity either, so we have no reason to include exponents in the model.</p>
<p>The next step is to check for non-normality of the residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d<span class="op">$</span>res &lt;-<span class="st"> </span><span class="kw">residuals</span>(mod)
<span class="kw">ggplot</span>(d, <span class="kw">aes</span>(res))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(d<span class="op">$</span>res), <span class="dt">sd =</span> <span class="kw">sd</span>(d<span class="op">$</span>res)), 
                <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/10_ex8-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(mod<span class="op">$</span>residuals)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  mod$residuals
## W = 0.56711, p-value &lt; 0.00000000000000022</code></pre>
<p>Our results and visualization indicate that there could be a problem with non-normality.</p>
<p>Let’s take a look at the results of the model again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = footage ~ age + education + log.inc, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1951.5  -526.0  -176.6   296.6 18199.8 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -4598.212    403.704 -11.390 &lt; 0.0000000000000002 ***
## age             9.633      1.695   5.683          0.000000015 ***
## education      32.968     14.126   2.334               0.0197 *  
## log.inc       536.881     36.366  14.763 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1111 on 2232 degrees of freedom
## Multiple R-squared:  0.1186, Adjusted R-squared:  0.1174 
## F-statistic: 100.1 on 3 and 2232 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>To interpret this model, we would say that a one unit increase in age corresponds with a 9.633 unit increase in square footage of home. Looking at education, we would say that a one unit increase in <code>log income</code> corresponds with a 536.9 unit increase in home square footage. Practically speaking though, how large is the difference between the age increase and the log income increase? We can see it is almost a 530 square foot difference, but when thinking about the overall distribution of the square footage variable, is that a lot?</p>
</div>
<div id="standardizing-coefficients" class="section level2">
<h2><span class="header-section-number">11.4</span> Standardizing Coefficients</h2>
<p>If you want to compare coefficients of different scales, you need to standardize them. There are three options when standardizing:</p>
<ol style="list-style-type: decimal">
<li>Standardize the dependent variable.</li>
<li>Standardize the independent variables.</li>
<li>Standardize all the variables.</li>
</ol>
<p>Standardizing a variable refers to scaling it in standard deviations. This allows us to compare variables that were originally measured in different units. Let’s use our previously developed model, but this time we will standardize the dependent variable only. Use the <code>scale()</code> function on the footage variable to standardize it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d<span class="op">$</span>z.footage &lt;-<span class="st"> </span><span class="kw">scale</span>(d<span class="op">$</span>footage)</code></pre></div>
<p>Now build the model and look at the results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z.mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(z.footage <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>log.inc, <span class="dt">data =</span> d)
<span class="kw">summary</span>(z.mod1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = z.footage ~ age + education + log.inc, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6500 -0.4447 -0.1493  0.2508 15.3881 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -5.595500   0.341336 -16.393 &lt; 0.0000000000000002 ***
## age          0.008145   0.001433   5.683          0.000000015 ***
## education    0.027875   0.011944   2.334               0.0197 *  
## log.inc      0.453940   0.030748  14.763 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9395 on 2232 degrees of freedom
## Multiple R-squared:  0.1186, Adjusted R-squared:  0.1174 
## F-statistic: 100.1 on 3 and 2232 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Since we only standardized the dependent variable, we would interpret this as saying that a one unit increase in age corresponds with a .008 standard deviation increase in square footage. For log income, we would say that a one unit increase in log income corresponds with a .45 standard deviation increase in square footage.</p>
<p>Now let’s standardize the independent variables only:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d<span class="op">$</span>z.age &lt;-<span class="st"> </span><span class="kw">scale</span>(d<span class="op">$</span>age)
d<span class="op">$</span>z.log.income &lt;-<span class="st"> </span><span class="kw">scale</span>(d<span class="op">$</span>log.inc)
d<span class="op">$</span>z.education &lt;-<span class="st"> </span><span class="kw">scale</span>(d<span class="op">$</span>education)</code></pre></div>
<p>Next build the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z.mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(footage <span class="op">~</span><span class="st"> </span>z.age <span class="op">+</span><span class="st"> </span>z.log.income <span class="op">+</span><span class="st"> </span>z.education, <span class="dt">data=</span>d)
<span class="kw">summary</span>(z.mod2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = footage ~ z.age + z.log.income + z.education, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1951.5  -526.0  -176.6   296.6 18199.8 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)   2019.67      23.50  85.951 &lt; 0.0000000000000002 ***
## z.age          134.76      23.71   5.683          0.000000015 ***
## z.log.income   376.71      25.52  14.763 &lt; 0.0000000000000002 ***
## z.education     59.10      25.32   2.334               0.0197 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1111 on 2232 degrees of freedom
## Multiple R-squared:  0.1186, Adjusted R-squared:  0.1174 
## F-statistic: 100.1 on 3 and 2232 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Now we would say that a one standard deviation increase in age corresponds with a 134.8 unit increase in square footage, and a one standard deviation increase in log income corresponds with a 376.71 unit increase in square footage. Comparing the coefficients here is rather simple and intuitive. Of course, we next need to standardize all the variables and interpret those.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z.mod3 &lt;-<span class="st"> </span><span class="kw">lm</span>(z.footage <span class="op">~</span><span class="st"> </span>z.log.income <span class="op">+</span><span class="st"> </span>z.education <span class="op">+</span><span class="st"> </span>z.age, <span class="dt">data =</span> d)
<span class="kw">summary</span>(z.mod3)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = z.footage ~ z.log.income + z.education + z.age, 
##     data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6500 -0.4447 -0.1493  0.2508 15.3881 
## 
## Coefficients:
##                           Estimate            Std. Error t value
## (Intercept)  0.0000000000000004024 0.0198677514270765854   0.000
## z.log.income 0.3185111526573831675 0.0215744999251085111  14.763
## z.education  0.0499665777182138754 0.0214094220526226224   2.334
## z.age        0.1139389459055853149 0.0200497182548632288   5.683
##                          Pr(&gt;|t|)    
## (Intercept)                1.0000    
## z.log.income &lt; 0.0000000000000002 ***
## z.education                0.0197 *  
## z.age                 0.000000015 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9395 on 2232 degrees of freedom
## Multiple R-squared:  0.1186, Adjusted R-squared:  0.1174 
## F-statistic: 100.1 on 3 and 2232 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Being careful to interpret this correctly, we would say that a one standard deviation change in log income corresponds with a .32 standard deviation increase in square footage.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="categorical-explanatory-variables-dummy-variables-and-interactions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="diagnosing-and-addressing-problems-in-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
