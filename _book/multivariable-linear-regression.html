<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration.</title>
  <meta name="description" content="Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration." />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration." />
  
  
  

<meta name="author" content="Alex Davis">
<meta name="author" content="Cody Adams">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bivariate-linear-regression.html">
<link rel="next" href="categorical-explanatory-variables-dummy-variables-and-interactions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i><b>1.1</b> Acknowledgments</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#requirements"><i class="fa fa-check"></i><b>1.2</b> Requirements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html"><i class="fa fa-check"></i><b>2</b> Introduction and Basics</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#r-as-a-calculator"><i class="fa fa-check"></i><b>2.1</b> R, as a Calculator</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#order-of-operations"><i class="fa fa-check"></i><b>2.1.1</b> Order of Operations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#objects"><i class="fa fa-check"></i><b>2.2</b> Objects</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#functions"><i class="fa fa-check"></i><b>2.3</b> Functions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#object-types"><i class="fa fa-check"></i><b>2.3.1</b> Object Types</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#packages"><i class="fa fa-check"></i><b>2.4</b> Packages</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#installing-packages"><i class="fa fa-check"></i><b>2.4.1</b> Installing Packages</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#loading-packages"><i class="fa fa-check"></i><b>2.4.2</b> Loading Packages</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#updating-packages"><i class="fa fa-check"></i><b>2.4.3</b> Updating Packages</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#r-help"><i class="fa fa-check"></i><b>2.5</b> R Help</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#setting-a-working-directory"><i class="fa fa-check"></i><b>2.6</b> Setting a Working Directory</a></li>
<li class="chapter" data-level="2.7" data-path="introduction-and-basics.html"><a href="introduction-and-basics.html#importing-your-data"><i class="fa fa-check"></i><b>2.7</b> Importing Your Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html"><i class="fa fa-check"></i><b>3</b> Describing and Visualizing Data’</a><ul>
<li class="chapter" data-level="3.1" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#factoring"><i class="fa fa-check"></i><b>3.1</b> Factoring</a><ul>
<li class="chapter" data-level="3.1.1" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#coerce-factoring"><i class="fa fa-check"></i><b>3.1.1</b> Coerce Factoring</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#recoding"><i class="fa fa-check"></i><b>3.2</b> Recoding</a><ul>
<li class="chapter" data-level="3.2.1" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#factoring-and-recoding"><i class="fa fa-check"></i><b>3.2.1</b> Factoring and Recoding</a></li>
<li class="chapter" data-level="3.2.2" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#creating-a-dummy-variable"><i class="fa fa-check"></i><b>3.2.2</b> Creating a Dummy Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#building-and-sorting-your-data"><i class="fa fa-check"></i><b>3.3</b> Building and Sorting Your Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#the-tidyverse"><i class="fa fa-check"></i><b>3.3.1</b> The Tidyverse</a></li>
<li class="chapter" data-level="3.3.2" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#other-methods-of-exploring-your-data"><i class="fa fa-check"></i><b>3.3.2</b> Other Methods of Exploring Your Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#working-with-nominal-data"><i class="fa fa-check"></i><b>3.4</b> Working with Nominal Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#finding-the-mode"><i class="fa fa-check"></i><b>3.4.1</b> Finding the Mode</a></li>
<li class="chapter" data-level="3.4.2" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#visualizing-nominal-data"><i class="fa fa-check"></i><b>3.4.2</b> Visualizing Nominal Data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#working-with-ordinal-data"><i class="fa fa-check"></i><b>3.5</b> Working with Ordinal Data</a></li>
<li class="chapter" data-level="3.6" data-path="describing-and-visualizing-data.html"><a href="describing-and-visualizing-data.html#working-with-interval-data"><i class="fa fa-check"></i><b>3.6</b> Working with Interval Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><i class="fa fa-check"></i><b>4</b> Visualizing Data, Probability, the Normal Distribution, and Z Scores</a><ul>
<li class="chapter" data-level="4.1" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#histograms-and-density"><i class="fa fa-check"></i><b>4.1</b> Histograms and Density</a><ul>
<li class="chapter" data-level="4.1.1" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#normal-distribution-and-histograms"><i class="fa fa-check"></i><b>4.1.1</b> Normal Distribution and Histograms</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#probability-and-distributions"><i class="fa fa-check"></i><b>4.2</b> Probability and Distributions</a></li>
<li class="chapter" data-level="4.3" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#visualizing-normality"><i class="fa fa-check"></i><b>4.3</b> Visualizing Normality</a></li>
<li class="chapter" data-level="4.4" data-path="visualizing-data-probability-the-normal-distribution-and-z-scores.html"><a href="visualizing-data-probability-the-normal-distribution-and-z-scores.html#z-scores"><i class="fa fa-check"></i><b>4.4</b> Z-Scores</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html"><i class="fa fa-check"></i><b>5</b> Foundations for Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#testing-for-normality"><i class="fa fa-check"></i><b>5.1</b> Testing for Normality</a><ul>
<li class="chapter" data-level="5.1.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#shapiro-wilk-test"><i class="fa fa-check"></i><b>5.1.1</b> Shapiro-Wilk Test</a></li>
<li class="chapter" data-level="5.1.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#testing-normality"><i class="fa fa-check"></i><b>5.1.2</b> Testing Normality</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#standard-errors"><i class="fa fa-check"></i><b>5.2</b> Standard Errors</a></li>
<li class="chapter" data-level="5.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#more-on-single-sample-t-tests"><i class="fa fa-check"></i><b>5.4</b> More on Single Sample T-tests</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html"><i class="fa fa-check"></i><b>6</b> Inference for Two Populations</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#proportions"><i class="fa fa-check"></i><b>6.1</b> Proportions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#two-populations"><i class="fa fa-check"></i><b>6.1.1</b> Two Populations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#cross-tabulations"><i class="fa fa-check"></i><b>6.2</b> Cross Tabulations</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#other-coefficients"><i class="fa fa-check"></i><b>6.2.1</b> Other Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#independent-t-tests"><i class="fa fa-check"></i><b>6.3</b> Independent t-tests</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#other-independent-sample-tests"><i class="fa fa-check"></i><b>6.3.1</b> Other Independent Sample Tests</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#paired-t-test"><i class="fa fa-check"></i><b>6.4</b> Paired t-test</a></li>
<li class="chapter" data-level="6.5" data-path="inference-for-two-populations.html"><a href="inference-for-two-populations.html#visualizing-differences-in-means"><i class="fa fa-check"></i><b>6.5</b> Visualizing Differences in Means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html"><i class="fa fa-check"></i><b>7</b> Covariance and Correlation</a><ul>
<li class="chapter" data-level="7.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance"><i class="fa fa-check"></i><b>7.1</b> Covariance</a><ul>
<li class="chapter" data-level="7.1.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance-by-hand"><i class="fa fa-check"></i><b>7.1.1</b> Covariance by Hand</a></li>
<li class="chapter" data-level="7.1.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance-in-r"><i class="fa fa-check"></i><b>7.1.2</b> Covariance in R</a></li>
<li class="chapter" data-level="7.1.3" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#covariance-in-class-data-set"><i class="fa fa-check"></i><b>7.1.3</b> Covariance in Class Data Set</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation"><i class="fa fa-check"></i><b>7.2</b> Correlation</a><ul>
<li class="chapter" data-level="7.2.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation-by-hand"><i class="fa fa-check"></i><b>7.2.1</b> Correlation by Hand</a></li>
<li class="chapter" data-level="7.2.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation-tests"><i class="fa fa-check"></i><b>7.2.2</b> Correlation Tests</a></li>
<li class="chapter" data-level="7.2.3" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#correlation-across-groups"><i class="fa fa-check"></i><b>7.2.3</b> Correlation Across Groups</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#visualizing-correlation"><i class="fa fa-check"></i><b>7.3</b> Visualizing Correlation</a><ul>
<li class="chapter" data-level="7.3.1" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#another-example-political-party"><i class="fa fa-check"></i><b>7.3.1</b> Another Example: Political Party</a></li>
<li class="chapter" data-level="7.3.2" data-path="covariance-and-correlation.html"><a href="covariance-and-correlation.html#one-more-visualization"><i class="fa fa-check"></i><b>7.3.2</b> One More Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html"><i class="fa fa-check"></i><b>8</b> Bivariate Linear Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#bivariate-linear-regression-by-hand"><i class="fa fa-check"></i><b>8.1</b> Bivariate Linear Regression by Hand</a><ul>
<li class="chapter" data-level="8.1.1" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#calculating-goodness-of-fit"><i class="fa fa-check"></i><b>8.1.1</b> Calculating Goodness of Fit</a></li>
<li class="chapter" data-level="8.1.2" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#checking-our-work"><i class="fa fa-check"></i><b>8.1.2</b> Checking Our Work</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#bivariate-regression-in-r"><i class="fa fa-check"></i><b>8.2</b> Bivariate Regression in R</a></li>
<li class="chapter" data-level="8.3" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#the-residuals"><i class="fa fa-check"></i><b>8.3</b> The Residuals</a></li>
<li class="chapter" data-level="8.4" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#comparing-models"><i class="fa fa-check"></i><b>8.4</b> Comparing Models</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#visualizing-multiple-models"><i class="fa fa-check"></i><b>8.4.1</b> Visualizing Multiple Models</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bivariate-linear-regression.html"><a href="bivariate-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>8.5</b> Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html"><i class="fa fa-check"></i><b>9</b> Multivariable Linear Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#part-i-calculating-least-squared-estimates"><i class="fa fa-check"></i><b>9.1</b> Part I: Calculating Least-Squared Estimates</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#matrix-algebra"><i class="fa fa-check"></i><b>9.1.1</b> Matrix Algebra</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#representing-system-of-linear-equations-as-matrices"><i class="fa fa-check"></i><b>9.1.2</b> Representing System of Linear Equations as Matrices</a></li>
<li class="chapter" data-level="9.1.3" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#ols-regression-and-matrices"><i class="fa fa-check"></i><b>9.1.3</b> OLS Regression and Matrices</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#part-ii-multiple-regression-in-r"><i class="fa fa-check"></i><b>9.2</b> Part II: Multiple Regression in R</a></li>
<li class="chapter" data-level="9.3" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#part-iii-hypothesis-testing-with-multivariable-regression"><i class="fa fa-check"></i><b>9.3</b> Part III: Hypothesis Testing with Multivariable Regression</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#visualizing-multivariable-linear-regression"><i class="fa fa-check"></i><b>9.3.1</b> Visualizing Multivariable Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="multivariable-linear-regression.html"><a href="multivariable-linear-regression.html#part-iv-predicting-with-ols-regression"><i class="fa fa-check"></i><b>9.4</b> Part IV: Predicting with OLS Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html"><i class="fa fa-check"></i><b>10</b> Categorical Explanatory Variables, Dummy Variables, and Interactions</a><ul>
<li class="chapter" data-level="10.1" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#dummy-variables"><i class="fa fa-check"></i><b>10.1</b> Dummy Variables</a><ul>
<li class="chapter" data-level="10.1.1" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#multiple-dummy-variables"><i class="fa fa-check"></i><b>10.1.1</b> Multiple Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#interactions"><i class="fa fa-check"></i><b>10.2</b> Interactions</a><ul>
<li class="chapter" data-level="10.2.1" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#interactions-with-two-non-binary-variables"><i class="fa fa-check"></i><b>10.2.1</b> Interactions with Two Non-binary Variables</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#releveling-variables"><i class="fa fa-check"></i><b>10.3</b> Releveling Variables</a></li>
<li class="chapter" data-level="10.4" data-path="categorical-explanatory-variables-dummy-variables-and-interactions.html"><a href="categorical-explanatory-variables-dummy-variables-and-interactions.html#interaction-plots"><i class="fa fa-check"></i><b>10.4</b> Interaction Plots</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html"><i class="fa fa-check"></i><b>11</b> Non-linearity, Non-normality, and Multicollinearity</a><ul>
<li class="chapter" data-level="11.1" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#non-linearity"><i class="fa fa-check"></i><b>11.1</b> Non-linearity</a><ul>
<li class="chapter" data-level="11.1.1" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#exploring-non-linearity"><i class="fa fa-check"></i><b>11.1.1</b> Exploring Non-linearity</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#non-normality"><i class="fa fa-check"></i><b>11.2</b> Non-normality</a></li>
<li class="chapter" data-level="11.3" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#multicollinearity"><i class="fa fa-check"></i><b>11.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="11.4" data-path="non-linearity-non-normality-and-multicollinearity.html"><a href="non-linearity-non-normality-and-multicollinearity.html#standardizing-coefficients"><i class="fa fa-check"></i><b>11.4</b> Standardizing Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html"><i class="fa fa-check"></i><b>12</b> Diagnosing and Addressing Problems in Linear Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#introduction-to-the-data"><i class="fa fa-check"></i><b>12.1</b> Introduction to the Data</a></li>
<li class="chapter" data-level="12.2" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#outliers"><i class="fa fa-check"></i><b>12.2</b> Outliers</a></li>
<li class="chapter" data-level="12.3" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>12.3</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="12.4" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#revisiting-linearity"><i class="fa fa-check"></i><b>12.4</b> Revisiting Linearity</a><ul>
<li class="chapter" data-level="12.4.1" data-path="diagnosing-and-addressing-problems-in-linear-regression.html"><a href="diagnosing-and-addressing-problems-in-linear-regression.html#normality"><i class="fa fa-check"></i><b>12.4.1</b> Normality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-with-a-binary-dv"><i class="fa fa-check"></i><b>13.1</b> Logistic Regression with a Binary DV</a><ul>
<li class="chapter" data-level="13.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#goodness-of-fit-logit-regression"><i class="fa fa-check"></i><b>13.1.1</b> Goodness of Fit, Logit Regression</a></li>
<li class="chapter" data-level="13.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#percent-correctly-predicted"><i class="fa fa-check"></i><b>13.1.2</b> Percent Correctly Predicted</a></li>
<li class="chapter" data-level="13.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logit-regression-with-groups"><i class="fa fa-check"></i><b>13.1.3</b> Logit Regression with Groups</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="logistic-regression.html"><a href="logistic-regression.html#ordered-logit-and-creating-an-index"><i class="fa fa-check"></i><b>13.2</b> Ordered Logit and Creating an Index</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="statistical-simulations.html"><a href="statistical-simulations.html"><i class="fa fa-check"></i><b>14</b> Statistical Simulations</a><ul>
<li class="chapter" data-level="14.1" data-path="statistical-simulations.html"><a href="statistical-simulations.html#the-basics"><i class="fa fa-check"></i><b>14.1</b> The Basics</a><ul>
<li class="chapter" data-level="14.1.1" data-path="statistical-simulations.html"><a href="statistical-simulations.html#plotting-predictions-with-zelig"><i class="fa fa-check"></i><b>14.1.1</b> Plotting Predictions with Zelig</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="statistical-simulations.html"><a href="statistical-simulations.html#other-models"><i class="fa fa-check"></i><b>14.2</b> Other Models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="statistical-simulations.html"><a href="statistical-simulations.html#ordered-logit"><i class="fa fa-check"></i><b>14.2.1</b> Ordered Logit</a></li>
<li class="chapter" data-level="14.2.2" data-path="statistical-simulations.html"><a href="statistical-simulations.html#another-example"><i class="fa fa-check"></i><b>14.2.2</b> Another Example</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="statistical-simulations.html"><a href="statistical-simulations.html#zelig-with-non-zelig-models"><i class="fa fa-check"></i><b>14.3</b> Zelig with non-Zelig Models:</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariable-linear-regression" class="section level1">
<h1><span class="header-section-number">9</span> Multivariable Linear Regression</h1>
<p>This lab covers the basics of multivariable linear regression. We begin by reviewing linear algebra to perform ordinary least squares (OLS) regression in matrix form. Then we cover an introduction to multiple linear regression and visualizations with R. The following packages are required for this lab:</p>
<ol style="list-style-type: decimal">
<li>tidyverse</li>
<li>psych</li>
<li>car</li>
<li>stargazer</li>
<li>reshape2</li>
<li>broom</li>
<li>skimr</li>
</ol>
<div id="part-i-calculating-least-squared-estimates" class="section level2">
<h2><span class="header-section-number">9.1</span> Part I: Calculating Least-Squared Estimates</h2>
<p>The previous lab introduced the estimated bivariate linear regression model as follows:</p>
<p><span class="math display">\[\hat{y}=\hat{\alpha}+\hat{\beta} x\]</span></p>
<p>Where <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are solved via the following formulas:</p>
<p><span class="math display">\[\hat{\alpha}=\bar{y} - \hat{\beta}\bar{x}\]</span></p>
<p><span class="math display">\[\hat{\beta}=\frac{cov(x,y)}{var(x)}\]</span></p>
<p>In this lab we use matrix algebra to calculate the least-squared estimates. This proves useful for multivariable linear regression models where the methods introduced for bivariate regression models become more complex and computationally cumbersome to express as equations.</p>
<div id="matrix-algebra" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Matrix Algebra</h3>
<p>For multivariable regression analysis, the formulas for calculating coefficients are more easily found using matrix algebra. In this section we cover the basics of matrix algebra relevant to calculating the coefficients for an estimated linear regression model. A matrix is a rectangular array of numbers organized in rows and columns and each number within a matrix is an element. A matrix is defined as consisting of <code>m</code> rows and <code>n</code> columns. <strong>Note:</strong> If m = n, the matrix is referred to as a square matrix.</p>
<p>The following is a general form of a matrix:</p>
<p><span class="math display">\[x_{m\times n} = \begin{bmatrix}
    x_{11} &amp; \dots &amp; x_{1n} \\
    \vdots &amp; \ddots &amp; \vdots \\
    x_{m1} &amp; \dots &amp; x_{mn}
\end{bmatrix}\]</span></p>
<p>There are a number of operations in matrix algebra; however, this review focuses on those pertinent to solutions of the least-squared equations in multiple regression:</p>
<div id="transpose-of-a-matrix" class="section level4">
<h4><span class="header-section-number">9.1.1.1</span> Transpose of a matrix</h4>
<p>The transpose of a matrix <strong>A</strong>, denoted as <strong>A’</strong>, is obtained by interchanging the rows and columns of the <strong><em>A</em></strong> matrix:</p>
<p><span class="math display">\[A = \begin{bmatrix}
    1 &amp; 2 &amp; 4 \\
    5 &amp; 7 &amp; 6
\end{bmatrix}, A&#39; = \begin{bmatrix}
    1 &amp; 5 \\
    2 &amp; 7 \\
    4 &amp; 5
\end{bmatrix}\]</span></p>
<p><strong>Note:</strong> The product of a matrix and its transpose is a square matrix.</p>
<p>The <code>matrix()</code> function in R will create a matrix object consisting of provided values in a defined number of rows and columns. The above matrix, <code>A</code>, is created as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">6</span>), <span class="dv">2</span>, <span class="dv">3</span>)
A</code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    6</code></pre>
<p>Further, the <code>t()</code> function will transpose a given matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Aprime &lt;-<span class="st"> </span><span class="kw">t</span>(A)
Aprime</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    4    5
## [3,]    7    6</code></pre>
</div>
<div id="row-column-multiplication" class="section level4">
<h4><span class="header-section-number">9.1.1.2</span> Row-column multiplication</h4>
<p>Matrix multiplication is done by summing the products of the row elements in the first matrix by the column elements in the second matrix in corresponding position. This is shown in the following example:</p>
<p><span class="math display">\[A = \begin{bmatrix}
    1 &amp; 2 &amp; 4 \\
    5 &amp; 7 &amp; 6
\end{bmatrix} \times A&#39; = \begin{bmatrix}
    1 &amp; 5 \\
    2 &amp; 7 \\
    4 &amp; 5
\end{bmatrix} = \begin{bmatrix}
    1*1+2*2+4*4 &amp; 1*5+2*7+4*6 \\
    5*1+7*2+6*4 &amp; 5*5+7*7+6*6
\end{bmatrix} = \begin{bmatrix}
    21 &amp; 43 \\
    43 &amp; 110
\end{bmatrix}\]</span></p>
<p>Similarly, the product of matrices can be calculated in R using the %*% operator:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">AxAprime &lt;-<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>Aprime
AxAprime</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   66   64
## [2,]   64   65</code></pre>
<p><strong>Note:</strong> Not all matrices can be multiplied. The number of columns in the first matrix must equal the number of columns in the second matrix. Further, unlike ordinary algebra multiplication, matrix multiplication is NOT commutative (order of operands matter). In the previous example we were able to find the product of A and A’, because the number of columns in A (3) is equal to the number of rows in A’ (3). Suppose we wanted the product of A’ and B, where B is defined as the following matrix:</p>
<p><span class="math display">\[B = \begin{bmatrix}
    1 &amp; 2 &amp; 3\\
    2 &amp; 6 &amp; 4 \\
    4 &amp; 5 &amp; 6
\end{bmatrix}\]</span></p>
<p>We are unable to find the product of AB because the number of columns in A’ (2) does not equal the number of rows in B (3). We are able to find the product of BA’ as follows:</p>
<p><span class="math display">\[ A&#39; \times B = \begin{bmatrix}
    1 &amp; 2 &amp; 4 \\
    5 &amp; 7 &amp; 6
\end{bmatrix} \times \begin{bmatrix}
    1 &amp; 2 &amp; 3\\
    2 &amp; 6 &amp; 4 \\
    4 &amp; 5 &amp; 6
\end{bmatrix} = \begin{bmatrix}
    21 &amp; 34 &amp; 35 \\
    42 &amp; 82 &amp; 79
\end{bmatrix}\]</span></p>
<p>Or, in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>), <span class="dv">2</span>, <span class="dv">3</span>)</code></pre></div>
<pre><code>## Warning in matrix(c(1, 2, 3, 2, 6, 4, 4, 5, 6), 2, 3): data length [9] is
## not a sub-multiple or multiple of the number of rows [2]</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">BxAprime &lt;-<span class="st"> </span>B <span class="op">%*%</span><span class="st"> </span>Aprime
BxAprime</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   55   53
## [2,]   38   38</code></pre>
<p>In ordinary algebra, 1 is the identity element, whereby any number multiplied by 1 returns that number:</p>
<p><span class="math display">\[ 5 \times 1 = 5\]</span></p>
<p>Similarly, there exists an identity element in matrix algebra, denoted by the symbol <code>I</code>. The identity matrix is a square matrix with a 1 in the same pattern, regardless of size:</p>
<p><span class="math display">\[I_{1\times1}=\begin{bmatrix} 1 \end{bmatrix}, I_{2\times2}=\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1\end{bmatrix}, I_{3\times3} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\]</span></p>
<p>The following demonstrates the identity property for matrices:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">I &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>),<span class="dv">3</span>, <span class="dv">3</span>)
AI &lt;-<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>I
AI</code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A</code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    6</code></pre>
<p>Lastly, the reciprocal of A is known as the inverse matrix, denoted as <span class="math inline">\(A^{-1}\)</span>. In ordinary algebra, the product of a number and its reciprocal is 1. In matrix algebra, the product of a matrix and its inverse is the identity matrix. <span class="math inline">\(AA^{-1}=A^{-1}A=I\)</span> <strong>Note:</strong> Inverse matrices only exist for square matrices, and not all square matrices possess inverses. The inverse of a matrix can be found via the <code>solve()</code> function as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A.inv &lt;-<span class="st"> </span><span class="kw">solve</span>(AxAprime)
A.inv</code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,]  0.3350515 -0.3298969
## [2,] -0.3298969  0.3402062</code></pre>
<p>The following R example demonstrates <code>I</code> as the product of <span class="math inline">\(AA^{-1}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Aident &lt;-<span class="st"> </span>A.inv <span class="op">%*%</span><span class="st">  </span>A
Aident</code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]
## [1,] -0.3247423 -0.3092784  0.3659794
## [2,]  0.3505155  0.3814433 -0.2680412</code></pre>
</div>
</div>
<div id="representing-system-of-linear-equations-as-matrices" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Representing System of Linear Equations as Matrices</h3>
<p>The previous lab introduced the estimated bivariate linear regression model as follows:</p>
<p><span class="math display">\[\hat{y_i}=\hat{\alpha}+\hat{\beta} x_i\]</span></p>
<p>Where <span class="math inline">\(\hat{\alpha}\)</span> anf <span class="math inline">\(\hat{\beta}\)</span> could be solved via the following formulas:</p>
<p><span class="math display">\[\hat{\alpha}=\bar{y} - \hat{\beta}\bar{x}\]</span></p>
<p><span class="math display">\[\hat{\beta}=\frac{cov(x,y)}{var(x)}\]</span></p>
<p>In this lab we demonstrate how to use matrix algebra to calculate the least-squared estimates. For bivariate linear regression, the above formulas and matrix algebra will produce the same result; however, for multivariable linear regression the underlying methods that develop the above formulas become computationally complex such that matrix algebra is the preferred method to calculate the coefficients.</p>
<p>Using bivariate regression we explored hypotheses related to how preference for renewable energy is a function of ideology. Sometimes the dependent variable (<code>y</code>) is not easily explainable via a single independent variable (<code>x</code>), but rather multiple independent variables (<span class="math inline">\(x_0, x_1, ..., x_n\)</span>). We can modify the bivariate regression model to append the additional variables of interest, as follows:</p>
<p><strong>Note:</strong> The <span class="math inline">\(\alpha\)</span> intercept coefficient is replaced with <span class="math inline">\(\beta_0\)</span>.</p>
<p><span class="math display">\[y_i=\beta_0+\sum^n_{j=1}{\beta_j x_{ij}}+\varepsilon_i\]</span></p>
<p>Where, n is the number of independent variables, <span class="math inline">\(\beta_0\)</span> is the regression intercept, and <span class="math inline">\(\beta_j\)</span> is the slope for the <span class="math inline">\(j^{th}\)</span> variable. The above model is the general form of a regression model, and as such, will work for bivariate and multivariable models. If <span class="math inline">\(n=1\)</span>, the model is exactly the same as the model stated in the textbook and previous lab.</p>
<p>Using this model we can imagine collected data as a system of linear equations. To demonstrate, suppose we collected the following data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ex.ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>),
                    <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>))
ex.ds</code></pre></div>
<pre><code>##   x y
## 1 1 1
## 2 2 1
## 3 3 2
## 4 4 2
## 5 5 4</code></pre>
<p>Using the linear regression model above, we can state the data as a system of linear equations:</p>
<p><span class="math display">\[1 = \beta_0 + \beta_1 * 1\]</span></p>
<p><span class="math display">\[1 = \beta_0 + \beta_1 * 2\]</span></p>
<p><span class="math display">\[2 = \beta_0 + \beta_1 * 3\]</span></p>
<p><span class="math display">\[2 = \beta_0 + \beta_1 * 4\]</span></p>
<p><span class="math display">\[4 = \beta_0 + \beta_1 * 5\]</span></p>
<p>Given we are working with two variables, <code>x</code> and <code>y</code>, our <code>n</code> is 1, so we are working with the bivariate linear regression model. Now, we can use ordinary algebra to solve for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. To do so, we will solve for one variable, then solve for the other. Given our system consists of 5 linear equations, the ordinary algebra approach is not practical.</p>
<p>This is where matrix algebra is useful. The general regression model can be expressed in the following matrix form, where we capitalize variables to represent matrices:</p>
<p><span class="math display">\[Y=X\beta_1+\varepsilon\]</span></p>
<p>The system of linear equations can be converted into the following matrices:</p>
<p><span class="math display">\[Y = \begin{bmatrix}1 \\ 1 \\ 2 \\ 2 \\ 4\end{bmatrix}, X = \begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ 1 &amp; 4 \\ 1 &amp; 5\end{bmatrix}, and \beta = \begin{bmatrix}\beta_0 \\ \beta_1\end{bmatrix}\]</span></p>
<p><strong>Note:</strong> The first column of the X matrix is always 1.</p>
</div>
<div id="ols-regression-and-matrices" class="section level3">
<h3><span class="header-section-number">9.1.3</span> OLS Regression and Matrices</h3>
<p>With data expressed in matrix form, we then use matrix algebra to calculate the least-squared estimates. The least-squared estimates formula is:</p>
<p><span class="math display">\[\hat{\beta}=(X&#39;X)^{-1}X&#39;Y\]</span></p>
<p>Using the matrices formed from the system of linear equations, we demonstrate calculating the least-squared estimates using R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>), <span class="dv">5</span>, <span class="dv">1</span>)
Y</code></pre></div>
<pre><code>##      [,1]
## [1,]    1
## [2,]    1
## [3,]    2
## [4,]    2
## [5,]    4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>), <span class="dv">5</span>, <span class="dv">2</span>)
X</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    1    1
## [2,]    1    2
## [3,]    1    3
## [4,]    1    4
## [5,]    1    5</code></pre>
<p>Now we need to find X’ with the <code>t()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Xprime &lt;-<span class="st"> </span><span class="kw">t</span>(X)
Xprime</code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    1    1    1    1
## [2,]    1    2    3    4    5</code></pre>
<p>Next calculate (X’X):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">XprimeX &lt;-<span class="st"> </span>Xprime <span class="op">%*%</span><span class="st"> </span>X
XprimeX</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    5   15
## [2,]   15   55</code></pre>
<p>Now find the inverse of (X’X):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">XprimeXinv &lt;-<span class="st"> </span><span class="kw">solve</span>(XprimeX)
XprimeXinv</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.1 -0.3
## [2,] -0.3  0.1</code></pre>
<p>Multiply <span class="math inline">\((X&#39;X)^{-1}\)</span> by <span class="math inline">\(X&#39;\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">XprimeXinvXprime &lt;-<span class="st"> </span>XprimeXinv <span class="op">%*%</span><span class="st"> </span>Xprime
XprimeXinvXprime</code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]  0.8  0.5  0.2 -0.1 -0.4
## [2,] -0.2 -0.1  0.0  0.1  0.2</code></pre>
<p>Now multiply by Y to find <span class="math inline">\(\hat{\beta}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span>XprimeXinvXprime <span class="op">%*%</span><span class="st"> </span>Y
b</code></pre></div>
<pre><code>##      [,1]
## [1,] -0.1
## [2,]  0.7</code></pre>
<p>We read this matrix as <span class="math inline">\(\beta_0\)</span> is the first position and <span class="math inline">\(\beta_1\)</span> is the following position. <strong>Note:</strong> We could just as easily used the method introduced in the last lab. The following R code demonstrates the equivalence:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>), <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>))
covar &lt;-<span class="st"> </span><span class="kw">cov</span>(df<span class="op">$</span>x, df<span class="op">$</span>y)
vari &lt;-<span class="st"> </span><span class="kw">var</span>(df<span class="op">$</span>x)
bhat &lt;-<span class="st"> </span>covar <span class="op">/</span><span class="st"> </span>vari
xbar &lt;-<span class="st"> </span><span class="kw">mean</span>(df<span class="op">$</span>x)
ybar &lt;-<span class="st"> </span><span class="kw">mean</span>(df<span class="op">$</span>y)
alpha &lt;-<span class="st"> </span>ybar <span class="op">-</span><span class="st"> </span>bhat <span class="op">*</span><span class="st"> </span>xbar

alpha</code></pre></div>
<pre><code>## [1] -0.1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bhat</code></pre></div>
<pre><code>## [1] 0.7</code></pre>
<p><strong>Note:</strong> In the previous code block, <code>alpha</code> is <span class="math inline">\(\beta_0\)</span> and <code>bhat</code> is <span class="math inline">\(\beta_1\)</span>.</p>
<p>With the coefficients calculated, our estimated linear regression model is:</p>
<p><span class="math display">\[\hat{y}=-0.10+0.70x\]</span></p>
<p>Let’s check our work using the <code>lm()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>X)
ols</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ 0 + X)
## 
## Coefficients:
##   X1    X2  
## -0.1   0.7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b</code></pre></div>
<pre><code>##      [,1]
## [1,] -0.1
## [2,]  0.7</code></pre>
<p>The previous example demonstrated calculating least-squared estimates for a bivariate regression model. Next is an example using matrix algebra to calculate the least-squared estimates for a multivariable linear regression model.</p>
<p>Suppose we have the following data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mv.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>),
                    <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>),
                    <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>))
mv.df</code></pre></div>
<pre><code>##   y x1 x2
## 1 1  1  1
## 2 1  2  2
## 3 2  3  2
## 4 2  4  4
## 5 4  5  3</code></pre>
<p>The system of linear equations is:</p>
<p><span class="math inline">\(1 = \beta_0 + \beta_1 * 1 + \beta_2 * 1\)</span> <span class="math inline">\(1 = \beta_0 + \beta_1 * 2 + \beta_2 * 2\)</span> <span class="math inline">\(2 = \beta_0 + \beta_1 * 3 + \beta_2 * 2\)</span> <span class="math inline">\(2 = \beta_0 + \beta_1 * 4 + \beta_2 * 4\)</span> <span class="math inline">\(4 = \beta_0 + \beta_1 * 5 + \beta_2 * 3\)</span></p>
<p>Converted to matrix form:</p>
<p><span class="math display">\[\begin{bmatrix}1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 2 \\ 1 &amp; 3 &amp; 2 \\ 1 &amp; 4 &amp; 4 \\ 1 &amp; 5 &amp; 3 \end{bmatrix} \times \begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_2\end{bmatrix} = \begin{bmatrix}1\\1\\2\\2\\4\end{bmatrix}\]</span></p>
<p>We will use R to find the least-squared coefficients. Note that calculating Bhat in R has been reduced to a single line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">4</span>), <span class="dv">5</span>, <span class="dv">1</span>)
X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>), <span class="dv">5</span>, <span class="dv">3</span>)
Bhat &lt;-<span class="st"> </span>(<span class="kw">solve</span>((<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X))) <span class="op">%*%</span><span class="st"> </span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>Y)
Bhat</code></pre></div>
<pre><code>##        [,1]
## [1,] -0.175
## [2,]  0.425
## [3,]  0.625</code></pre>
<p>Again, we check our work using the <code>lm()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>X)
ols</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ 0 + X)
## 
## Coefficients:
##     X1      X2      X3  
## -0.175   0.425   0.625</code></pre>
</div>
</div>
<div id="part-ii-multiple-regression-in-r" class="section level2">
<h2><span class="header-section-number">9.2</span> Part II: Multiple Regression in R</h2>
<p>The R syntax for multiple linear regression is similar to what we used for bivariate regression: add the independent variables to the <code>lm()</code> function. Construct a model that looks at climate change certainty as the dependent variable with age and ideology as the independent variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sub.ds &lt;-<span class="st"> </span>ds <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="st">&quot;glbcc_cert&quot;</span>, <span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;age&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()
model1 &lt;-<span class="st"> </span><span class="kw">lm</span>(sub.ds<span class="op">$</span>glbcc_cert <span class="op">~</span><span class="st"> </span>sub.ds<span class="op">$</span>ideol <span class="op">+</span><span class="st"> </span>sub.ds<span class="op">$</span>age)</code></pre></div>
<p>Now look at the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sub.ds$glbcc_cert ~ sub.ds$ideol + sub.ds$age)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.1037 -1.5439  0.3458  1.9603  4.3913 
## 
## Coefficients:
##               Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)   8.650915   0.258926  33.411 &lt;0.0000000000000002 ***
## sub.ds$ideol -0.392264   0.030372 -12.915 &lt;0.0000000000000002 ***
## sub.ds$age   -0.003368   0.003705  -0.909               0.363    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.629 on 2515 degrees of freedom
## Multiple R-squared:  0.06372,    Adjusted R-squared:  0.06297 
## F-statistic: 85.58 on 2 and 2515 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Before interpreting these results, we need to review partial effects. Chapter 12 of the textbook discusses partial effects in great detail. Essentially, multivariable regression “controls” for the effects of other dependent variables when reporting the effect of one particular variable. This is not accidental. Explore this for our model. First construct a bivariate regression model for each of independent variable in the multivariable regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model2 &lt;-<span class="st"> </span><span class="kw">lm</span>(sub.ds<span class="op">$</span>glbcc_cert <span class="op">~</span><span class="st"> </span>sub.ds<span class="op">$</span>ideol)
model3 &lt;-<span class="st"> </span><span class="kw">lm</span>(sub.ds<span class="op">$</span>glbcc_cert <span class="op">~</span><span class="st"> </span>sub.ds<span class="op">$</span>age)
<span class="kw">stargazer</span>(model2, model3, <span class="dt">single.row =</span> <span class="ot">TRUE</span>, <span class="dt">type =</span> <span class="st">&quot;text&quot;</span>)</code></pre></div>
<pre><code>## 
## ==================================================================
##                                        Dependent variable:        
##                                 ----------------------------------
##                                             glbcc_cert            
##                                        (1)              (2)       
## ------------------------------------------------------------------
## ideol                           -0.395*** (0.030)                 
## age                                               -0.008** (0.004)
## Constant                        8.459*** (0.150)  7.087*** (0.236)
## ------------------------------------------------------------------
## Observations                          2,518            2,518      
## R2                                    0.063            0.002      
## Adjusted R2                           0.063            0.001      
## Residual Std. Error (df = 2516)       2.629            2.714      
## F Statistic (df = 1; 2516)         170.341***         4.077**     
## ==================================================================
## Note:                                  *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Notice the ideology coefficient in the bivariate model is larger than the ideology coefficient in the multivariable regression model. The same is also true for the age variable. This is because the bivariate model reports the total effects of X on Y, but the multivariable regression model reports the effects of X on Y when “controlling” for the other independent variables. Look at the multivariable regression model again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sub.ds$glbcc_cert ~ sub.ds$ideol + sub.ds$age)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.1037 -1.5439  0.3458  1.9603  4.3913 
## 
## Coefficients:
##               Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)   8.650915   0.258926  33.411 &lt;0.0000000000000002 ***
## sub.ds$ideol -0.392264   0.030372 -12.915 &lt;0.0000000000000002 ***
## sub.ds$age   -0.003368   0.003705  -0.909               0.363    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.629 on 2515 degrees of freedom
## Multiple R-squared:  0.06372,    Adjusted R-squared:  0.06297 
## F-statistic: 85.58 on 2 and 2515 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>To interpret these results: a one unit increase in ideology corresponds with a -0.392 unit decrease in climate change certainty with all other variables held constant at their means. Further, given the p-value &lt; <span class="math inline">\(\alpha\)</span> = 0.05, the change in climate change certainty is statistically significant. Further, assessing the p-value of the age coefficient yields that the partial effect of age on climate change certainty is not statistically significant. You may have noticed a difference in the p-value for the age variable’s coefficient between the bivariate and multivariable regression models. This is due to the reduction in error in the model by adding the ideology variable. This is a good example of why, in most cases, multivariable regression provides a clearer picture of relationships. Only looking at age and climate change risk, we could potentially conclude that there is a statistically significant relationship; however, when appending ideology to model, we find that ideology is the more likely cause of change in climate change certainty.</p>
</div>
<div id="part-iii-hypothesis-testing-with-multivariable-regression" class="section level2">
<h2><span class="header-section-number">9.3</span> Part III: Hypothesis Testing with Multivariable Regression</h2>
<p>Perhaps we want to explore the relationship between opinions about fossil fuels and ideology. The class data set includes a question asking respondents what percentage of Oklahoma’s electricity should come from fossil fuels. It is reasonable to posit that more conservative individuals will want a higher percentage of the state’s electricity to come from fossil fuels. For this test, we include other independent variables: age, income, and education. We establish a hypothesis that the more conservative a respondent is, the more electricity they want to come from fossil fuels, all other variables held constant. The null hypothesis is that there is no difference in preferred percentage of electricity coming from fossil fuels by ideology. First we look at our variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(ds<span class="op">$</span>okelec_foss)</code></pre></div>
<pre><code>##  Factor w/ 63 levels &quot;0&quot;,&quot;10&quot;,&quot;100&quot;,..: 51 28 44 3 28 13 35 51 57 16 ...</code></pre>
<p>Notice that R reads the fossil fuels variable as a factor. We change this to a numeric variable. We coerce it and create a new variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ds<span class="op">$</span>foss &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(ds<span class="op">$</span>okelec_foss)</code></pre></div>
<p>Now let’s look at all the variables. First we create a subset of the data and remove missing observations, then use the <code>skim()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ds.sub &lt;-<span class="st"> </span>ds <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="st">&quot;income&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;foss&quot;</span>, <span class="st">&quot;age&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()

ds.sub <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">skim</span>()</code></pre></div>
<pre><code>## Skim summary statistics
##  n obs: 2283 
##  n variables: 5 
## 
## ── Variable type:integer ───────────────────────────────────────────────────
##   variable missing complete    n  mean    sd p0  p25 p50 p75 p100     hist
##        age       0     2283 2283 60.11 14.08 18 51.5  62  70   99 ▁▂▃▆▇▆▂▁
##  education       0     2283 2283  5.09  1.81  1  4     6   6    8 ▁▃▂▆▂▇▅▂
##      ideol       0     2283 2283  4.64  1.74  1  4     5   6    7 ▂▃▂▆▁▃▇▅
## 
## ── Variable type:numeric ───────────────────────────────────────────────────
##  variable missing complete    n     mean       sd    p0   p25   p50   p75
##      foss       0     2283 2283    31.48    16.19     1    16    35    44
##    income       0     2283 2283 70622.59 59882.75 10000 35000 59000 90000
##    p100     hist
##      63 ▃▆▁▂▇▃▆▁
##  900000 ▇▁▁▁▁▁▁▁</code></pre>
<p>Now construct the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>(foss <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>ideol, <span class="dt">data =</span> ds.sub)</code></pre></div>
<p>Note the different syntax in constructing this model. Instead of writing <code>dataset$variable</code> every time, you can write the variable names, and at the end of the model include <code>data=dataset</code>.</p>
<p>Now examine the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = foss ~ income + education + age + ideol, data = ds.sub)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.917 -10.845   1.464  11.665  39.861 
## 
## Coefficients:
##                 Estimate   Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  5.828128065  1.895228915   3.075              0.00213 ** 
## income       0.000014020  0.000005537   2.532              0.01140 *  
## education   -0.190966563  0.183425325  -1.041              0.29793    
## age          0.188847307  0.022485238   8.399 &lt; 0.0000000000000002 ***
## ideol        3.075532222  0.183457245  16.764 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.95 on 2278 degrees of freedom
## Multiple R-squared:  0.1491, Adjusted R-squared:  0.1476 
## F-statistic: 99.81 on 4 and 2278 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>To interpret these results, we start with the intercept. In a regression model, the intercept is the expected mean of our dependent variable when our independent variables are 0. In this case the expected mean is 5.83. In some models this has meaning; however, given none of our independent variables adopt a zero value, this provides minimal value to interpretation. Now examine the variable that the hypothesis is concerned with: ideology. We see there is a statistically significant coefficient of 3.07. We interpret this by saying that a one unit increase in ideology (from liberal to conservative) corresponds with a 3.07 unit increase in preferred percentage of electricity coming from fossil fuels, all other variables held constant. There are also statistically significant relationships for age and income, suggesting an increase in those correspond with an increase in the dependent variable as well. The adjusted R squared value of .15 suggests that our model accounts for 15 percent of the variability in the dependent variable.</p>
<p>Next we need to visualize the model. Visualizing multiple linear regression is not as simple as visualizing bivariate relationships. There is no intuitive way for us to visualize, in one graphic, how all of these variables look while all others are held constant simultaneously. For a relationship with one dependent variable and two independent variables, we can make a three dimensional scatter plot and regression plane; however, these still don’t provide a very intuitive visualization.</p>
<div id="visualizing-multivariable-linear-regression" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Visualizing Multivariable Linear Regression</h3>
<p>The best way to visualize multiple linear regression is to create a visualization for each independent variable while holding the other independent variables constant. Doing this allows us to see how each relationship between the DV and IV looks. Constructing a quick and dirty visualization for each IV in <code>ggplot2</code> is simiular to the methods used for bivariate linear regression. We will go into greater detail in the next section when we cover predictions with OLS, but making a quick visualization is rather simple. The <code>augment()</code> function from the <code>broom</code> package is very useful for this. The function transforms the data created from an OLS model into a tidyverse data frame format. Use <code>augment()</code>, then melt the data into long form, and create a <code>ggplot2</code> visualization for each IV by using <code>facet_wrap()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">melt</span>(<span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;income&quot;</span>), <span class="dt">variable.name =</span> <span class="kw">c</span>(<span class="st">&quot;IV&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(., <span class="kw">aes</span>(value, foss)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>IV, <span class="dt">scales =</span> <span class="st">&quot;free_x&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/8_mlr10-1.png" width="672" /></p>
<p>Now we can see the general relationship between each independent variable and the dependent variable. The <code>geom_smooth()</code> function defaults to showing 95% confidence intervals. You can disable the confidence intervals with the <code>se=FALSE</code> argument, but that is not recommended. Notice the very large confidence interval in the income visualization, especially at the higher income levels. This happens because there are fewer observations with very high incomes. A smaller sample size leads to a larger standard error, and in turn larger confidence intervals.</p>
</div>
</div>
<div id="part-iv-predicting-with-ols-regression" class="section level2">
<h2><span class="header-section-number">9.4</span> Part IV: Predicting with OLS Regression</h2>
<p>Regression analysis is performed not only to explain relationships, but also to predict. In R, we can use the model we built to predict values of the dependent variable based on the values of the independent variables. Doing so is rather simple. Recall that our model attempts to explain how much of the state’s electricity a respondent thinks should come from fossil fuels as a function of their ideology, age, education, and income.</p>
<p>To start predicting, we need to identify what values we want to assign to our independent variables. Perhaps you wanted to know the preferred percentage of energy coming from fossil fuels for an individual with a bachelor’s degree, an income of 45000, 40 years old, and a moderate (3) ideology. First we need to know the beta coefficients for each variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(model)</code></pre></div>
<pre><code>##    (Intercept)         income      education            age          ideol 
##  5.82812806450  0.00001401976 -0.19096656321  0.18884730710  3.07553222165</code></pre>
<p>Now recall the scalar formula for multiple linear regression:</p>
<p><span class="math display">\[\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \hat{\beta_3}x_3\]</span></p>
<p>Therefore for our model, the formula would be</p>
<p><span class="math display">\[\hat{y} = \beta_{intercept} + \beta_{income} + \beta_{educ} + \beta_{age} + \beta_{ideol}\]</span></p>
<p><strong>Note:</strong> For a bachelor’s degree, the value of the education variable is 6.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="fl">5.83</span> <span class="op">+</span><span class="st"> </span>(.<span class="dv">000014</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">45000</span>)) <span class="op">+</span><span class="st"> </span>(<span class="op">-</span>.<span class="dv">19</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">6</span>)) <span class="op">+</span><span class="st"> </span>(.<span class="dv">19</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">40</span>)) <span class="op">+</span><span class="st"> </span>(<span class="fl">3.07</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">3</span>)))</code></pre></div>
<pre><code>## [1] 22.13</code></pre>
<p>Based on the calculation, a predicted result is 22% of the state’s electricity should come from fossil fuels. There is a more precise way to do this calculation, as is often the case. Using the <code>augment()</code> function from the <code>broom</code> package, we can tell R to return predicted values based on specifications of variable values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">ideol =</span> <span class="dv">3</span>, <span class="dt">income =</span> <span class="dv">45000</span>, <span class="dt">education =</span> <span class="dv">6</span>, <span class="dt">age =</span> <span class="dv">40</span>))</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   ideol income education   age .fitted .se.fit
## * &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1     3  45000         6    40    22.1   0.648</code></pre>
<p><strong>Note:</strong> The original estimate is a little off due to rounding. Using <code>augment()</code> returns a data frame in tidy format. The .fitted value is the predicted value of interest. You can use the <code>predict()</code> function in a similar way, but doing so returns information in vector, not data frame, format.</p>
<p>The <code>augment()</code> function can return multiple values at a time. We can tell R to predict values for a sequence of values, like the range of ideology values. Sequencing one variable while holding the others constant is very common in OLS analysis. The method we will most often use will be holding all other IVs constant at their means.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">ideol =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, <span class="dt">income =</span> <span class="kw">mean</span>(ds.sub<span class="op">$</span>income),
                               <span class="dt">education =</span> <span class="kw">mean</span>(ds.sub<span class="op">$</span>education),
                               <span class="dt">age =</span> <span class="kw">mean</span>(ds.sub<span class="op">$</span>age)))</code></pre></div>
<pre><code>## # A tibble: 7 x 6
##   ideol income education   age .fitted .se.fit
## * &lt;int&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1     1 70623.      5.09  60.1    20.3   0.738
## 2     2 70623.      5.09  60.1    23.3   0.577
## 3     3 70623.      5.09  60.1    26.4   0.435
## 4     4 70623.      5.09  60.1    29.5   0.334
## 5     5 70623.      5.09  60.1    32.6   0.320
## 6     6 70623.      5.09  60.1    35.7   0.400
## 7     7 70623.      5.09  60.1    38.7   0.534</code></pre>
<p>Recall that the earlier hypothesis stated that the more conservative a respondent, the more electricity they will prefer comes from fossil fuels. We can safely reject the null hypothesis, given the clear relationship and overall evidence that there is a positive relationship. To conlcude, let’s build a solid, paper-worthy visualization of the relationship between ideology and opinions on fossil fuels from our model.</p>
<p><strong>Note</strong>:Until now, we have used <code>geom_smooth()</code> to create regression lines. However, the superior method of creating regression lines is to generate predictions outside of ggplot and use <code>geom_line()</code> to plot the line and <code>geom_ribbom()</code> to plot the confidence intervals. This is because <code>geom_smooth()</code> does not let you set the specific IV values. However, the <code>augment()</code> (or <code>predict()</code>!) function tells R to predict values of the DV based on specific values of the IV. To create a good regression line and confidence interval using the <code>augment()</code> function, instruct R to hold all DVs at their means except ideology. Sequence ideology from 1 to 7, and include <code>se.fit=TRUE</code>, then assign the fitted values to an object:</p>
<p>The next step is to calculate the confidence interval. This is rather simple! Using the <code>augment()</code> function in tandem with the <code>mutate()</code> function, we can create a tidy data frame that provides us with all the information needed to construct a visualization with a confidence interval. Assign the data frame to an object called <code>fit.df</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">ideol =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, <span class="dt">income =</span> <span class="kw">mean</span>(ds.sub<span class="op">$</span>income),
                               <span class="dt">education =</span> <span class="kw">mean</span>(ds.sub<span class="op">$</span>education),
                               <span class="dt">age =</span> <span class="kw">mean</span>(ds.sub<span class="op">$</span>age))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">upper =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit,
         <span class="dt">lower =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit) -&gt;<span class="st"> </span>fit.df
fit.df</code></pre></div>
<pre><code>## # A tibble: 7 x 8
##   ideol income education   age .fitted .se.fit upper lower
##   &lt;int&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1 70623.      5.09  60.1    20.3   0.738  21.7  18.8
## 2     2 70623.      5.09  60.1    23.3   0.577  24.5  22.2
## 3     3 70623.      5.09  60.1    26.4   0.435  27.3  25.6
## 4     4 70623.      5.09  60.1    29.5   0.334  30.2  28.8
## 5     5 70623.      5.09  60.1    32.6   0.320  33.2  31.9
## 6     6 70623.      5.09  60.1    35.7   0.400  36.4  34.9
## 7     7 70623.      5.09  60.1    38.7   0.534  39.8  37.7</code></pre>
<p>Now build the visualization:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(fit.df, <span class="kw">aes</span>(ideol, .fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="fl">1.5</span>, <span class="dt">color =</span> <span class="st">&quot;dodgerblue2&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymax =</span> upper, <span class="dt">ymin =</span> lower), <span class="dt">alpha =</span> .<span class="dv">5</span>, <span class="dt">fill =</span> <span class="st">&quot;dodgerblue2&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Fossil Fuel Energy by Ideology&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;% of State&#39;s Electricity Should Come From Fossil Fuels&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Ideology&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;3&quot;</span>, <span class="st">&quot;4&quot;</span>, <span class="st">&quot;5&quot;</span>, <span class="st">&quot;6&quot;</span>, <span class="st">&quot;7&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">15</span>, <span class="dv">40</span>), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">7</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="_main_files/figure-html/8_mlr12-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bivariate-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="categorical-explanatory-variables-dummy-variables-and-interactions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
