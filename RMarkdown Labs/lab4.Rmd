---
title: 'Lab Four: Foundations for Inference'
output: pdf_document

---

This lab introduces the tools for the foundation of inference by examining the normal distribution, standard errors, confidence intervals, p-values, and single sample t-tests. The following packages are required:

1. car
2. psych
3. sm
4. HistData
5. tidyverse

```{r 4_setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(psych)
library(HistData)
library(tidyverse)
ds <- read_csv("https://github.com/ripberjt/qrmlabs/raw/master/Class%20Data%20Set%20Factored.csv")
options(scipen=999)
```

## Part I: Testing for Normality

Lab three introduced the basics of normal distributions. Much of the statistical work done is built on the condition of data that follows a normal distribution. Inspecting data for normality is done via numerous methods.

Recall the `rnorm()` function to generate random values that follow a normal distribution given specified parameters. The following `random` object will consist of 1000 random values given $\mu$ = 50 and $\sigma$ = 5:

```{r 4_norm1, echo = TRUE}
data <- data.frame(random = rnorm(1000, mean = 50, sd = 5))
```

Visualizing data is an important first step to inspecting data for normality. The previous lab introduced density plots, box plots, and Q-Q plots as means of visualizing data and their relationships to a normal distribution. Recall that the Q-Q plot graphs the quantiles of data against quantiles of a normal distribution, given the $\mu$ and $\sigma$ of the data.

```{r 4_qq, echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center'}
ggplot(data, aes(sample = random)) +
  stat_qq() +
  stat_qq_line()
```

The Q-Q plot of the `random` variable in the `data` dataset closly follows the normal distribution This is expected given the `random` variable was created using `rnorm()` function. __Note:__ Visualizing data for normality is an informal approach to inspecting for normality. Various metiongs exist to statistically test the hypothesis that a given sample comes from a normally distributed population:

1. Shapiro-Wilk test
2. Anderson-Darling test
3. Kolmogorov-Smirnov test
4. Pearson's Chi-Squared test

The Shapiro-Wilk test is the most popular method in the social sciences.

### Shapiro-Wilk Test

The `shapiro.test()` function in R employs the Shapiro-Wilk test. Use of the Shapiro-Wilk test is contingent on univariate and continuous data. The hypotheses for the Shapiro-Wilk test are:

$H_0$: The sample come from a normally distributed population  
$H_A$: The sample *does not* come from a normally distributed population

__Note:__ The Shapiro-Wilk test provides a p-value for the test statistic, `W`. This lab will focus on the p-value approach for statistical tests, using an $\alpha$ value of 0.05 as the significance level.

```{r 4_sw test, echo = TRUE}
shapiro.test(data$random)
```

In this case, the p-value from the Shapiro-Wilk test is greater than $\alpha$ = 0.05 so we *cannot* reject $H_0$---that the `random` variable comes from a normal distribution. Again, this is expected given that the `random` variable was created via the `rnorm()` function. If the p-value were less than $\alpha$ = 0.05, we *could* reject $H_0$ and, by extention, conclude that the `random` variable *does not* come from a normally distributed population. 

### Testing Normality 

R provides data pertaining to Yellowstone National Park's Old Faithful geyser in the `faithful` object. Old Faithful eruptions are recorded as duration, in minutes, between events. First, the `describe()` function provides valuable information for the `eruptions` variable in the `faithful` object.

```{r 4_faith, echo = TRUE}
describe(faithful$eruptions)
```

Comparing the `eruptions` data to a normal distribution, given $\mu$ and $\sigma$ from the `eruptions` data, is available via the *geom_density()* and *stat_function()* functions.

```{r 4_faith2, echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center'}
ggplot(faithful, aes(eruptions)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = mean(faithful$eruptions), 
                                         sd = sd(faithful$eruptions)), color = "blue")
```

The black line represents the `eruptions` data, and the blue line represents the normal distribution given the $\mu$ and $\sigma$ values of `eruptions`. __Note:__ The `eruptions` data appears bimodal, and does not fit the normal distribution model given the parameters calculated via the `eruptions` data. 

The `eruptions` data is further examined using Q-Q plots via the `qqPlot()` function:

```{r 4_faith4, echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center'}
ggplot(faithful, aes(sample = eruptions)) +
  stat_qq() +
  stat_qq_line()
```

Most of the points in the Q-Q plot fall well outside the line, further suggesting the `eruptions` data are likely not normally distributed. Lastly, a Shapiro-Wilk test can confirm whether the `eruptions` data is normally distributed:

```{r 4_faith3, echo = TRUE}
shapiro.test(faithful$eruptions)
```

The p-value for the Shapiro-Wilk test is less than $\alpha$ = 0.05, leading us to reject $H_0$ and therefore conclude that the `eruptions` data are *not* normally distributed. __Note:__ The visual plots are likely enough to confirm the `eruptions` data are not normally distributed.

The `MacdonellDF` variable within the `HistData` package consists of finger length data. Again, visualizing the data for normality is performed via various methods. A box plot is generated for the `MacdonellDF` data:

```{r 4_f, echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center'}
ggplot(MacdonellDF, aes(y = finger)) +
  geom_boxplot()
```

The box plot appears balanced, indicating normality. Generating a density plot should show the distribution of `MacdonellDF` variable as similar to the projected normal distribution given the $\mu$ and $\sigma$ parameters of the `MacdonellDF` variable.

```{r 4_f2, echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center'}
ggplot(MacdonellDF, aes(finger)) +
  geom_density() +
  stat_function(fun = "dnorm", args = list(mean = mean(MacdonellDF$finger), 
                                                       sd = sd(MacdonellDF$finger)),
                color = "red", size = 1.5,
                alpha = 0.3)
```

Additionally, a Q-Q plot:

```{r 4_f3, echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center'}
ggplot(MacdonellDF, aes(sample = finger)) +
  stat_qq() +
  stat_qq_line()
```

The visualizations all suggest that the finger length data are normally distributed; however, a Shapiro-Wilk test is still useful to test for normality.

```{r 4_f4, echo = TRUE}
shapiro.test(MacdonellDF$finger)
```

__Note:__ Contrary to the visualizations, which suggest normality, the p-value for the Shapiro-Wilk test is less than $\alpha$ = 0.05, causing us to reject the null hypothesis of normality and therefore conclude that the data do not come from a normal distribution. This is because the sample is relatively large (n = 3,000), resulting in an overly sensitive test. In these cases, trust the visualizations.

## Part II : Standard Error of the Mean (SEM)

Recall that the standard error of the mean (SEM) is the standard deviation of the sample distribution, calculated as the square root of the standard deviation divided by the square root of the sample size.

- $\frac{s}{\sqrt{n}}$

R does not provide a single purpose function to calculate the SEM. As a result, we simply apply the formula above to calculate the SEM in R; here we calculate the SEM of age in the class dataset:

```{r 4_se, echo = TRUE}
sd(ds$age, na.rm = T)/sqrt(length(ds$age) - sum(is.na(ds$age)))
```

Alternatively, the `describe()` function includes the SEM statistic for a given variable, as follows:

```{r 4_se2, echo = TRUE}
describe(ds$age)
describe(ds$age)$se
```

## Part III: Confidence Intervals (CIs)

The standard error is vital to inferential statistics. For example, the standard error is required to calculate confidence intervals (CIs). CIs employ standard error to assist with inferring information from a sample of a larger population, through inclusion of uncertainty. The formula to calculate a CI around the SEM is:

$$CI=\bar{x} \pm z\frac{s}{\sqrt{n}} = \bar{x} \pm z*SE$$

where z corresponds with confidence level. Here are the most common confidence levels and corresponding values of z:

    - 90% = 1.645
    - 95% = 1.960
    - 99% = 2.576

To calculate these values or select a different value, use the following code:

```{r 4_ci1, echo = TRUE}
qnorm(1 - 0.05 / 2) #0.05 = 95% CI
qnorm(1 - 0.2 / 2) #here, 0.05 = 80% CI
```

Here, we calculate a 95% CI around the mean of the `age` variable within `ds`: 

First calculate the mean:

```{r 4_ci2, echo = TRUE}
mean(ds$age, na.rm = T)
```

Second, find the SEM:

```{r 4_ci3, echo = TRUE}
describe(ds$age)$se
```

Given that the z-score for a 95% CI is 1.96, the upper bound of the CI is calculated as follows:

```{r 4_ci4, echo = TRUE}
60.38 + 1.96 * 0.28
```

The lower bound of the confidence interval is calculated as follows:

```{r 4_ci5, echo = TRUE}
60.38 - 1.96 * 0.28
```

Our best guess is that the average age in the population is 60.37; but we are 95% confident that the age is somewhere between 59.8 and 60.9. To increase confidence in the estimate (reduce the likelihood of error), increase the CI from 95% to 99%. The z-score associated to 99% is 2.58, so the 99% CI is calculated as follows:

```{r 4_ci7, echo = TRUE}
age.se <- describe(ds$age)$se
mean(ds$age, na.rm = T) + 2.58 * age.se
mean(ds$age, na.rm = T) - 2.58 * age.se
```

Our best guess is that the average age in the population is 60.37; but we are 99%  confident that the age is somewhere between 59.6 and 61.1. We can use these CIs to test hypotheses about the average age of Oklahomans. For example: we can use it to test the following hypothesis:
$H_0: \mu = 60$
$H_A: \mu \neq 60$

In this case, the both the 95% and the 99% CIs include the null value (60), so we cannot reject the null hypothesis.

## Part IIII: P-values
Like CIs, we can manually calculate p-values in R. This requires the following steps:

1. Define a significe level ($\alpha = 0.05$)
2. Calculate the sample mean ($\bar{x}$)
3. Calculate the SEM
4. Calculate the z-score of the sample mean, assuming the null value is the population mean ($\mu$) and the SE is the standard deviation ($\sigma$)
5. Calculate the p-value, the probability of observing a sample mean of at least $\bar{x}$ if $\mu$ is the null value

Here, is an example using age in the class dataset. We begin by calculating and storing the values we will need.

```{r 4_p, echo = TRUE}
xbar <- mean(ds$age, na.rm = T)
nv <- 60
s <- sd(ds$age, na.rm = T)
n <- 2547
se <- s / sqrt(n)
z <- (xbar - nv) / se
```

Now, we can use these values to calculate a p-value.

```{r 4_p2, echo = TRUE}
2 * pnorm(z, lower.tail = FALSE)
```

In this case, the p-value is 0.191, which is greater than $\alpha$ = 0.05; therefore, we *cannot* reject the null hypothesis ($H_0: \mu = 60$).

## Part IV: Single Sample T-tests
The hypotheses and tests above are single sample tests, where we test a sample value ($\bar{x}$) against a known null value ($\mu = 60$). A more common version of this test uses the `Student's t-distribution` rather than the `standard normal z-distribution` to calculate CIs and p-values. This is called the Single sample t-test. Note that when $n \gtrapprox 100$, the single sample z-test and t-test produce the same results. 

To execute the tests we describe above, use the the `t.test` function. Here is an example, using the same test as above: 

```{r 4_ci9, echo = TRUE}
t.test(ds$age, mu = 60)
```

Note that the `t.test` function, automatically calculates the sample mean, the t-score, CIs, and a p-value. By default, the `t.test` function uses the 95% CI. As above, both the CIs and the p-value indicate that we *cannot* reject the null hypothesis that $\mu = 60$ in Oklahoma. 