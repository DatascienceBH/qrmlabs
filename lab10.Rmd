---
title: 'Lab Ten: Non-normality, Non-linearity, and Multicollinearity'
author: "Alex Davis"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(psych)
library(car)
library(memisc)
library(stargazer)
library(gridExtra)
library(reshape2)
options(scipen = 999)
setwd("~/Methods Labs")
ds <- read.csv("Class Data Set Factored.csv")
```

This lab will focus on addressing issues that arise with non-normality, non-linearity, and multicollinearity. We'll begin by addressing non-linearity. 

## Part One: Non-linearity





### Exploring Non-linearity

With linear relationships between variables in their functional forms being one of the assumptions of OLS, we should always spend time exploring the linearity of the models we make.


Perhaps you wanted to examine the relationship between ideology and candidate support in the 2016 presidential election. Our class data set asks respondents to indicate on a scale of 1 to 5 the level of support they have for the candidate they voted for in the 2016 presidential election. Let's build a model that regresses candidate support on ideology and relevent controls. First get your data and remove missing observations.

```{r poly, echo=TRUE}
sub <- subset(ds, select = c("vote_cand_spt", "ideol","education", "income", "age", "gender", "f.gender", "glbcc_cert", "f.party", "glbcc_risk", "cncrn_econ"))
sub <- na.omit(sub)
```

Now examine the candidate support variable:

```{r poly2, echo=TRUE}
psych::describe(sub$vote_cand_spt)
```

Now a table:

```{r poly3, echo=TRUE}
table(sub$vote_cand_spt)
```

We can see that the mode is 3, a response that indicates a moderate amount of support for the candidate the respondent voted for. We can also tell that there is a negative skew, beacuse there are a lot more responses in the higher values than the lower ones.


Next let's build the model:

```{r lin, echo=TRUE}
model <- lm(vote_cand_spt ~ ideol + education + income + age + gender, data = sub)
summary(model)
```

According to our model, the ideology variable does not play any role in candidate support. Before giving up completely, we should examine the linearity of the variables. One way to do this is to plot the residuals by the values of the independent variables. In theory these relationships should constitute a straight line, with the residuals spread around a line at zero.  To build this visualization we need to:

1. Create variables in the data set for the residuals and fitted (predicted) values.
2. Melt the data into long form, sorted by independent variables.
3. Visualize the  relationship between the residuals and IVs simultaneously using *facet_wrap()*.

First the residuals and fitted values:

```{r lin2, echo=TRUE}
sub$mod.fit <- predict(model)
sub$mod.res <- residuals(model)
```

Now we need to melt the data. This takes data that is wide and makes it long. More specifically for our purposes, it is going to create a column in the data that sorts values by the independent variables:

```{r lin3, echo=TRUE}
sub.melt <- melt(sub, measure.vars = c("ideol", "education", "income", "age", "gender", "mod.fit"))
head(sub.melt)
```

There are now two new columns, a "variable" column and a "value" column, the values that correspond with the variable listed in the variable column. 

The next step is to plot the residuals by the values of the independent variables. We're also going to use *geom_smooth()* to create a loess line that approximates the spread of our data, and we will use *geom_hline()* to plot a horizontal line at 0. Then we will use *facet_wrap()* to tell create different plots for each independent variable:

```{r lin4, echo=TRUE}
ggplot(sub.melt, aes(value, mod.res, group=variable)) +
  geom_point(shape=1) +
  geom_smooth(aes(value, mod.res), method = "loess") +
  geom_hline(yintercept=0) +
  facet_wrap(~variable, scales = "free_x")
```

We can see that there are some potential non-linearities. The ideology visualization makes a compelling case that the ideology variable is non-linear. The next step is to consider adding an exponent, squaring the ideology variable. In the social sciences we want to be wary of overfitting our data. Therefore its important to have an understanding of why you are including an exponent, and to have a grasp on the theoretical reasons for doing do.

When thinking about how ideology might influence candidate support, this could be an instance when a quadratic model might be appropriate. Think about it: it doesn't make any sense to theorize that the more conservative an individaul, the more enthusiastic they were for the candidate they voted for, regardless of the candidate, but it **does** make sense to theorize that the more ideologically extreme an individaul is (very liberal or very conservative) the more they supported the candidate they voted for. Perhaps the moderates voting in the 2016 election felt left our or alienated by the polarized political environment, and therefore could have had support for the candidate they voted for. 

Let's start by building a new model, one that includes ideology squared. It is important to note, though, that the syntax to use a polynomial is: *poly(var,# of powers)*. You cannot just put ideol^2 in the model by itself. You include the *poly* function because it creates an independent variable for each of the powers, which is required to create orthogonal power terms. Let's construct the model:

```{r poly4, echo=TRUE}
poly <- lm(vote_cand_spt ~ poly(ideol,2) + education + income + age + gender, data = sub)
summary(poly)
```

We see that our squared ideology term is statistically significant, but the coefficient does not really provide us an intuituitive interpretation. We need to visualize the relationship to really see what's going on. First, though, let's see if this model provides a better fit for our data. Compare the models using *anova()*. This will test if our second model improved the fit.

```{r comp, echo=TRUE}
anova(model, poly)
```


The results suggest that including ideology squared did improve the fit. We can also look at the adjusted R-squared values:

```{r comp2, echo=TRUE}
mtable(model, poly)
```

These results make a strong case that the polynomial model is superior. This is backed up by tests and theory. Let's move onto the visualization. Since ideoloy is our variable of interest, we will visualize the relationship between ideology and candidate support while holding the other variables constant at their means.

Start by looking at a scatter plot. We'll need to jitter the data because ideology is a fixed scale:

```{r poly5, echo=TRUE}
ggplot(sub, aes(ideol, vote_cand_spt)) +
  geom_jitter(aes(ideol, vote_cand_spt)) +
  geom_point(aes(ideol, vote_cand_spt))
```


Now let's add the regression line and confidence interval. To do this we will:

1. Create predicted values and standard error values of candidate support using the *predict()* function, while holding all other values constant at their mean.
2. Generate upper and lower bands of the confidence interval.
3. Add all vectors to a data frame.
4. Visualize.

First predict. We are going to sequence ideology from 1 to 7 by 0.1 instead of by 1 so that we get a smoother line:

```{r poly6, echo=TRUE}
p.poly <- predict(poly, newdata = data.frame(ideol=seq(1,7,.1), education=mean(sub$education), income=mean(sub$income),
                                             age=mean(sub$age), gender=mean(sub$gender)), se.fit = T)
```

Now create the upper and lower bands of the confidence interval:

```{r poly7, echo=TRUE}
up.poly <- p.poly$fit + 1.96*p.poly$se.fit
low.poly <- p.poly$fit - 1.96*p.poly$se.fit
```

Add all vectors to a data frame:

```{r poly8, echo=TRUE}
df.poly <- data.frame(ideol=seq(1,7,.1), p.poly, up.poly, low.poly)
```

The next step is to visualize. Use *geom_line()* to create the line, and *geom_ribbon()* to create the confidence interval. For practice, let's label the axes and add a title. 

```{r poly9, echo=TRUE}
ggplot(df.poly, aes(ideol, fit)) +
  geom_line(data=df.poly, aes(ideol, fit), size=1, color="dodgerblue") +
  geom_ribbon(aes(ymin=low.poly, ymax=up.poly), alpha=.2) +
  coord_cartesian(ylim=c(2:5), xlim=c(1:7)) +
  xlab("Ideology") +
  ylab("Support for Candidate") +
  ggtitle("Ideology and Support for Candidate") +
  theme_bw()
  
```

This visualization seems to support our theory that the relationship between ideology and candidate support is quadratic, that more ideologically extreme individuals have more support for the candidate they voted for than moderates. 


Let's now use an example where a cubed exponent would be appropriate. A very common model we've used has been exploring the relationship between ideology and climate change risk. Perhaps the relationship between the two is not best described linearly. Let's first make a model we've looked at many times before:

```{r cube, echo=TRUE}
lm1 <- lm(glbcc_risk ~ age + gender + ideol + education, data = sub)
summary(lm1)
```

Now let's plot the residuals by the independent variables:

```{r cube2, echo=TRUE}
sub$lm1.fit <- predict(lm1)
sub$lm1.res <- residuals(lm1)
lm1.melt <- melt(sub, measure.vars = c("ideol", "education", "age", "gender", "lm1.fit"))
ggplot(lm1.melt, aes(value, lm1.res, group=variable)) +
  geom_point(shape=1) +
  geom_smooth(aes(value, lm1.res), method = "loess") +
  geom_hline(yintercept=0) +
  facet_wrap(~variable, scales = "free_x")
```

Looking at the ideology variable, we can see that it is likely not linear. It looks more like the loess line moves above and below the line at zero, suggesting that a cube term might be more appropriate than a square term. Let's build a model that cubes ideology:

```{r cube3, echo=TRUE}
cubed <- lm(formula = glbcc_risk ~ age + gender + poly(ideol, 3)  + education, data = sub)
summary(cubed)
```



Now compare the two models:

```{r cube4, echo=TRUE}
mtable(lm1, cubed)
```

It actually looks like the cube term does not describe the data very well. The adjusted R squared appears to marginally increase in the cubed model, but that does not tell us much. Let's use anova:

```{r cube5, echo=TRUE}
anova(lm1, cubed)
```


The anova test tells us that our cubed model is a better fit. This is likely due to the square term, which registered very significant. Nonetheless, let's visualize this so you have experience seeing cubed lines. 

We'll follow the same steps as last time, predicting values, calculating the confidence interval, and plotting using *geom_line()* and *geom_ribbon()*. 

```{r cube6, echo=TRUE}
p.cubed <- predict(cubed, newdata = data.frame(ideol=seq(1,7,.1), education=mean(sub$education), income=mean(sub$income),
                                             age=mean(sub$age), gender=mean(sub$gender)), se.fit = T)
up.cubed <- p.cubed$fit + 1.96*p.cubed$se.fit
low.cubed <- p.cubed$fit - 1.96*p.cubed$se.fit
df.cubed <- data.frame(ideol=seq(1,7,.1), p.cubed, up.cubed, low.cubed)
windows()
ggplot(df.cubed, aes(ideol, fit)) +
  geom_line(data=df.cubed, aes(ideol, fit), size=1, color="dodgerblue") +
  geom_ribbon(aes(ymin=low.cubed, ymax=up.cubed), alpha=.2) +
  coord_cartesian(ylim=c(1:10), xlim=c(1:7)) +
  xlab("Ideology") +
  ylab("Climate Change Risk") +
  ggtitle("Ideology and Climate Change Risk") +
  theme_bw()
```

## Part Two: Non-normality

This section will go over the problem of non-normality and how to deal with it. One of the key assumptions of OLS is that the residuals of the model are normally ditributed. It is also important to make sure your variables are not skewed too far either negatively or positively. This examine will cover how to make sure your residuals are normally distributed, as well as how to handle greatly-skewed variables. 

To begin, suppose you wanted to examine how different independent variables are related to a vote for Donald Trump in the 2016 presidential election. We will need to clean up the class data set variable on 2016 presidential votes. Let's look at it:

```{r non, echo=TRUE}
table(ds$vote_cand)
```

The codebook for the class data set tells us that a response of 0 indicates a vote for Trump, 1 is for Clinton, 2 for Gary Johnson, 3 for Jill Stein, and 4 for a different candidate. Let's change the variable so that it only includes Trump and Clinton. To transform this variable into a binary indicator of a vote for Trump or Clinton, we need to recode the variable so that responses of 1 equals 0, a response of 0 equals 1, with all else an NA:

```{r non2, echo=TRUE}
ds$v.trump <- car::recode(ds$vote_cand, "0=1; 1=0; else=NA;NA=NA")
table(ds$v.trump)
```

Now let's pull our variables into a new data set and remove missing observations:

```{r non3, echo=TRUE}
new.ds <- subset(ds, select=c("income", "gender", "ideol", "v.trump", "education"))
new.ds <- na.omit(new.ds)
```

Now let's look at our independent variables that are not binary:

```{r non4, echo=TRUE}
psych::describe(new.ds$income)
psych::describe(new.ds$education)
psych::describe(new.ds$ideol)
```

Now let's look at the density distrubtions of them:

```{r non5, echo=TRUE}
inc <- ggplot(new.ds, aes(income))+
  geom_density(adjust=2)
educ <- ggplot(new.ds, aes(education))+
  geom_density(adjust=2)
id <- ggplot(new.ds, aes(ideol))+
  geom_density(adjust=2)
grid.arrange(inc, educ, id, nrow=1, ncol=3)

```


It is clear that the income variable has a large positive. Education and ideology are swked, but not terribly so. One way to fix a skewed variable is to transform it, often by a log. Let's do that:

```{r non6, echo=TRUE}
new.ds$log.inc <- log(new.ds$income)
```

Now let's look at it:

```{r non7, echo=TRUE}
psych::describe(new.ds$log.inc)
```

```{r non8, echo=TRUE}
ggplot(new.ds, aes(log.inc))+
  geom_density(adjust=3)
```

We can see that there is much less skew. Transforming a variable does not really change how you use it in a model, but it does change the interpretation of it. Now the variable is ordered in logs. So a mean of 10.96 incidicates 10.96 natural logs income. Now let's build the model, using the log of income instead of the income variable:

```{r non9, echo=TRUE}
lm.trump <- lm(v.trump ~  log.inc + education + gender + ideol, data=new.ds)
summary(lm.trump)
```

Regarding the ideology variable, there really should not be any surprises. More conservative individuals voted for Trump. Education is significant, but the coefficient is rather small. Our model suggeests that education played a minor role in voting, with more education tending to go with voting for Clinton. Our log.income variable does not seem to play a role. Let's now look at the normality of the residuals. First assign the residuals to our data set:

```{r non10, echo=TRUE}
new.ds$res <- residuals(lm.trump)
```

Now make a density plot of the residuals, but also include a normal curve that has the mean and standard deviation of the residuals:

```{r non11, echo=TRUE}
ggplot(new.ds, aes(res))+
  geom_density(adjust=3) +
  stat_function(fun = dnorm, args=list(mean=mean(new.ds$res), sd=sd(new.ds$res)), 
                color="red")
```

The pure eye test indicates that we might have an issue with non-normality of the residuals. Let's run the Shapiro-Wilk test as well:

```{r non12, echo=TRUE}
shapiro.test(lm.trump$residuals)
```

Recall that the Shapiro-Wilk test tests against the null hypothesis that the data is normally distributed. Our test result indicates that the residuals might not be normal, which is corroborated by the visualization. In a future lab we will go over one way to correct this, robust estimators. 


## Part Three: Multicollinearity

Multicollinearity occurs when the independent varaibles are perfectly correlated. This would render one unable to do any statistical analysis. Even though perfect multicollinearity is very rare, having highly correlated independent variables. The first way to explore potential multicollinearity is to check the correlation of the IVs. 

```{r mc, echo=TRUE}
cor(data.frame(new.ds$ideol, new.ds$log.inc, new.ds$education))
```

There does not appear to be extremely highly-correlated variables. We should also find the variance inflation factor, which measures the increase in variance of the other coefficients due to the inclusion of a particular variable:

```{r mc2, echo=TRUE}
vif(lm.trump)
```

Generally speaking, you dont' want to have a value greater than 5. This model does not appear to have an issue with multicollinearity. 

Now let's use an example that combines everything we've gone over so far. Let's examine the relationship between square footage of the respondent's home and income, age, and education. Start by selecting the data and removing missing observations:

```{r ex, echo=TRUE}
d <- subset(ds, select=c("footage", "income", "age", "education"))
d <- na.omit(d)
head(d)
```

First let's examine the independent variables:

```{r ex2, echo=TRUE}
psych::describe(d$age)
psych::describe(d$education)
psych::describe(d$income)
```

Like earlier, we should do a log-transformation of income:

```{r ex3, echo=TRUE}
d$log.inc <- log(d$income)
```

Now build the model:

```{r ex4, echo=TRUE}
mod <- lm(footage ~ age + education + log.inc, data=d)
summary(mod)
```

Check for multicollinearity:

```{r ex5, echo=TRUE}
cor(data.frame(d$age,d$education,d$log.inc))
```

```{r ex6, echo=TRUE}
vif(mod)
```

Taking all this into account, there does not appear to be a problem with multicollinearity. 

Now let's examine the linearity of the variables. Recall the plot we made earlier that plots the independent variables by the residuals. Let's do that again:

```{r ex7, echo=TRUE}
d$mod.fit <- predict(mod)
d$mod.res <- residuals(mod)
d.melt <- melt(d, measure.vars = c("education", "age", "mod.fit", "log.inc"))
ggplot(d.melt, aes(value, mod.res,group=variable)) +
  geom_point(shape=1) +
  geom_smooth(aes(value, mod.res), method="loess", color="red")+
  geom_hline(yintercept=0) +
  facet_wrap(~variable, scales = "free_x")
```


There does not appear to be an issue with non-linearity either, so we have no reason to include any exponents in the model. 

The next step is to check for non-normality of the residuals:

```{r ex8, echo=TRUE}
d$res <- residuals(mod)
ggplot(d, aes(res))+
  geom_density() +
  stat_function(fun = dnorm, args=list(mean=mean(d$res), sd=sd(d$res)), 
                color="red")
```



```{r ex9, echo=TRUE}
shapiro.test(mod$residuals)
```


Our results and visualization indicate that there could be a problem with non-normality. 

Let's take a look at the results of the model again:

```{r ex10, echo=TRUE}
summary(mod)
```

To interpret this model, we woudl say that a one unit increase in age corresponds with a 9.633 unit increase in square footage of home. Looking down at education, we would say that a one unit increase in *log income* corresponds with a 536.9 unit increase in home square footage. Practically speaking though, how large is the difference between the age increase and the log income increase? Obviously we cna see it is almost a 530 square foot difference, but when thinking about the overall distribution of the square footage variable, is that a lot? Moreover, a one unity chage in age is simply a one year change, whereas a one unity change in income is different, and the same applies to a one unit change in education.


## Part Four: Standardizing Coefficients

If you want to be able to compare coefficients, you need to standardize them. There are three options when standardizing:

1. Standardize the DV
2. Standardize the IVs
3. Standardize all the variables.

Standardizing a variable makes it scaled in standard deviations. This allows us to compare across variables that were orignally scaled differently. Let's use the same model, but this time we will standardize the dependent variable only. Use the *scale()* function on the footage variable to standardize it:

```{r stan, echo=TRUE}
d$z.footage <- scale(d$footage)
```

Now build the model and look at the results:

```{r stan2, echo=TRUE}
z.mod1 <- lm(z.footage ~ age + education + log.inc, data=d)
summary(z.mod1)
```

Since we only standardized the dependent variable, we would interpret this as saying that a one unit increase in age corresponds with a .008 standard deviation increase in square footage. For log income, we would say that a one unit increase in log income corresponds with a .45 standard deviation increase in square footage. Of course, these results hold all other variables constant. 

Now let's standardize the independent variables only:

```{r stan3, echo=TRUE}
d$z.age <- scale(d$age)
d$z.log.income <- scale(d$log.inc)
d$z.education <- scale(d$education)
```

Next build the model:

```{r stan4, echo=TRUE}
z.mod2 <- lm(footage ~ z.age + z.log.income + z.education, data=d)
summary(z.mod2)
```

Now we would say that a one standard deviation increase in age corresponds with a 134.8 unit increase in square footage, and a one standard deviation increase in log income corresponds with a 376.71 unit increase in square footage. Comparing the coefficients here is rather simple and intuitive. Of course, we next need to standardize all the variables and interpret those.

```{r stan5, echo=TRUE}
z.mod3 <- lm(z.footage ~ z.log.income + z.education + z.age, data=d)
summary(z.mod3)
```

Being careful to interpret this correctly, we would say that a one standard deviation change in log income corresponds with a .32 standard deviation increase in square footage, and so on. 








































