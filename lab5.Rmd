---
title: 'Lab Five: Inference for Two Populations'
author:
 - Alex Davis
 - Cody Adams
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Methods Labs")
ds <- read.csv("Class Data Set Factored.csv")
library(ggplot2)
library(psych)
library(car)
library(vcd)
options(scipen = 999)
```

This lab covers the basics of inference for two populations. We'll go through proportions and cross tabulations; the different types of two sample t-tests; difference in one and two tailed tests; and how to plot means and the differences between means. The following packages are required for this lab: 

1. ggplot2
2. psych
3. car
4. vcd

## Part I: Proportions

To start with proportions, We will use the _glbcc_ variable from the class dataset that contains respondents' opinions on global climate change. We start with describing the data. A zero indicates "no" and 1 indicates "yes."

```{r prop, echo=TRUE}
t.gcc <- table(ds$glbcc)
t.gcc
```

Now we can describe the population proportions. To do this we use the _prop.table()_ function. We will specify the _prop.table()_ function to round to 4 decimals.

```{r prop2, echo=TRUE}
p.t.gcc <- round(prop.table(t.gcc), 4)
p.t.gcc 
```

The _prop.table()_ function describes the proportions of the population that believe humans cause climate change. Let's visualize the proportions. 

__Note:__ This lab introduces the _ggplots_ package, which will be used for visualizations moving forward. The _ggplot_ package includes various methods of visualizing data beyond the base visualization functions provided by R.

Data frames are required for ggplot visualizations, which can be constructed from the tables using the _data.frame()_ and _c()_ functions. The following is an example method of creating a data frame manually with values from the previous tables.

```{r prop3, echo=TRUE}
p.t.gcc
df <- data.frame(answer = c("0","1"), glbcc = c("0.4287", "0.5713"))
ggplot(df, aes(x=answer, y=glbcc)) +
  geom_bar(stat = "identity") 
```

As we learned in the last lab, there are uncertainties associated to point estimates. We can include confidence intervals to get a range of values for which we are confident (at some level) the value actually falls between. To calculate confidence intervals we need to find the standard error of the proportionn and assign it to an object that we will name _se_.

```{r prop4, echo=TRUE}
se <-sqrt((0.5713*(1-0.5713)/2547)) 
se
```

Alternatively, we can use the _describe()_ function to avoid rounding errors:

```{r prop5, echo=TRUE}
se <- describe(ds$glbcc)$se
se
```

With the standard error object, we can calculate the confidence intervals. Here we will calculate the following: the upper bounds and lower bounds for the yes and no confidence intervals.

$$CI=\hat{p} \pm z\frac{s}{\sqrt{n}},$$

Where $\hat{p}$ is the sample proportion. Recall that 95% confidence corresponds to a 1.96 z-score.

The proportion table provides the values we will use:

```{r prop6, echo=TRUE}
p.t.gcc
```

We assign these values to a _no_ and _yes_ object:

```{r prop7, echo=TRUE}
no <- 0.4287
yes <- 0.5713
```

Next we will calculate the confidence intervals:

```{r prop8, echo=TRUE}
no.up <- no + 1.96 * se
no.low <- no - 1.96 * se
yes.up <- yes + 1.96 * se
yes.low <- yes - 1.96 * se

```

For simplicity in creating a data frame, we can use the previously created objects. 

```{r prop9, echo=TRUE}
df <- data.frame(answer = c("0", "1"), glbcc = c(no,yes), lower = c(no.low,yes.low),
                 upper = c(no.up, yes.up))
```

With the new data frame we can create a visualization that includes the confidence intervals. The code is similar to the previous _ggplot_ code used, but now with the addition of the _geom\_errorbar()_ function. The _geom\_errorbar()_ function requires arguments to use the lower and upper bound values for the confidence intervals:

```{r prop10, echo=TRUE}
ggplot(df, aes(x=answer, y=glbcc)) +
  geom_bar(stat="identity")+
  geom_errorbar(aes(ymin=lower, ymax=upper))
```

### Two Populations

Research is often interested in differences between distinct populations. Using the _glbcc_ variable, we will review responses differentiated by gender.

```{r 2prop, echo=TRUE}
t.gcc.gend <- table(ds$glbcc, ds$f.gender)
t.gcc.gend
t.gcc.gend.p <- round(prop.table(t.gcc.gend, 2), 4)
t.gcc.gend.p

```

Respondent opinions on climate change can be examined on the difference of gender. This requires extracting the second row from the data set to create a table: 

```{r 2prop2, echo=TRUE}
yes.table <- t.gcc.gend.p[2,]
yes.table
```

To create a visualization, we will start with the mean proportion of support by gender:

```{r 2prop3, echo=TRUE}
men2 <- 0.5273
women2 <- 0.6013
```

Next we will calculate the standard error by gender. We will use the _subset()_ function to simplify the calculations:

```{r 2prop4, echo=TRUE}
male <- subset(ds, gender==1)
female <- subset(ds, gender==0)
```

Now the _describe()_ function used for the standard errors per gender:

```{r 2prop5, echo=TRUE}
men.se <- describe(male$glbcc)$se
women.se <- describe(female$glbcc)$se
```

The upper and lower bounds for the gender confidence intervals:

```{r 2prop6, echo=TRUE}
men.up <- men2 + 1.96*men.se
men.low <- men2 - 1.96*men.se
women.up <- women2 + 1.96*women.se
women.low <- women2 - 1.96*women.se
```

The confidence intervals used to create a data frame for the visualization:

```{r 2prop7, echo=TRUE}

df2 <- data.frame(gender = c("Men", "Women"), glbcc = c(men2,women2), lower = c(men.low,women.low),
                 upper = c(men.up, women.up))
```

Now the _ggplot_ visualization is constructed:

```{r 2prop8, echo=TRUE}
ggplot(df2, aes(x=gender, y=glbcc)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=lower, ymax=upper))
```

Suppose we wondered whether women believe humans cause climate change more than men: this visualization provides only a partial answer. By the "eye test," the visualization appears to show that women have a higher value than men, and furthermore the confidence intervals do not overlap; however, the eye test alone is insufficient. An empirical test is required.

To start, we formulate the following hypotheses:  
$H_0$: there is no difference between genders  
$H_1$: there is a difference between genders  

We can use a two sample t-test to test these hypotheses. Using the different data sets created earlier for genders and the _glbcc_ variable we will find the 95% confidence interval, p-value, and point estimate.

```{r 2prop9, echo=TRUE}
t.test(male$glbcc, female$glbcc)
women2 - men2
```

The t-test yields a p-value < $\alpha$ = 0.05, thereby the null hypothesis is rejected to conclude there is a statistical significance in responses by gender. Further, the point estimate calculated as 0.074 informs us 7% more women than men believe humans cause climate change. __Note:__ The confidence interval tells us that, with 95% confidence, the difference between women and men is between 3% and 11%. Judgment is required to determine whether gender difference is substantive.

## Part II: Cross Tabulations

Another way to examine the difference of gender and beliefs about climate change is cross tabulation. Cross tabulations describe relationships between two variables. The basic building block of cross tabulations are tables, a skill acquired in previous labs.

For this section, we will use the _glbcc\_risk_ variable, that measures the level of risk respondents associate with climate change (on a scale of zero to ten). The range associated to this scale is simplified to zero to five:

For this section let's use a different climate change variable from our class data set: glbcc_risk. This variable measures the level of risk the repsondent associates with climate change, on a scale of zero to ten. That's a pretty wide scale, so let's brush up on our recoding and recode the variable to only go from zero to five:

```{r ct1, echo=TRUE}
ds$r.gccrsk <- recode(ds$glbcc_risk, "0:1=1; 2:3=2; 4:6=3; 7:8:=4; 9:10=5")
table(ds$r.gccrsk)
```

Next the variable is separated by gender using the _table()_ function. The dependent variable (_r.gccrsk_) is specified followed by the independent variable (_f.gender_):

```{r ct2, echo=TRUE}
gcc.table <- table(ds$r.gccrsk, ds$f.gender)
gcc.table
```

The _prop.table()_ function describes the relationship by proportions. We convert the proportion to percentage, then use the percentage by column and by row. The 2 and the 1 tell R to do so: 

```{r ct3, echo=TRUE}
round(prop.table(gcc.table, 2) * 100, 1)
```

##GOT HERE##
We can see that there appears to be a difference between men and women. However, we don't know for sure. In this case we can use a chi-square test to explore whether there is a statistically significant difference. A chi-square test is a way of testing if there is a relationship between two variables. Fortunately we don't have to calculate it by hand, but essentially you take the expected frequency for each cell of a cross tabulation (based on total percentages) and subtracts the observed frequencies, then square those, then divide by the expected frequencies, and then sum the values. You then use that value you just calculated (the chi-square statistic) and the degrees of freedom to determine if there is a statistically significant relationship, based on the chi-square distribution. 

In R, you can simply use the chisq.test() function on the table we created:

```{r ct4, echo=TRUE}
chisq.test(gcc.table)
```

You can also use the summary() function on the table:

```{r ct5, echo=TRUE}
summary(gcc.table)
```


Based on our chi-square test, there does appear to be a statistically significant relationship between gender and perceived risk from climate change. With every test and finding, we should be thinking about substantive significance as well as statistical significance. With this relationship we found, we should ask how strong the relationship actually is. There are a variety of methods that can test this, but for the test we just ran, finding Cramer's V is most appropriate. We can find it by using the assocstats function. It will return a variety of coefficients and numbers, but we need only to use Cramer's V:

```{r ct6, echo=TRUE}
assocstats(gcc.table)
```

A Cramer's V score of 0.093 indicates a pretty weak association. The test returns a value on a scale of 0 to 1, with 1 being complete association. Our score of 0.093 is weak. 

Let's say we wanted to look at gender as a control variable, with ideology as the indepedent variable. We can make a new table with perceived risk of climate change, ideology (Conservative, Moderate, Liberal), and gender:

```{r, ct7, echo=TRUE}
gcc.table2 <- table(ds$r.gccrsk, ds$f.ideology, ds$f.gender)
gcc.table2
```

Now we need to run a chi-square test and find Cramer's V for men and women separately. We do this by putting [,,1] inside the chi square function for men, and [,,2] for women. 

First for men:

```{r ct8, echo=TRUE}
chisq.test(gcc.table2[,,1])
assocstats(gcc.table2[,,1])
```

Now for women:

```{r ct9, echo=TRUE}
chisq.test(gcc.table2[,,2])
assocstats(gcc.table2[,,2])
```

Both chi-square tests return significant results, and both Cramer's V scores are about .4, indicating strong and significant relationships between ideology and perceived risk of climate change for both men and women. 

### Other Coefficients

There are other coefficient scores that are appropriate to use in certain situations. The Phi coefficient is used with a 2x2 contingency table. The contigency coefficient, C, is used for square tables. Keep these different methods in mind when doing cross-tabulations and chi-square tests.

## Part III: Independent T-Tests

This section will provide more elaboration on t tests. So far we have started using t tests more often, as the t distribution is superior to the normal z distribution for sampling distribution and for small n sizes, and it begins to approximate the normal distribution with high n sizes. We're now going to spend time exploring t tests for two populations.

When using t tests for two populations, you need to first decide if you should use an independent t test or a paired t test. An independent t test is used when the two groups you are testing are independent from each other. A paired t test is used when the two groups are paired or connected. For our class data set, it is mostly appropriate to use independent t tests. For an independent t test, you have to make sure that the variance between the two groups is not the same.

Let's examine if there is a difference between the risk associated with climate change for Democrats and Republicans in our survey. In order to test only Democrats and Republicans, we need to recode our factored party variable to only include Democrats and Republicans:

```{r ind, echo=TRUE}
ds$f.part <- recode(ds$f.party.2, "'Dem'='Dem'; 'Rep'='Rep'; else=NA")
table(ds$f.part)
```

Now let's start making sure there is not similar variance for Democrats and Republicans on their beliefs about the risk of climate change. First we'll look at their standard deviation and standard error:

```{r ind2, echo=TRUE}
by(ds$glbcc_risk, ds$f.part, describe)
```

Now we'll use a Levene Test to actually test if they have similar variance. If there is a p value of less than .05, we can assume that the variance is not similar.

```{r ind3, echo=TRUE}
leveneTest(ds$glbcc_risk, ds$f.part)
```

Now we are able to do an independent t test for two populations. Let's establish a null hypothesis and an alternative hypothesis. It's reasonable for us to hypothesize that there is a difference in perceived risk of climate change between Demccrats and Republicans, that the difference in their means is not zero. Therefore the null hypothesis is that there is no difference between Democrats and Republicans in their perceived risk of climate change. Let's now test this:

```{r ind4, echo=TRUE}
t.test(ds$glbcc_risk ~ ds$f.part, var.equal = FALSE) # var.equal=FALSE is default, you don't need to include it.
```

Based on our results, there appears to be a differnece in perceived risk of climate change between Democrats and Republicans. We have to be clear about the null we are rejecting, though. Our null hypothesis is that there is no difference or that the difference is zero. What we have here is a test result that tells us that the result is **not** zero. 

The test we just conducted is a two-tailed t test. Whenever your hypothesis is that the difference is not zero, you conduct a two tailed t test. If we wanted to specify a new hypothesis that the difference in perceived risk from climate change between Democrats and Republicans is greater than (or less than) zero, we would use a one-tailed t test. To do this, we include the command alt="greater" inside our t test function. Let's test this:

```{r ind5, echo=TRUE}
t.test(ds$glbcc_risk ~ ds$f.part, alt = "greater")
```

Notice this time that our alternative hypotehsis is that the true difference in means is greater than zero. Again, we would be able to reject the null. 

### Other Independent Tests

Let's suppose we now want to test if there is a difference in perceived risk of climate change between Democrats, Republicans, and Independents. One option would be to conduct a Tukey Honest Significant Difference test, which can be used to test difference in means across more than two groups. For this test, the variances also cannot be the same. First we'll check the variance:

```{r hsd, echo=TRUE}
leveneTest(ds$glbcc_risk~ds$f.party.2)
```

This passes the Levene Test, so we can assume the variance is not the same. Now let's conduct a Tukey HSD test. Notice we're now using the f.party.2 variable, which is that factored variable that includes Democrats, Republicans, and Independents:

```{r hsd2, echo=TRUE}
TukeyHSD(aov(ds$glbcc_risk~ds$f.party.2))
```
It looks like there are statistically significant differences between all three groups. We can quickly calculate the p values with a Pairwise t test. 

```{r pairwise, echo=TRUE}
pairwise.t.test(ds$glbcc_risk, ds$f.party.2, pool.sd=FALSE)
```

You read these results by looking at the cell labels. The top right cell is blank because that would be independents vs independents. We can see that the p values for all three comparisons are close approaching zero.

Another test we could use is the Wilcox Test. The Wilcox test can be used for an ordinal dependent variable. The test is not parametric and does't assume a distribution. It returns a p value and tests the null hypothesis that the different populations are identical. The Wilcox test can only compare two populations at a time, so let's go back to our f.part variable that includes only Democrats and Republicans.

```{r wilcox, echo=TRUE}
wilcox.test(ds$glbcc_risk ~ ds$f.part)
```

Again, a p value approaching zero. 

There is also a Pairwise-Wilcox test, which combines the Pairwise and Wilcox tests.

## Part IV: Paired T Test

Let's move into a discussion of paired t tests. For a paired t test, the data needs to be connected somehow. For this section, we'll condstruct a data frame with hypothetical data that is appropriate for a paired t test. 

Imagine there is a class with ten students and in the class they take two exams. The teacher wants to examine if there is an overall difference between the students' performances on exam one and exam two. This is a prime opportunity to use a paired t test. Let's build the hypothetical data frame:

```{r paired, echo=TRUE}
Student <- c("st1", "st2", "st3", "st4", "st5", "st6", "st7", "st8", "st9", "st10")
Exam1 <- c(99, 98, 67, 68, 70, 71, 72, 88, 75, 83)
Exam2 <- c(94, 93, 62, 63, 65, 66, 67, 83, 70, 76) 
exam.ds <- data.frame(Student, Exam1, Exam2)
exam.ds
```

We just created a data frame named "exam.ds" with three vectors, the student, exam one, and exam two.

First let's check the variance between the two groups:

```{r paired2, echo=TRUE}
var.test(exam.ds$Exam1, exam.ds$Exam2)

```

Based on the variance test, we can assume that there is similar variance. Now let's specify our hypotheses. Let's start with a two tailed t test. The alternative hypothesis will be that that difference in means between the two exams is not zero. The null is that there is no difference, that the difference is zero. Since these exams are connected (the same students took them), we will use a paired t test. We also need to specify that the variance is equal:

```{r paired3, echo=TRUE}
t.test(exam.ds$Exam1, exam.ds$Exam2, paired = TRUE, var.equal = TRUE)
```

Based on the result of this test, we can reject the null that the difference in means is zero. Let's now use a one tailed t test, still paired. We need to specify a new hypothesis, this time that the differnece in means between the two groups is greater than zero:

```{r paired4, echo=TRUE}
t.test(exam.ds$Exam1, exam.ds$Exam2, paired = TRUE, var.equal = TRUE, alt = "greater")
```

Again, we can confidently reject the null. 

## Part V: Visualizing Differences in Means

Let's now imagine that the teacher wanted these results turned into visualzations. Let's build some good visualizations that compare the test scores from exam one to exam two. To make this visualization we need to calculate the mean scores of both exams, the standard errors for both examns, and the confidence intervals for the mean scores of both exams.

First the mean scores:

```{r vis, echo=TRUE}
exam1 <- mean(exam.ds$Exam1, na.rm = T)
exam2 <- mean(exam.ds$Exam2, na.rm = T)

```

Now the standard errors of the exams:

```{r vis2, echo=TRUE}
exam1.se <- describe(exam.ds$Exam1)$se
exam2.se <- describe(exam.ds$Exam2)$se
```

Now we need to calculate the confidence intervals for both exams. We'll construct 95% confidence intervals, so we will use 1.96:

```{r vis3, echo=TRUE}
exam1up <- exam1 + exam1.se*1.96
exam1low <- exam1 - exam1.se*1.96
exam2up <- exam2 + exam2.se*1.96
exam2low <- exam2 - exam2.se*1.96
```

Next we need to create a data frame that has all of this information in respective vectors:

```{r vis4, echo=TRUE}
test <- c("Exam1", "Exam2")
scores <- c(exam1, exam2)
upper <- c(exam1up, exam2up)
lower <- c(exam1low, exam2low)
examdf <- data.frame(test, scores, upper, lower)
```

Now that we have the data frame, we will start building the visualization. Let's make this a more complete visualization than the previous ones we have made in this lab. Instead of making a bar plot, let's make a graphic that puts a point at the means for each exam, and then condfidence intervals as well. We'll also add some color and some labels. These commands are very reproducible for other data sets and visualizations using GGplot:

```{r vis5, echo = TRUE}
ggplot(examdf, aes(x=test, y=scores)) + # building the basic aesthetic
  geom_point(size=3, shape=19, col = "dodgerblue3") + # telling R to place points, the shape and color as well.
  geom_errorbar(aes(ymin=lower, ymax=upper), width=.1, col="dodgerblue3") + # making error bars based on the confidence intervals
  ylim(10,100) + # Setting the y axis
  ggtitle("Exam Scores") + # Main title
  xlab("Exams") + # Name for x axis
  ylab("Scores") # Name for y axis
```

Following the basic steps above will help you make nice visualizations for different needs. Looking at this visualization, it may appear that there really is not a differnece between the two tests. After all, the confidence intervals overlap. However, what we are testing is if the difference between the means of the two tests is zero or not. This visualization is plotting the mean test scores with confidence intervals. So even though the visualization may not look like it, we still reject the null becuase our t test showed that there is a statistically significant difference that is not zero. 

Let's suppose that the teacher wanted a visualization that plots each students' test scores from exam 1 and exam 2 so that they can easily compare them. We can create a grouped bar plot relatively simply that would allow the teacher to do just that. 

First we would need to create a different data frame, one with an Exam vector, a student vector, and a score vector:

```{r vis6, echo=TRUE}
Exam <- rep(c("Exam1", "Exam2"), each = 10) # Repeats "Exam 1" 10 times and then "Exam 2" 10 times
student <- rep(c("st1", "st2", "st3", "st4", "st5", "st6", "st7", "st8", "st9", "st10"),2) # Repeats the 10 student names 2 times
scores <- c(99, 98, 67, 68, 70, 71, 72, 88, 75, 83, 94, 93, 62, 63, 65, 66, 67, 83, 70, 76) # Lists the 10 Exam 1 scores and then the 10 exam 2 scores. 
exam.com <- data.frame(Exam, student, scores) # Creates the data frame
```

Now let's look at the new data frame. You'll notice that the way we constructed the data frame allows R to pair the test scores for the two exams with each student.

```{r vis7, echo=TRUE}
exam.com
```

Now we build a grouped bar plot. We build this like we would any other bar plot, but this time we tell R to fill the visualization based on the exam (Exam 1 or 2), and we use a position.dodge() function to tell R to place the bars next to each other:

```{r vis8, echo=TRUE}

ggplot(exam.com, aes(x=student, y=scores, fill=Exam)) +
  geom_bar(stat="identity", position = position_dodge())
```